justin@sundesk:~/Documents/Learn/RL-Quadcopter-2$ python ddpg_quadcopter.py --verbose=2 --dropout=0.3 --learn_r=0.0001 --theta=2.0 --sigma=0.1 --action-init-var=0.001 --hidden_units_1=512 --hidden_units_2=64 --memory=1000000 --nb_steps=1000000
/home/justin/.local/lib/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
./gym/gym/__init__.py:15: UserWarning: gym.undo_logger_setup is deprecated. gym no longer modifies the global logging configuration
  warnings.warn("gym.undo_logger_setup is deprecated. gym no longer modifies the global logging configuration")
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
A_observation_input (InputLa (None, 1, 24)             0
_________________________________________________________________
flatten_1 (Flatten)          (None, 24)                0
_________________________________________________________________
A_h0 (Dense)                 (None, 512)               12800
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0
_________________________________________________________________
A_h1 (Dense)                 (None, 64)                32832
_________________________________________________________________
dropout_2 (Dropout)          (None, 64)                0
_________________________________________________________________
activation_2 (Activation)    (None, 64)                0
_________________________________________________________________
A_h2 (Dense)                 (None, 64)                4160
_________________________________________________________________
dropout_3 (Dropout)          (None, 64)                0
_________________________________________________________________
activation_3 (Activation)    (None, 64)                0
_________________________________________________________________
A_h3 (Dense)                 (None, 64)                4160
_________________________________________________________________
dropout_4 (Dropout)          (None, 64)                0
_________________________________________________________________
activation_4 (Activation)    (None, 64)                0
_________________________________________________________________
A_last (Dense)               (None, 4)                 260
_________________________________________________________________
dropout_5 (Dropout)          (None, 4)                 0
_________________________________________________________________
activation_5 (Activation)    (None, 4)                 0
=================================================================
Total params: 54,212
Trainable params: 54,212
Non-trainable params: 0
_________________________________________________________________
None
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
A_observation_input (InputLayer (None, 1, 24)        0
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 24)           0           A_observation_input[0][0]
__________________________________________________________________________________________________
Q_s1 (Dense)                    (None, 512)          12800       flatten_2[0][0]
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 512)          0           Q_s1[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 512)          0           dropout_6[0][0]
__________________________________________________________________________________________________
Q_action_input (InputLayer)     (None, 4)            0
__________________________________________________________________________________________________
Q_h1 (Dense)                    (None, 64)           32832       activation_6[0][0]
__________________________________________________________________________________________________
Q_a1 (Dense)                    (None, 64)           320         Q_action_input[0][0]
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 64)           0           Q_h1[0][0]
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 64)           0           Q_a1[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 64)           0           dropout_8[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 64)           0           dropout_7[0][0]
__________________________________________________________________________________________________
Q_add (Add)                     (None, 64)           0           activation_8[0][0]
                                                                 activation_7[0][0]
__________________________________________________________________________________________________
Q_h2 (Dense)                    (None, 64)           4160        Q_add[0][0]
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 64)           0           Q_h2[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 64)           0           dropout_9[0][0]
__________________________________________________________________________________________________
Q_h3 (Dense)                    (None, 64)           4160        activation_9[0][0]
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 64)           0           Q_h3[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 64)           0           dropout_10[0][0]
__________________________________________________________________________________________________
Q_h4 (Dense)                    (None, 64)           4160        activation_10[0][0]
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 64)           0           Q_h4[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 64)           0           dropout_11[0][0]
__________________________________________________________________________________________________
Q_last (Dense)                  (None, 1)            65          activation_11[0][0]
==================================================================================================
Total params: 58,497
Trainable params: 58,497
Non-trainable params: 0
__________________________________________________________________________________________________
None
2018-04-11 13:52:02.220945: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
Training for 1000000 steps ...
    757/1000000: episode: 1, duration: 3.353s, episode steps: 757, steps per second: 226, episode reward: 210.031, mean reward: 0.277 [-10.309, 0.453], mean action: -0.003 [-1.087, 1.057], mean observation: 0.208 [-0.148, 1.465], loss: 0.021121, mean_absolute_error: 0.164741, mean_q: 0.415382
   1714/1000000: episode: 2, duration: 7.162s, episode steps: 957, steps per second: 134, episode reward: 128.241, mean reward: 0.134 [-10.576, 0.369], mean action: -0.010 [-1.166, 1.252], mean observation: 0.044 [-2.319, 1.213], loss: 0.061376, mean_absolute_error: 0.137789, mean_q: 0.427897
   2293/1000000: episode: 3, duration: 4.355s, episode steps: 579, steps per second: 133, episode reward: 39.225, mean reward: 0.068 [-10.409, 0.203], mean action: -0.000 [-1.129, 1.164], mean observation: 0.223 [-0.337, 1.766], loss: 0.052498, mean_absolute_error: 0.139651, mean_q: 0.538044
   3056/1000000: episode: 4, duration: 6.355s, episode steps: 763, steps per second: 120, episode reward: 133.987, mean reward: 0.176 [-10.652, 0.366], mean action: -0.181 [-1.155, 1.134], mean observation: 0.208 [-3.552, 2.309], loss: 0.072962, mean_absolute_error: 0.134195, mean_q: 0.555708
   3650/1000000: episode: 5, duration: 4.964s, episode steps: 594, steps per second: 120, episode reward: 149.781, mean reward: 0.252 [-10.892, 0.487], mean action: -0.998 [-1.155, -0.827], mean observation: 0.253 [-1.488, 2.619], loss: 0.083483, mean_absolute_error: 0.151918, mean_q: 0.654671
   4233/1000000: episode: 6, duration: 4.730s, episode steps: 583, steps per second: 123, episode reward: 93.115, mean reward: 0.160 [-10.445, 0.338], mean action: -0.987 [-1.132, -0.845], mean observation: 0.252 [-0.247, 1.547], loss: 0.085126, mean_absolute_error: 0.172821, mean_q: 0.786689
   5268/1000000: episode: 7, duration: 7.845s, episode steps: 1035, steps per second: 132, episode reward: 330.406, mean reward: 0.319 [-10.363, 0.514], mean action: -1.008 [-1.173, -0.874], mean observation: 0.069 [-1.982, 1.000], loss: 0.086112, mean_absolute_error: 0.174297, mean_q: 0.894176
   6137/1000000: episode: 8, duration: 6.630s, episode steps: 869, steps per second: 131, episode reward: -9.475, mean reward: -0.011 [-11.198, 0.215], mean action: -0.926 [-1.129, 1.122], mean observation: 0.140 [-3.611, 3.471], loss: 0.076406, mean_absolute_error: 0.176703, mean_q: 1.043176
   6349/1000000: episode: 9, duration: 1.625s, episode steps: 212, steps per second: 130, episode reward: -3.621, mean reward: -0.017 [-10.937, 0.251], mean action: -0.519 [-1.119, 1.082], mean observation: 0.187 [-7.544, 6.068], loss: 0.097520, mean_absolute_error: 0.188322, mean_q: 1.072258
   6569/1000000: episode: 10, duration: 1.834s, episode steps: 220, steps per second: 120, episode reward: 12.083, mean reward: 0.055 [-11.028, 0.383], mean action: -0.475 [-1.072, 1.098], mean observation: 0.170 [-7.806, 6.036], loss: 0.100469, mean_absolute_error: 0.195005, mean_q: 1.077823
   6852/1000000: episode: 11, duration: 2.163s, episode steps: 283, steps per second: 131, episode reward: 16.800, mean reward: 0.059 [-10.748, 0.691], mean action: -0.514 [-1.082, 1.026], mean observation: 0.053 [-7.917, 6.022], loss: 0.094485, mean_absolute_error: 0.206930, mean_q: 1.113422
   7088/1000000: episode: 12, duration: 1.785s, episode steps: 236, steps per second: 132, episode reward: -43.384, mean reward: -0.184 [-11.316, 0.198], mean action: -0.481 [-1.142, 1.051], mean observation: 0.128 [-7.867, 5.972], loss: 0.112765, mean_absolute_error: 0.221134, mean_q: 1.091604
   7250/1000000: episode: 13, duration: 1.244s, episode steps: 162, steps per second: 130, episode reward: 30.393, mean reward: 0.188 [-10.062, 0.302], mean action: -0.486 [-1.082, 1.102], mean observation: 0.188 [-3.276, 6.014], loss: 0.122849, mean_absolute_error: 0.225401, mean_q: 1.093495
   7533/1000000: episode: 14, duration: 2.181s, episode steps: 283, steps per second: 130, episode reward: -36.926, mean reward: -0.130 [-10.862, 0.492], mean action: -0.495 [-1.105, 1.082], mean observation: 0.056 [-7.993, 6.061], loss: 0.102101, mean_absolute_error: 0.223479, mean_q: 1.118165
   7757/1000000: episode: 15, duration: 1.763s, episode steps: 224, steps per second: 127, episode reward: 25.387, mean reward: 0.113 [-10.936, 0.433], mean action: -0.494 [-1.087, 1.054], mean observation: 0.154 [-7.931, 6.045], loss: 0.123910, mean_absolute_error: 0.233918, mean_q: 1.130705
   8039/1000000: episode: 16, duration: 2.224s, episode steps: 282, steps per second: 127, episode reward: 8.817, mean reward: 0.031 [-10.306, 0.382], mean action: -0.500 [-1.107, 1.111], mean observation: 0.026 [-8.047, 6.072], loss: 0.145772, mean_absolute_error: 0.239848, mean_q: 1.146865
   8269/1000000: episode: 17, duration: 1.796s, episode steps: 230, steps per second: 128, episode reward: -9.266, mean reward: -0.040 [-11.183, 0.331], mean action: -0.488 [-1.099, 1.049], mean observation: 0.147 [-7.982, 6.029], loss: 0.110044, mean_absolute_error: 0.236862, mean_q: 1.161481
   8490/1000000: episode: 18, duration: 1.778s, episode steps: 221, steps per second: 124, episode reward: -36.458, mean reward: -0.165 [-11.296, 0.190], mean action: -0.485 [-1.086, 1.076], mean observation: 0.173 [-7.923, 6.056], loss: 0.159032, mean_absolute_error: 0.251052, mean_q: 1.155240
   8710/1000000: episode: 19, duration: 1.694s, episode steps: 220, steps per second: 130, episode reward: -21.947, mean reward: -0.100 [-11.138, 0.212], mean action: -0.495 [-1.086, 1.069], mean observation: 0.158 [-7.844, 6.041], loss: 0.113450, mean_absolute_error: 0.241497, mean_q: 1.180880
   8919/1000000: episode: 20, duration: 1.626s, episode steps: 209, steps per second: 129, episode reward: -15.456, mean reward: -0.074 [-10.840, 0.145], mean action: -0.501 [-1.149, 1.062], mean observation: 0.151 [-7.345, 6.043], loss: 0.164797, mean_absolute_error: 0.252219, mean_q: 1.207550
   9111/1000000: episode: 21, duration: 1.478s, episode steps: 192, steps per second: 130, episode reward: 1.450, mean reward: 0.008 [-10.469, 0.153], mean action: -0.489 [-1.087, 1.064], mean observation: 0.163 [-5.934, 6.048], loss: 0.132808, mean_absolute_error: 0.244434, mean_q: 1.233095
   9340/1000000: episode: 22, duration: 1.776s, episode steps: 229, steps per second: 129, episode reward: -22.401, mean reward: -0.098 [-11.145, 0.247], mean action: -0.490 [-1.119, 1.165], mean observation: 0.164 [-7.961, 6.081], loss: 0.144834, mean_absolute_error: 0.243860, mean_q: 1.277631
   9619/1000000: episode: 23, duration: 2.184s, episode steps: 279, steps per second: 128, episode reward: -0.793, mean reward: -0.003 [-10.788, 0.586], mean action: -0.493 [-1.120, 1.130], mean observation: 0.055 [-7.999, 6.073], loss: 0.155012, mean_absolute_error: 0.250480, mean_q: 1.296365
   9887/1000000: episode: 24, duration: 2.091s, episode steps: 268, steps per second: 128, episode reward: -50.912, mean reward: -0.190 [-11.027, 0.314], mean action: -0.505 [-1.119, 1.056], mean observation: 0.088 [-7.973, 6.064], loss: 0.117648, mean_absolute_error: 0.246118, mean_q: 1.299005
  10087/1000000: episode: 25, duration: 1.529s, episode steps: 200, steps per second: 131, episode reward: -0.825, mean reward: -0.004 [-10.654, 0.181], mean action: -0.497 [-1.118, 1.057], mean observation: 0.188 [-6.556, 6.027], loss: 0.141860, mean_absolute_error: 0.247280, mean_q: 1.324155
  10373/1000000: episode: 26, duration: 2.321s, episode steps: 286, steps per second: 123, episode reward: -39.483, mean reward: -0.138 [-10.896, 0.478], mean action: -0.523 [-1.134, 1.042], mean observation: 0.057 [-7.941, 5.986], loss: 0.163418, mean_absolute_error: 0.254449, mean_q: 1.347039
  10646/1000000: episode: 27, duration: 2.158s, episode steps: 273, steps per second: 127, episode reward: 11.545, mean reward: 0.042 [-10.545, 0.418], mean action: -0.496 [-1.099, 1.047], mean observation: 0.044 [-7.956, 6.036], loss: 0.148447, mean_absolute_error: 0.254209, mean_q: 1.357267
  10902/1000000: episode: 28, duration: 1.966s, episode steps: 256, steps per second: 130, episode reward: -29.281, mean reward: -0.114 [-10.948, 0.290], mean action: -0.484 [-1.067, 1.136], mean observation: 0.063 [-8.015, 6.080], loss: 0.168762, mean_absolute_error: 0.259554, mean_q: 1.362680
  11160/1000000: episode: 29, duration: 1.989s, episode steps: 258, steps per second: 130, episode reward: -66.459, mean reward: -0.258 [-11.271, 0.223], mean action: -0.475 [-1.093, 1.146], mean observation: 0.120 [-8.002, 6.026], loss: 0.164974, mean_absolute_error: 0.258789, mean_q: 1.377812
  11359/1000000: episode: 30, duration: 1.543s, episode steps: 199, steps per second: 129, episode reward: 15.838, mean reward: 0.080 [-10.717, 0.307], mean action: -0.516 [-1.135, 1.055], mean observation: 0.187 [-6.722, 6.078], loss: 0.157989, mean_absolute_error: 0.259681, mean_q: 1.419436
  11621/1000000: episode: 31, duration: 2.024s, episode steps: 262, steps per second: 129, episode reward: -22.875, mean reward: -0.087 [-11.069, 0.433], mean action: -0.523 [-1.103, 1.047], mean observation: 0.099 [-7.982, 6.062], loss: 0.137968, mean_absolute_error: 0.254430, mean_q: 1.445402
  11894/1000000: episode: 32, duration: 2.101s, episode steps: 273, steps per second: 130, episode reward: -19.856, mean reward: -0.073 [-10.964, 0.504], mean action: -0.516 [-1.112, 1.077], mean observation: 0.062 [-7.963, 6.033], loss: 0.194265, mean_absolute_error: 0.270207, mean_q: 1.453263
  12104/1000000: episode: 33, duration: 1.635s, episode steps: 210, steps per second: 128, episode reward: -14.899, mean reward: -0.071 [-10.824, 0.148], mean action: -0.481 [-1.143, 1.140], mean observation: 0.165 [-7.575, 6.081], loss: 0.159976, mean_absolute_error: 0.266116, mean_q: 1.439499
  12367/1000000: episode: 34, duration: 2.180s, episode steps: 263, steps per second: 121, episode reward: 21.096, mean reward: 0.080 [-10.847, 0.538], mean action: -0.478 [-1.046, 1.107], mean observation: 0.084 [-7.959, 6.038], loss: 0.160269, mean_absolute_error: 0.262673, mean_q: 1.498137
  12603/1000000: episode: 35, duration: 1.921s, episode steps: 236, steps per second: 123, episode reward: -58.666, mean reward: -0.249 [-11.427, 0.164], mean action: -0.504 [-1.134, 1.056], mean observation: 0.168 [-7.933, 6.005], loss: 0.158963, mean_absolute_error: 0.259599, mean_q: 1.516366
  12855/1000000: episode: 36, duration: 1.973s, episode steps: 252, steps per second: 128, episode reward: -59.834, mean reward: -0.237 [-11.322, 0.259], mean action: -0.491 [-1.117, 1.084], mean observation: 0.114 [-8.030, 6.075], loss: 0.192671, mean_absolute_error: 0.274397, mean_q: 1.508865
  12951/1000000: episode: 37, duration: 0.711s, episode steps: 96, steps per second: 135, episode reward: 13.425, mean reward: 0.140 [-9.761, 0.245], mean action: -0.516 [-1.105, 1.095], mean observation: 0.139 [-0.431, 2.825], loss: 0.214354, mean_absolute_error: 0.278248, mean_q: 1.504507
  13137/1000000: episode: 38, duration: 1.445s, episode steps: 186, steps per second: 129, episode reward: 22.355, mean reward: 0.120 [-9.948, 0.196], mean action: -0.812 [-1.073, 1.024], mean observation: 0.185 [-1.651, 4.404], loss: 0.206391, mean_absolute_error: 0.276901, mean_q: 1.525898
  13717/1000000: episode: 39, duration: 4.470s, episode steps: 580, steps per second: 130, episode reward: 18.228, mean reward: 0.031 [-11.007, 0.247], mean action: -0.932 [-1.114, 0.948], mean observation: 0.267 [-2.696, 3.746], loss: 0.177224, mean_absolute_error: 0.274226, mean_q: 1.581721
  13944/1000000: episode: 40, duration: 1.712s, episode steps: 227, steps per second: 133, episode reward: 17.858, mean reward: 0.079 [-10.887, 0.397], mean action: -0.696 [-1.091, 1.036], mean observation: 0.234 [-5.174, 5.427], loss: 0.219970, mean_absolute_error: 0.284391, mean_q: 1.627738
  14186/1000000: episode: 41, duration: 2.254s, episode steps: 242, steps per second: 107, episode reward: -17.486, mean reward: -0.072 [-11.141, 0.288], mean action: -0.659 [-1.095, 1.043], mean observation: 0.223 [-5.613, 5.499], loss: 0.173177, mean_absolute_error: 0.273877, mean_q: 1.634264
  14456/1000000: episode: 42, duration: 2.085s, episode steps: 270, steps per second: 129, episode reward: -13.484, mean reward: -0.050 [-11.333, 0.407], mean action: -0.605 [-1.104, 1.089], mean observation: 0.178 [-5.899, 5.615], loss: 0.203067, mean_absolute_error: 0.287977, mean_q: 1.687157
  14674/1000000: episode: 43, duration: 1.707s, episode steps: 218, steps per second: 128, episode reward: 16.074, mean reward: 0.074 [-10.922, 0.395], mean action: -0.580 [-1.103, 1.041], mean observation: 0.234 [-5.935, 5.725], loss: 0.195645, mean_absolute_error: 0.294646, mean_q: 1.734910
  14933/1000000: episode: 44, duration: 1.988s, episode steps: 259, steps per second: 130, episode reward: -13.455, mean reward: -0.052 [-11.494, 0.522], mean action: -0.629 [-1.106, 1.083], mean observation: 0.183 [-6.306, 5.791], loss: 0.194554, mean_absolute_error: 0.299827, mean_q: 1.809448
  15182/1000000: episode: 45, duration: 1.940s, episode steps: 249, steps per second: 128, episode reward: -106.658, mean reward: -0.428 [-12.001, 0.142], mean action: -0.257 [-1.099, 1.019], mean observation: -0.031 [-7.140, 5.016], loss: 0.182005, mean_absolute_error: 0.302288, mean_q: 1.822215
  15422/1000000: episode: 46, duration: 1.885s, episode steps: 240, steps per second: 127, episode reward: -56.643, mean reward: -0.236 [-12.172, 0.546], mean action: -0.327 [-1.050, 1.059], mean observation: -0.078 [-6.467, 3.982], loss: 0.185704, mean_absolute_error: 0.306696, mean_q: 1.877558
  15662/1000000: episode: 47, duration: 1.869s, episode steps: 240, steps per second: 128, episode reward: -115.996, mean reward: -0.483 [-12.163, 0.169], mean action: -0.384 [-1.061, 1.097], mean observation: -0.102 [-6.441, 4.125], loss: 0.174641, mean_absolute_error: 0.305172, mean_q: 1.885730
  15918/1000000: episode: 48, duration: 2.050s, episode steps: 256, steps per second: 125, episode reward: -63.671, mean reward: -0.249 [-11.933, 0.389], mean action: -0.487 [-1.086, 1.058], mean observation: -0.075 [-5.921, 4.180], loss: 0.209079, mean_absolute_error: 0.315257, mean_q: 1.926273
  16131/1000000: episode: 49, duration: 1.733s, episode steps: 213, steps per second: 123, episode reward: -65.374, mean reward: -0.307 [-11.663, 0.163], mean action: -0.405 [-1.098, 1.069], mean observation: 0.008 [-6.145, 4.017], loss: 0.198444, mean_absolute_error: 0.316793, mean_q: 1.938430
  16304/1000000: episode: 50, duration: 1.324s, episode steps: 173, steps per second: 131, episode reward: 12.138, mean reward: 0.070 [-10.437, 0.251], mean action: -0.363 [-1.117, 1.045], mean observation: 0.071 [-4.772, 3.965], loss: 0.181223, mean_absolute_error: 0.313396, mean_q: 1.940998
  16568/1000000: episode: 51, duration: 2.058s, episode steps: 264, steps per second: 128, episode reward: -138.540, mean reward: -0.525 [-12.627, 0.453], mean action: -0.425 [-1.056, 1.060], mean observation: -0.117 [-6.472, 3.895], loss: 0.186009, mean_absolute_error: 0.312906, mean_q: 1.976546
  16793/1000000: episode: 52, duration: 1.712s, episode steps: 225, steps per second: 131, episode reward: -12.029, mean reward: -0.053 [-11.617, 0.485], mean action: -0.380 [-1.099, 1.036], mean observation: -0.042 [-6.264, 4.066], loss: 0.205340, mean_absolute_error: 0.321370, mean_q: 2.004502
  17043/1000000: episode: 53, duration: 1.936s, episode steps: 250, steps per second: 129, episode reward: -61.108, mean reward: -0.244 [-11.719, 0.348], mean action: -0.086 [-1.167, 1.072], mean observation: -0.084 [-6.072, 4.083], loss: 0.209620, mean_absolute_error: 0.326795, mean_q: 2.000817
  17288/1000000: episode: 54, duration: 1.925s, episode steps: 245, steps per second: 127, episode reward: -97.193, mean reward: -0.397 [-11.906, 0.266], mean action: -0.061 [-1.176, 1.130], mean observation: -0.078 [-6.371, 4.204], loss: 0.213509, mean_absolute_error: 0.330362, mean_q: 2.030762
  17532/1000000: episode: 55, duration: 1.892s, episode steps: 244, steps per second: 129, episode reward: -63.062, mean reward: -0.258 [-11.783, 0.374], mean action: -0.068 [-1.094, 1.065], mean observation: -0.084 [-6.255, 3.943], loss: 0.213561, mean_absolute_error: 0.332903, mean_q: 2.002212
  17760/1000000: episode: 56, duration: 1.757s, episode steps: 228, steps per second: 130, episode reward: -16.543, mean reward: -0.073 [-11.453, 0.431], mean action: -0.059 [-1.159, 1.092], mean observation: -0.073 [-6.283, 3.926], loss: 0.210129, mean_absolute_error: 0.329486, mean_q: 2.018429
  18007/1000000: episode: 57, duration: 1.885s, episode steps: 247, steps per second: 131, episode reward: -17.996, mean reward: -0.073 [-11.711, 0.693], mean action: -0.027 [-1.094, 1.082], mean observation: -0.125 [-6.640, 3.872], loss: 0.224419, mean_absolute_error: 0.334377, mean_q: 2.058459
  18257/1000000: episode: 58, duration: 1.900s, episode steps: 250, steps per second: 132, episode reward: -72.807, mean reward: -0.291 [-11.902, 0.493], mean action: -0.015 [-1.101, 1.108], mean observation: -0.119 [-6.721, 3.971], loss: 0.225464, mean_absolute_error: 0.338473, mean_q: 2.037696
  18470/1000000: episode: 59, duration: 1.672s, episode steps: 213, steps per second: 127, episode reward: -55.586, mean reward: -0.261 [-11.922, 0.344], mean action: 0.010 [-1.121, 1.062], mean observation: -0.071 [-6.725, 4.030], loss: 0.206029, mean_absolute_error: 0.334243, mean_q: 2.054662
  18690/1000000: episode: 60, duration: 1.705s, episode steps: 220, steps per second: 129, episode reward: -8.376, mean reward: -0.038 [-11.623, 0.514], mean action: -0.020 [-1.130, 1.061], mean observation: -0.068 [-6.672, 4.057], loss: 0.201676, mean_absolute_error: 0.335981, mean_q: 2.052939
  18931/1000000: episode: 61, duration: 1.868s, episode steps: 241, steps per second: 129, episode reward: -28.388, mean reward: -0.118 [-11.896, 0.619], mean action: 0.019 [-1.101, 1.117], mean observation: -0.122 [-6.821, 3.937], loss: 0.234214, mean_absolute_error: 0.343517, mean_q: 2.072348
  19098/1000000: episode: 62, duration: 1.300s, episode steps: 167, steps per second: 128, episode reward: 13.275, mean reward: 0.079 [-10.762, 0.344], mean action: 0.015 [-1.128, 1.117], mean observation: 0.035 [-6.106, 3.975], loss: 0.216419, mean_absolute_error: 0.345667, mean_q: 2.077226
  19328/1000000: episode: 63, duration: 1.824s, episode steps: 230, steps per second: 126, episode reward: -92.765, mean reward: -0.403 [-12.157, 0.308], mean action: -0.007 [-1.056, 1.067], mean observation: -0.074 [-6.814, 3.933], loss: 0.224279, mean_absolute_error: 0.345722, mean_q: 2.097069
  19541/1000000: episode: 64, duration: 1.670s, episode steps: 213, steps per second: 128, episode reward: -60.992, mean reward: -0.286 [-11.826, 0.278], mean action: 0.045 [-1.095, 1.100], mean observation: -0.045 [-6.735, 3.962], loss: 0.198870, mean_absolute_error: 0.340215, mean_q: 2.118788
  19785/1000000: episode: 65, duration: 1.876s, episode steps: 244, steps per second: 130, episode reward: -95.196, mean reward: -0.390 [-12.216, 0.458], mean action: 0.036 [-1.028, 1.093], mean observation: -0.114 [-6.785, 3.970], loss: 0.221075, mean_absolute_error: 0.347575, mean_q: 2.158647
  19982/1000000: episode: 66, duration: 1.499s, episode steps: 197, steps per second: 131, episode reward: -46.471, mean reward: -0.236 [-11.659, 0.240], mean action: 0.024 [-1.074, 1.101], mean observation: -0.018 [-6.789, 3.961], loss: 0.203875, mean_absolute_error: 0.347518, mean_q: 2.098984
  20165/1000000: episode: 67, duration: 1.411s, episode steps: 183, steps per second: 130, episode reward: -4.047, mean reward: -0.022 [-11.231, 0.348], mean action: 0.020 [-1.070, 1.078], mean observation: 0.001 [-6.830, 3.986], loss: 0.223258, mean_absolute_error: 0.351038, mean_q: 2.143203
  20399/1000000: episode: 68, duration: 1.787s, episode steps: 234, steps per second: 131, episode reward: -75.885, mean reward: -0.324 [-12.095, 0.441], mean action: 0.017 [-1.102, 1.152], mean observation: -0.101 [-6.791, 3.917], loss: 0.248510, mean_absolute_error: 0.358337, mean_q: 2.103297
  20630/1000000: episode: 69, duration: 1.787s, episode steps: 231, steps per second: 129, episode reward: -80.242, mean reward: -0.347 [-11.927, 0.292], mean action: 0.035 [-1.091, 1.091], mean observation: -0.072 [-6.786, 3.933], loss: 0.207622, mean_absolute_error: 0.348266, mean_q: 2.170825
  20849/1000000: episode: 70, duration: 1.697s, episode steps: 219, steps per second: 129, episode reward: -37.675, mean reward: -0.172 [-11.839, 0.438], mean action: 0.009 [-1.083, 1.125], mean observation: -0.088 [-6.727, 3.992], loss: 0.190351, mean_absolute_error: 0.348001, mean_q: 2.160803
  21004/1000000: episode: 71, duration: 1.202s, episode steps: 155, steps per second: 129, episode reward: 10.072, mean reward: 0.065 [-10.563, 0.273], mean action: 0.036 [-1.051, 1.095], mean observation: 0.076 [-5.524, 3.938], loss: 0.206720, mean_absolute_error: 0.353718, mean_q: 2.135455
  21214/1000000: episode: 72, duration: 1.595s, episode steps: 210, steps per second: 132, episode reward: -13.743, mean reward: -0.065 [-11.617, 0.470], mean action: 0.013 [-1.114, 1.056], mean observation: -0.052 [-6.826, 3.889], loss: 0.242125, mean_absolute_error: 0.358098, mean_q: 2.148079
  21466/1000000: episode: 73, duration: 1.919s, episode steps: 252, steps per second: 131, episode reward: -39.482, mean reward: -0.157 [-11.878, 0.587], mean action: 0.031 [-1.054, 1.128], mean observation: -0.132 [-6.721, 4.066], loss: 0.203590, mean_absolute_error: 0.354781, mean_q: 2.180413
  21692/1000000: episode: 74, duration: 1.771s, episode steps: 226, steps per second: 128, episode reward: -43.575, mean reward: -0.193 [-11.941, 0.486], mean action: 0.039 [-1.131, 1.104], mean observation: -0.109 [-6.910, 3.824], loss: 0.266198, mean_absolute_error: 0.369269, mean_q: 2.158710
  21824/1000000: episode: 75, duration: 1.002s, episode steps: 132, steps per second: 132, episode reward: -5.873, mean reward: -0.044 [-10.295, 0.090], mean action: 0.038 [-1.061, 1.146], mean observation: 0.139 [-5.217, 3.991], loss: 0.206414, mean_absolute_error: 0.356134, mean_q: 2.215103
  22035/1000000: episode: 76, duration: 1.776s, episode steps: 211, steps per second: 119, episode reward: -58.889, mean reward: -0.279 [-11.845, 0.263], mean action: 0.044 [-1.141, 1.079], mean observation: -0.080 [-6.834, 4.084], loss: 0.225162, mean_absolute_error: 0.364155, mean_q: 2.214205
  22211/1000000: episode: 77, duration: 1.332s, episode steps: 176, steps per second: 132, episode reward: 11.560, mean reward: 0.066 [-10.956, 0.375], mean action: 0.055 [-1.076, 1.097], mean observation: 0.017 [-6.777, 3.869], loss: 0.257542, mean_absolute_error: 0.365776, mean_q: 2.185646
  22371/1000000: episode: 78, duration: 1.233s, episode steps: 160, steps per second: 130, episode reward: -7.354, mean reward: -0.046 [-10.785, 0.186], mean action: 0.032 [-1.110, 1.088], mean observation: 0.079 [-6.039, 4.009], loss: 0.241732, mean_absolute_error: 0.366792, mean_q: 2.178219
  22532/1000000: episode: 79, duration: 1.225s, episode steps: 161, steps per second: 131, episode reward: 5.799, mean reward: 0.036 [-10.661, 0.255], mean action: 0.043 [-1.093, 1.068], mean observation: 0.069 [-6.053, 3.917], loss: 0.250128, mean_absolute_error: 0.371246, mean_q: 2.221483
  22716/1000000: episode: 80, duration: 1.384s, episode steps: 184, steps per second: 133, episode reward: 3.604, mean reward: 0.020 [-11.102, 0.357], mean action: 0.027 [-1.132, 1.074], mean observation: -0.017 [-6.865, 3.936], loss: 0.244412, mean_absolute_error: 0.368510, mean_q: 2.225540
  22940/1000000: episode: 81, duration: 1.677s, episode steps: 224, steps per second: 134, episode reward: -66.808, mean reward: -0.298 [-12.079, 0.401], mean action: 0.056 [-1.078, 1.108], mean observation: -0.109 [-6.951, 3.840], loss: 0.245976, mean_absolute_error: 0.370200, mean_q: 2.289101
  23171/1000000: episode: 82, duration: 1.781s, episode steps: 231, steps per second: 130, episode reward: -33.096, mean reward: -0.143 [-11.826, 0.515], mean action: 0.046 [-1.135, 1.097], mean observation: -0.121 [-6.958, 3.861], loss: 0.247682, mean_absolute_error: 0.370229, mean_q: 2.264207
  23430/1000000: episode: 83, duration: 1.908s, episode steps: 259, steps per second: 136, episode reward: -105.782, mean reward: -0.408 [-11.962, 0.345], mean action: 0.050 [-1.070, 1.036], mean observation: -0.116 [-6.739, 3.982], loss: 0.258676, mean_absolute_error: 0.377494, mean_q: 2.247750
  23650/1000000: episode: 84, duration: 1.753s, episode steps: 220, steps per second: 125, episode reward: -37.318, mean reward: -0.170 [-11.645, 0.362], mean action: 0.058 [-1.139, 1.067], mean observation: -0.063 [-6.796, 4.050], loss: 0.196212, mean_absolute_error: 0.361822, mean_q: 2.280513
  23787/1000000: episode: 85, duration: 1.154s, episode steps: 137, steps per second: 119, episode reward: 11.792, mean reward: 0.086 [-10.231, 0.232], mean action: 0.003 [-1.108, 1.079], mean observation: 0.119 [-5.274, 3.974], loss: 0.220825, mean_absolute_error: 0.370234, mean_q: 2.231668
  23932/1000000: episode: 86, duration: 1.153s, episode steps: 145, steps per second: 126, episode reward: 16.735, mean reward: 0.115 [-10.354, 0.290], mean action: 0.041 [-1.053, 1.080], mean observation: 0.082 [-5.399, 3.979], loss: 0.252478, mean_absolute_error: 0.380849, mean_q: 2.259973
  24155/1000000: episode: 87, duration: 1.959s, episode steps: 223, steps per second: 114, episode reward: -73.986, mean reward: -0.332 [-12.141, 0.367], mean action: 0.108 [-1.111, 1.051], mean observation: -0.092 [-6.870, 3.979], loss: 0.242205, mean_absolute_error: 0.379503, mean_q: 2.262883
  24388/1000000: episode: 88, duration: 1.958s, episode steps: 233, steps per second: 119, episode reward: -89.477, mean reward: -0.384 [-12.029, 0.262], mean action: 0.144 [-1.130, 1.073], mean observation: -0.125 [-6.872, 4.077], loss: 0.231085, mean_absolute_error: 0.377829, mean_q: 2.275669
  24593/1000000: episode: 89, duration: 1.743s, episode steps: 205, steps per second: 118, episode reward: -70.633, mean reward: -0.345 [-11.956, 0.198], mean action: 0.118 [-1.065, 1.084], mean observation: -0.049 [-6.900, 4.048], loss: 0.244331, mean_absolute_error: 0.381112, mean_q: 2.309342
  24816/1000000: episode: 90, duration: 1.710s, episode steps: 223, steps per second: 130, episode reward: -46.756, mean reward: -0.210 [-11.640, 0.303], mean action: 0.174 [-1.116, 1.064], mean observation: -0.067 [-6.832, 3.971], loss: 0.263218, mean_absolute_error: 0.392010, mean_q: 2.283908
  25006/1000000: episode: 91, duration: 1.492s, episode steps: 190, steps per second: 127, episode reward: -39.745, mean reward: -0.209 [-11.550, 0.210], mean action: 0.120 [-1.042, 1.090], mean observation: -0.003 [-6.857, 3.955], loss: 0.234874, mean_absolute_error: 0.384805, mean_q: 2.264974
  25259/1000000: episode: 92, duration: 1.885s, episode steps: 253, steps per second: 134, episode reward: -39.479, mean reward: -0.156 [-12.022, 0.681], mean action: 0.199 [-1.080, 1.111], mean observation: -0.143 [-6.865, 3.937], loss: 0.255905, mean_absolute_error: 0.388925, mean_q: 2.277624
  25464/1000000: episode: 93, duration: 1.537s, episode steps: 205, steps per second: 133, episode reward: -67.882, mean reward: -0.331 [-11.833, 0.173], mean action: 0.111 [-1.103, 1.070], mean observation: -0.078 [-6.955, 4.051], loss: 0.279671, mean_absolute_error: 0.395981, mean_q: 2.308306
  25710/1000000: episode: 94, duration: 1.825s, episode steps: 246, steps per second: 135, episode reward: -48.799, mean reward: -0.198 [-11.885, 0.499], mean action: 0.211 [-1.086, 1.128], mean observation: -0.121 [-6.911, 4.028], loss: 0.249405, mean_absolute_error: 0.394704, mean_q: 2.264461
  25914/1000000: episode: 95, duration: 1.544s, episode steps: 204, steps per second: 132, episode reward: -75.699, mean reward: -0.371 [-12.005, 0.196], mean action: 0.133 [-1.132, 1.119], mean observation: -0.056 [-6.989, 3.905], loss: 0.234040, mean_absolute_error: 0.389094, mean_q: 2.272651
  26148/1000000: episode: 96, duration: 1.754s, episode steps: 234, steps per second: 133, episode reward: -109.497, mean reward: -0.468 [-12.202, 0.270], mean action: 0.209 [-1.145, 1.080], mean observation: -0.086 [-6.936, 3.901], loss: 0.261173, mean_absolute_error: 0.392312, mean_q: 2.285686
  26387/1000000: episode: 97, duration: 1.790s, episode steps: 239, steps per second: 134, episode reward: -64.736, mean reward: -0.271 [-11.789, 0.327], mean action: 0.208 [-1.106, 1.055], mean observation: -0.091 [-6.813, 4.064], loss: 0.274593, mean_absolute_error: 0.404593, mean_q: 2.268816
  26637/1000000: episode: 98, duration: 1.867s, episode steps: 250, steps per second: 134, episode reward: -56.087, mean reward: -0.224 [-12.135, 0.699], mean action: 0.233 [-1.105, 1.117], mean observation: -0.149 [-6.964, 3.864], loss: 0.269589, mean_absolute_error: 0.404770, mean_q: 2.291072
  26831/1000000: episode: 99, duration: 1.453s, episode steps: 194, steps per second: 133, episode reward: 2.951, mean reward: 0.015 [-11.256, 0.404], mean action: 0.176 [-1.105, 1.112], mean observation: -0.025 [-6.879, 3.999], loss: 0.215349, mean_absolute_error: 0.396928, mean_q: 2.273957
  27067/1000000: episode: 100, duration: 1.783s, episode steps: 236, steps per second: 132, episode reward: -86.480, mean reward: -0.366 [-11.680, 0.180], mean action: 0.235 [-1.146, 1.097], mean observation: -0.126 [-6.984, 3.819], loss: 0.250745, mean_absolute_error: 0.403328, mean_q: 2.243573
  27229/1000000: episode: 101, duration: 1.224s, episode steps: 162, steps per second: 132, episode reward: -15.213, mean reward: -0.094 [-10.859, 0.144], mean action: 0.073 [-1.084, 1.108], mean observation: 0.040 [-6.007, 4.065], loss: 0.244804, mean_absolute_error: 0.405808, mean_q: 2.267919
  27408/1000000: episode: 102, duration: 1.343s, episode steps: 179, steps per second: 133, episode reward: -22.406, mean reward: -0.125 [-11.126, 0.170], mean action: 0.135 [-1.076, 1.101], mean observation: -0.013 [-6.888, 3.985], loss: 0.252227, mean_absolute_error: 0.409434, mean_q: 2.247579
  27644/1000000: episode: 103, duration: 1.762s, episode steps: 236, steps per second: 134, episode reward: -135.163, mean reward: -0.573 [-12.482, 0.239], mean action: 0.207 [-1.047, 1.114], mean observation: -0.105 [-6.935, 3.963], loss: 0.228535, mean_absolute_error: 0.400926, mean_q: 2.294126
  27898/1000000: episode: 104, duration: 1.932s, episode steps: 254, steps per second: 132, episode reward: -91.064, mean reward: -0.359 [-11.790, 0.289], mean action: 0.224 [-1.089, 1.127], mean observation: -0.144 [-6.919, 4.041], loss: 0.255196, mean_absolute_error: 0.409714, mean_q: 2.282911
  28115/1000000: episode: 105, duration: 1.604s, episode steps: 217, steps per second: 135, episode reward: -41.242, mean reward: -0.190 [-11.549, 0.270], mean action: 0.209 [-1.068, 1.069], mean observation: -0.066 [-6.845, 3.992], loss: 0.281827, mean_absolute_error: 0.420721, mean_q: 2.293741
  28315/1000000: episode: 106, duration: 1.513s, episode steps: 200, steps per second: 132, episode reward: 1.357, mean reward: 0.007 [-11.411, 0.446], mean action: 0.127 [-1.103, 1.058], mean observation: -0.033 [-6.921, 4.000], loss: 0.233057, mean_absolute_error: 0.405084, mean_q: 2.270843
  28437/1000000: episode: 107, duration: 0.916s, episode steps: 122, steps per second: 133, episode reward: 12.023, mean reward: 0.099 [-10.016, 0.213], mean action: 0.015 [-1.069, 1.109], mean observation: 0.141 [-4.748, 3.992], loss: 0.284856, mean_absolute_error: 0.421860, mean_q: 2.260076
  28605/1000000: episode: 108, duration: 1.254s, episode steps: 168, steps per second: 134, episode reward: -29.273, mean reward: -0.174 [-11.005, 0.075], mean action: 0.059 [-1.061, 1.085], mean observation: 0.005 [-6.482, 3.912], loss: 0.261975, mean_absolute_error: 0.418644, mean_q: 2.287398
  28857/1000000: episode: 109, duration: 1.881s, episode steps: 252, steps per second: 134, episode reward: -54.301, mean reward: -0.215 [-11.714, 0.516], mean action: 0.063 [-1.059, 1.138], mean observation: -0.146 [-7.073, 3.900], loss: 0.230739, mean_absolute_error: 0.409580, mean_q: 2.293262
  29070/1000000: episode: 110, duration: 1.584s, episode steps: 213, steps per second: 134, episode reward: -76.210, mean reward: -0.358 [-11.907, 0.166], mean action: -0.019 [-1.110, 1.062], mean observation: -0.081 [-7.040, 4.073], loss: 0.275322, mean_absolute_error: 0.425776, mean_q: 2.263198
  29298/1000000: episode: 111, duration: 1.701s, episode steps: 228, steps per second: 134, episode reward: -84.219, mean reward: -0.369 [-12.139, 0.325], mean action: 0.008 [-1.087, 1.093], mean observation: -0.116 [-7.041, 3.985], loss: 0.266008, mean_absolute_error: 0.424559, mean_q: 2.238319
  29544/1000000: episode: 112, duration: 1.839s, episode steps: 246, steps per second: 134, episode reward: -88.202, mean reward: -0.359 [-11.799, 0.298], mean action: -0.002 [-1.109, 1.066], mean observation: -0.147 [-7.037, 3.951], loss: 0.272427, mean_absolute_error: 0.427384, mean_q: 2.237678
  29781/1000000: episode: 113, duration: 1.913s, episode steps: 237, steps per second: 124, episode reward: -85.019, mean reward: -0.359 [-12.027, 0.356], mean action: -0.016 [-1.119, 1.139], mean observation: -0.145 [-7.092, 3.885], loss: 0.261127, mean_absolute_error: 0.423574, mean_q: 2.285886
  29976/1000000: episode: 114, duration: 1.578s, episode steps: 195, steps per second: 124, episode reward: -11.734, mean reward: -0.060 [-11.337, 0.324], mean action: 0.019 [-1.059, 1.105], mean observation: -0.032 [-7.010, 3.979], loss: 0.236636, mean_absolute_error: 0.419218, mean_q: 2.247074
  30167/1000000: episode: 115, duration: 1.451s, episode steps: 191, steps per second: 132, episode reward: -8.514, mean reward: -0.045 [-11.217, 0.306], mean action: 0.001 [-1.077, 1.097], mean observation: -0.007 [-7.011, 3.993], loss: 0.276929, mean_absolute_error: 0.428257, mean_q: 2.202652
  30398/1000000: episode: 116, duration: 1.701s, episode steps: 231, steps per second: 136, episode reward: -63.258, mean reward: -0.274 [-12.139, 0.514], mean action: -0.005 [-1.132, 1.115], mean observation: -0.104 [-7.074, 3.897], loss: 0.273800, mean_absolute_error: 0.430859, mean_q: 2.236683
  30625/1000000: episode: 117, duration: 1.669s, episode steps: 227, steps per second: 136, episode reward: -50.586, mean reward: -0.223 [-11.813, 0.384], mean action: 0.006 [-1.091, 1.137], mean observation: -0.073 [-7.049, 3.983], loss: 0.272032, mean_absolute_error: 0.431413, mean_q: 2.204190
  30858/1000000: episode: 118, duration: 1.716s, episode steps: 233, steps per second: 136, episode reward: -34.627, mean reward: -0.149 [-12.003, 0.622], mean action: -0.012 [-1.126, 1.108], mean observation: -0.092 [-7.039, 4.080], loss: 0.254990, mean_absolute_error: 0.435809, mean_q: 2.233139
  31094/1000000: episode: 119, duration: 1.741s, episode steps: 236, steps per second: 136, episode reward: -85.128, mean reward: -0.361 [-12.177, 0.393], mean action: -0.007 [-1.095, 1.143], mean observation: -0.121 [-7.026, 3.989], loss: 0.256861, mean_absolute_error: 0.436755, mean_q: 2.222312
  31328/1000000: episode: 120, duration: 1.724s, episode steps: 234, steps per second: 136, episode reward: -126.682, mean reward: -0.541 [-12.141, 0.135], mean action: -0.014 [-1.095, 1.133], mean observation: -0.061 [-7.033, 4.029], loss: 0.291296, mean_absolute_error: 0.446444, mean_q: 2.197731
  31569/1000000: episode: 121, duration: 1.799s, episode steps: 241, steps per second: 134, episode reward: -38.032, mean reward: -0.158 [-11.872, 0.564], mean action: 0.002 [-1.080, 1.101], mean observation: -0.113 [-7.025, 3.978], loss: 0.277426, mean_absolute_error: 0.446720, mean_q: 2.198095
  31800/1000000: episode: 122, duration: 1.699s, episode steps: 231, steps per second: 136, episode reward: -100.017, mean reward: -0.433 [-12.011, 0.201], mean action: -0.010 [-1.076, 1.095], mean observation: -0.062 [-7.025, 3.923], loss: 0.274136, mean_absolute_error: 0.448587, mean_q: 2.176244
  32002/1000000: episode: 123, duration: 1.488s, episode steps: 202, steps per second: 136, episode reward: -36.907, mean reward: -0.183 [-11.355, 0.193], mean action: 0.100 [-1.106, 1.167], mean observation: -0.018 [-6.929, 4.203], loss: 0.272564, mean_absolute_error: 0.443854, mean_q: 2.191696
  32268/1000000: episode: 124, duration: 1.959s, episode steps: 266, steps per second: 136, episode reward: 9.526, mean reward: 0.036 [-10.836, 0.512], mean action: 0.295 [-1.097, 1.102], mean observation: 0.100 [-7.873, 6.032], loss: 0.273601, mean_absolute_error: 0.446180, mean_q: 2.197163
  32533/1000000: episode: 125, duration: 1.957s, episode steps: 265, steps per second: 135, episode reward: -38.140, mean reward: -0.144 [-11.168, 0.470], mean action: 0.299 [-1.035, 1.140], mean observation: 0.138 [-7.835, 6.019], loss: 0.241568, mean_absolute_error: 0.441624, mean_q: 2.202238
  32805/1000000: episode: 126, duration: 2.008s, episode steps: 272, steps per second: 135, episode reward: -0.651, mean reward: -0.002 [-10.840, 0.560], mean action: 0.330 [-1.035, 1.158], mean observation: 0.120 [-7.897, 6.007], loss: 0.259474, mean_absolute_error: 0.447437, mean_q: 2.208709
  33075/1000000: episode: 127, duration: 2.129s, episode steps: 270, steps per second: 127, episode reward: -49.234, mean reward: -0.182 [-10.954, 0.400], mean action: 0.356 [-1.085, 1.159], mean observation: 0.107 [-7.931, 6.048], loss: 0.285155, mean_absolute_error: 0.454537, mean_q: 2.221711
  33305/1000000: episode: 128, duration: 1.828s, episode steps: 230, steps per second: 126, episode reward: -45.732, mean reward: -0.199 [-11.406, 0.253], mean action: 0.393 [-1.079, 1.120], mean observation: 0.190 [-7.832, 5.978], loss: 0.246730, mean_absolute_error: 0.449127, mean_q: 2.186691
  33571/1000000: episode: 129, duration: 1.988s, episode steps: 266, steps per second: 134, episode reward: -50.542, mean reward: -0.190 [-10.643, 0.183], mean action: 0.326 [-1.102, 1.081], mean observation: 0.077 [-7.941, 6.037], loss: 0.313602, mean_absolute_error: 0.469174, mean_q: 2.216600
  33807/1000000: episode: 130, duration: 1.767s, episode steps: 236, steps per second: 134, episode reward: -2.614, mean reward: -0.011 [-10.814, 0.318], mean action: 0.394 [-1.162, 1.103], mean observation: 0.139 [-7.993, 6.069], loss: 0.262802, mean_absolute_error: 0.455684, mean_q: 2.191709
  34045/1000000: episode: 131, duration: 1.785s, episode steps: 238, steps per second: 133, episode reward: -9.130, mean reward: -0.038 [-10.806, 0.283], mean action: 0.437 [-1.183, 1.083], mean observation: 0.136 [-7.953, 6.040], loss: 0.263296, mean_absolute_error: 0.457582, mean_q: 2.216434
  34196/1000000: episode: 132, duration: 1.131s, episode steps: 151, steps per second: 133, episode reward: 9.199, mean reward: 0.061 [-10.095, 0.163], mean action: 0.510 [-1.049, 1.144], mean observation: 0.203 [-3.029, 5.951], loss: 0.251746, mean_absolute_error: 0.457988, mean_q: 2.179152
  34434/1000000: episode: 133, duration: 1.774s, episode steps: 238, steps per second: 134, episode reward: -35.428, mean reward: -0.149 [-11.346, 0.346], mean action: 0.449 [-1.055, 1.095], mean observation: 0.180 [-7.853, 5.999], loss: 0.268324, mean_absolute_error: 0.462091, mean_q: 2.201506
  34641/1000000: episode: 134, duration: 1.548s, episode steps: 207, steps per second: 134, episode reward: 25.250, mean reward: 0.122 [-10.817, 0.411], mean action: 0.479 [-1.045, 1.135], mean observation: 0.208 [-7.727, 6.063], loss: 0.268506, mean_absolute_error: 0.461197, mean_q: 2.176988
  34865/1000000: episode: 135, duration: 1.671s, episode steps: 224, steps per second: 134, episode reward: -20.525, mean reward: -0.092 [-11.139, 0.273], mean action: 0.478 [-1.071, 1.110], mean observation: 0.202 [-7.929, 6.037], loss: 0.250324, mean_absolute_error: 0.456442, mean_q: 2.201407
  35140/1000000: episode: 136, duration: 2.047s, episode steps: 275, steps per second: 134, episode reward: -44.897, mean reward: -0.163 [-10.458, 0.232], mean action: 0.483 [-1.103, 1.111], mean observation: 0.061 [-7.980, 6.043], loss: 0.266224, mean_absolute_error: 0.465715, mean_q: 2.158086
  35366/1000000: episode: 137, duration: 1.690s, episode steps: 226, steps per second: 134, episode reward: -19.052, mean reward: -0.084 [-11.074, 0.261], mean action: 0.479 [-1.077, 1.115], mean observation: 0.197 [-7.942, 6.007], loss: 0.273165, mean_absolute_error: 0.463470, mean_q: 2.234140
  35590/1000000: episode: 138, duration: 1.696s, episode steps: 224, steps per second: 132, episode reward: -4.820, mean reward: -0.022 [-10.977, 0.299], mean action: 0.467 [-1.119, 1.088], mean observation: 0.188 [-7.961, 6.020], loss: 0.280399, mean_absolute_error: 0.466961, mean_q: 2.198836
  35839/1000000: episode: 139, duration: 1.874s, episode steps: 249, steps per second: 133, episode reward: -14.031, mean reward: -0.056 [-11.157, 0.473], mean action: 0.490 [-1.063, 1.118], mean observation: 0.154 [-7.906, 5.990], loss: 0.275499, mean_absolute_error: 0.468768, mean_q: 2.178125
  36109/1000000: episode: 140, duration: 2.020s, episode steps: 270, steps per second: 134, episode reward: -34.343, mean reward: -0.127 [-10.864, 0.493], mean action: 0.509 [-1.087, 1.135], mean observation: 0.112 [-8.016, 6.085], loss: 0.277691, mean_absolute_error: 0.473195, mean_q: 2.140045
  36331/1000000: episode: 141, duration: 1.660s, episode steps: 222, steps per second: 134, episode reward: -17.536, mean reward: -0.079 [-11.217, 0.324], mean action: 0.504 [-1.085, 1.132], mean observation: 0.183 [-8.002, 6.070], loss: 0.284076, mean_absolute_error: 0.477648, mean_q: 2.127729
  36568/1000000: episode: 142, duration: 1.773s, episode steps: 237, steps per second: 134, episode reward: -75.522, mean reward: -0.319 [-11.391, 0.174], mean action: 0.484 [-1.143, 1.101], mean observation: 0.160 [-8.022, 6.101], loss: 0.286150, mean_absolute_error: 0.479529, mean_q: 2.166110
  36804/1000000: episode: 143, duration: 1.762s, episode steps: 236, steps per second: 134, episode reward: 9.101, mean reward: 0.039 [-10.821, 0.386], mean action: 0.505 [-1.156, 1.127], mean observation: 0.143 [-8.004, 6.071], loss: 0.287037, mean_absolute_error: 0.484209, mean_q: 2.119672
  37081/1000000: episode: 144, duration: 2.063s, episode steps: 277, steps per second: 134, episode reward: -18.647, mean reward: -0.067 [-10.484, 0.379], mean action: 0.516 [-1.039, 1.135], mean observation: 0.094 [-7.969, 6.032], loss: 0.278309, mean_absolute_error: 0.477424, mean_q: 2.155195
  37349/1000000: episode: 145, duration: 1.993s, episode steps: 268, steps per second: 134, episode reward: -11.782, mean reward: -0.044 [-10.441, 0.341], mean action: 0.478 [-1.108, 1.155], mean observation: 0.084 [-7.948, 6.105], loss: 0.282451, mean_absolute_error: 0.483371, mean_q: 2.092270
  37622/1000000: episode: 146, duration: 2.214s, episode steps: 273, steps per second: 123, episode reward: -37.824, mean reward: -0.139 [-10.937, 0.471], mean action: 0.489 [-1.060, 1.116], mean observation: 0.119 [-7.882, 5.993], loss: 0.293534, mean_absolute_error: 0.487955, mean_q: 2.131536
  37878/1000000: episode: 147, duration: 2.104s, episode steps: 256, steps per second: 122, episode reward: -21.393, mean reward: -0.084 [-10.884, 0.375], mean action: 0.518 [-1.052, 1.186], mean observation: 0.101 [-8.004, 6.038], loss: 0.272778, mean_absolute_error: 0.484652, mean_q: 2.126463
  38080/1000000: episode: 148, duration: 1.708s, episode steps: 202, steps per second: 118, episode reward: 19.958, mean reward: 0.099 [-10.816, 0.377], mean action: 0.488 [-1.025, 1.105], mean observation: 0.216 [-7.192, 6.012], loss: 0.297309, mean_absolute_error: 0.491830, mean_q: 2.092440
  38211/1000000: episode: 149, duration: 0.981s, episode steps: 131, steps per second: 134, episode reward: -2.574, mean reward: -0.020 [-10.079, 0.082], mean action: 0.497 [-1.048, 1.091], mean observation: 0.247 [-1.699, 5.143], loss: 0.279737, mean_absolute_error: 0.493094, mean_q: 2.011638
  38464/1000000: episode: 150, duration: 1.913s, episode steps: 253, steps per second: 132, episode reward: -30.206, mean reward: -0.119 [-11.034, 0.370], mean action: 0.467 [-1.134, 1.085], mean observation: 0.145 [-7.945, 6.049], loss: 0.301412, mean_absolute_error: 0.491591, mean_q: 2.115116
  38713/1000000: episode: 151, duration: 1.844s, episode steps: 249, steps per second: 135, episode reward: -37.173, mean reward: -0.149 [-11.287, 0.406], mean action: 0.497 [-1.062, 1.117], mean observation: 0.162 [-7.895, 6.033], loss: 0.301170, mean_absolute_error: 0.496356, mean_q: 2.109227
  38961/1000000: episode: 152, duration: 1.840s, episode steps: 248, steps per second: 135, episode reward: 17.705, mean reward: 0.071 [-10.996, 0.596], mean action: 0.486 [-1.124, 1.103], mean observation: 0.138 [-7.959, 6.073], loss: 0.285599, mean_absolute_error: 0.492688, mean_q: 2.029379
  39142/1000000: episode: 153, duration: 1.339s, episode steps: 181, steps per second: 135, episode reward: 23.288, mean reward: 0.129 [-10.404, 0.297], mean action: 0.497 [-1.087, 1.125], mean observation: 0.219 [-5.723, 6.029], loss: 0.287317, mean_absolute_error: 0.492322, mean_q: 2.082351
  39312/1000000: episode: 154, duration: 1.266s, episode steps: 170, steps per second: 134, episode reward: 7.180, mean reward: 0.042 [-10.404, 0.200], mean action: 0.487 [-1.056, 1.091], mean observation: 0.224 [-4.668, 6.026], loss: 0.266067, mean_absolute_error: 0.491651, mean_q: 2.081496
  39560/1000000: episode: 155, duration: 1.841s, episode steps: 248, steps per second: 135, episode reward: -15.595, mean reward: -0.063 [-10.985, 0.373], mean action: 0.488 [-1.137, 1.134], mean observation: 0.117 [-7.924, 6.050], loss: 0.302128, mean_absolute_error: 0.495099, mean_q: 2.078789
  39715/1000000: episode: 156, duration: 1.144s, episode steps: 155, steps per second: 136, episode reward: 22.837, mean reward: 0.147 [-10.125, 0.276], mean action: 0.494 [-1.027, 1.070], mean observation: 0.241 [-3.202, 5.941], loss: 0.297884, mean_absolute_error: 0.500133, mean_q: 2.039341
  39986/1000000: episode: 157, duration: 1.999s, episode steps: 271, steps per second: 136, episode reward: -8.868, mean reward: -0.033 [-10.886, 0.607], mean action: 0.443 [-1.081, 1.073], mean observation: 0.104 [-7.860, 6.050], loss: 0.278454, mean_absolute_error: 0.497632, mean_q: 2.088990
  40242/1000000: episode: 158, duration: 1.891s, episode steps: 256, steps per second: 135, episode reward: -14.552, mean reward: -0.057 [-10.726, 0.288], mean action: 0.395 [-1.041, 1.088], mean observation: 0.101 [-7.812, 5.993], loss: 0.294745, mean_absolute_error: 0.499851, mean_q: 2.029994
  40509/1000000: episode: 159, duration: 1.994s, episode steps: 267, steps per second: 134, episode reward: 2.401, mean reward: 0.009 [-11.014, 0.621], mean action: 0.412 [-1.103, 1.097], mean observation: 0.121 [-7.789, 5.967], loss: 0.277293, mean_absolute_error: 0.501993, mean_q: 2.050484
  40665/1000000: episode: 160, duration: 1.166s, episode steps: 156, steps per second: 134, episode reward: 7.462, mean reward: 0.048 [-10.152, 0.156], mean action: 0.518 [-1.008, 1.097], mean observation: 0.212 [-3.275, 5.947], loss: 0.319154, mean_absolute_error: 0.505860, mean_q: 2.029110
  40920/1000000: episode: 161, duration: 1.921s, episode steps: 255, steps per second: 133, episode reward: -12.765, mean reward: -0.050 [-11.221, 0.516], mean action: 0.372 [-1.049, 1.122], mean observation: 0.152 [-7.842, 5.975], loss: 0.311885, mean_absolute_error: 0.509464, mean_q: 2.024213
  41180/1000000: episode: 162, duration: 1.924s, episode steps: 260, steps per second: 135, episode reward: 7.180, mean reward: 0.028 [-10.795, 0.433], mean action: 0.348 [-1.098, 1.168], mean observation: 0.103 [-7.782, 5.954], loss: 0.301720, mean_absolute_error: 0.505380, mean_q: 2.030523
  41421/1000000: episode: 163, duration: 1.784s, episode steps: 241, steps per second: 135, episode reward: -8.022, mean reward: -0.033 [-11.047, 0.378], mean action: 0.356 [-1.072, 1.132], mean observation: 0.161 [-7.880, 6.046], loss: 0.278862, mean_absolute_error: 0.497121, mean_q: 2.084751
  41594/1000000: episode: 164, duration: 1.276s, episode steps: 173, steps per second: 136, episode reward: 17.356, mean reward: 0.100 [-10.312, 0.245], mean action: 0.457 [-1.062, 1.128], mean observation: 0.240 [-4.834, 5.993], loss: 0.322516, mean_absolute_error: 0.515680, mean_q: 2.009601
  41873/1000000: episode: 165, duration: 2.064s, episode steps: 279, steps per second: 135, episode reward: -66.082, mean reward: -0.237 [-10.653, 0.189], mean action: 0.319 [-1.126, 1.163], mean observation: 0.044 [-7.813, 6.030], loss: 0.300188, mean_absolute_error: 0.506515, mean_q: 2.014360
  42124/1000000: episode: 166, duration: 1.882s, episode steps: 251, steps per second: 133, episode reward: 2.728, mean reward: 0.011 [-10.942, 0.431], mean action: 0.337 [-1.094, 1.075], mean observation: 0.140 [-7.835, 6.008], loss: 0.314091, mean_absolute_error: 0.512094, mean_q: 1.946301
  42362/1000000: episode: 167, duration: 1.765s, episode steps: 238, steps per second: 135, episode reward: 22.359, mean reward: 0.094 [-11.014, 0.501], mean action: 0.376 [-1.043, 1.093], mean observation: 0.156 [-7.769, 5.975], loss: 0.257219, mean_absolute_error: 0.499942, mean_q: 2.015218
  42635/1000000: episode: 168, duration: 2.018s, episode steps: 273, steps per second: 135, episode reward: -41.850, mean reward: -0.153 [-10.984, 0.459], mean action: 0.257 [-1.062, 1.078], mean observation: 0.108 [-7.817, 6.019], loss: 0.294191, mean_absolute_error: 0.509656, mean_q: 2.028827
  42880/1000000: episode: 169, duration: 1.840s, episode steps: 245, steps per second: 133, episode reward: -156.795, mean reward: -0.640 [-12.535, 0.194], mean action: 0.008 [-1.136, 1.120], mean observation: 0.209 [-6.951, 5.204], loss: 0.328560, mean_absolute_error: 0.521632, mean_q: 1.974492
  43041/1000000: episode: 170, duration: 1.201s, episode steps: 161, steps per second: 134, episode reward: -12.259, mean reward: -0.076 [-10.606, 0.092], mean action: 0.010 [-1.085, 1.087], mean observation: 0.241 [-5.869, 5.294], loss: 0.313152, mean_absolute_error: 0.519814, mean_q: 1.930859
  43238/1000000: episode: 171, duration: 1.464s, episode steps: 197, steps per second: 135, episode reward: 1.130, mean reward: 0.006 [-11.215, 0.360], mean action: -0.024 [-1.040, 1.107], mean observation: 0.230 [-6.905, 5.239], loss: 0.301824, mean_absolute_error: 0.516974, mean_q: 2.010761
  43466/1000000: episode: 172, duration: 1.693s, episode steps: 228, steps per second: 135, episode reward: -37.073, mean reward: -0.163 [-11.779, 0.415], mean action: -0.143 [-1.101, 1.069], mean observation: 0.183 [-6.978, 5.335], loss: 0.317609, mean_absolute_error: 0.520672, mean_q: 1.971323
  43716/1000000: episode: 173, duration: 1.842s, episode steps: 250, steps per second: 136, episode reward: -94.767, mean reward: -0.379 [-12.305, 0.488], mean action: -0.164 [-1.129, 1.081], mean observation: 0.191 [-6.957, 5.186], loss: 0.325596, mean_absolute_error: 0.528094, mean_q: 1.933600
  43987/1000000: episode: 174, duration: 2.020s, episode steps: 271, steps per second: 134, episode reward: -60.555, mean reward: -0.223 [-11.519, 0.498], mean action: 0.216 [-1.092, 1.097], mean observation: 0.140 [-7.599, 5.572], loss: 0.289632, mean_absolute_error: 0.521918, mean_q: 1.929075
  44259/1000000: episode: 175, duration: 2.025s, episode steps: 272, steps per second: 134, episode reward: -58.951, mean reward: -0.217 [-10.922, 0.319], mean action: 0.394 [-1.078, 1.125], mean observation: 0.075 [-7.859, 6.002], loss: 0.289176, mean_absolute_error: 0.518864, mean_q: 1.946987
  44405/1000000: episode: 176, duration: 1.091s, episode steps: 146, steps per second: 134, episode reward: 0.230, mean reward: 0.002 [-10.177, 0.114], mean action: 0.503 [-1.085, 1.107], mean observation: 0.254 [-2.841, 5.911], loss: 0.283632, mean_absolute_error: 0.515699, mean_q: 1.980127
  44643/1000000: episode: 177, duration: 1.777s, episode steps: 238, steps per second: 134, episode reward: -32.965, mean reward: -0.139 [-11.274, 0.334], mean action: 0.482 [-1.109, 1.136], mean observation: 0.181 [-7.899, 6.039], loss: 0.324215, mean_absolute_error: 0.534465, mean_q: 1.999463
  44885/1000000: episode: 178, duration: 1.787s, episode steps: 242, steps per second: 135, episode reward: -40.647, mean reward: -0.168 [-11.365, 0.347], mean action: 0.465 [-1.060, 1.167], mean observation: 0.167 [-7.876, 6.008], loss: 0.303979, mean_absolute_error: 0.526944, mean_q: 1.877745
  45157/1000000: episode: 179, duration: 2.012s, episode steps: 272, steps per second: 135, episode reward: -47.983, mean reward: -0.176 [-10.864, 0.390], mean action: 0.426 [-1.152, 1.113], mean observation: 0.085 [-7.955, 6.050], loss: 0.298294, mean_absolute_error: 0.528280, mean_q: 1.892373
  45324/1000000: episode: 180, duration: 1.254s, episode steps: 167, steps per second: 133, episode reward: -7.923, mean reward: -0.047 [-10.506, 0.118], mean action: 0.456 [-1.106, 1.076], mean observation: 0.269 [-4.418, 6.034], loss: 0.272070, mean_absolute_error: 0.525660, mean_q: 1.889055
  45530/1000000: episode: 181, duration: 1.535s, episode steps: 206, steps per second: 134, episode reward: 11.338, mean reward: 0.055 [-10.966, 0.380], mean action: 0.434 [-1.084, 1.145], mean observation: 0.215 [-7.405, 6.044], loss: 0.325554, mean_absolute_error: 0.540584, mean_q: 1.957761
  45769/1000000: episode: 182, duration: 1.767s, episode steps: 239, steps per second: 135, episode reward: -73.781, mean reward: -0.309 [-11.504, 0.194], mean action: 0.426 [-1.091, 1.101], mean observation: 0.197 [-7.840, 6.021], loss: 0.296348, mean_absolute_error: 0.533280, mean_q: 1.904852
  46034/1000000: episode: 183, duration: 2.009s, episode steps: 265, steps per second: 132, episode reward: 2.639, mean reward: 0.010 [-10.961, 0.620], mean action: 0.443 [-1.058, 1.117], mean observation: 0.106 [-7.837, 6.036], loss: 0.303317, mean_absolute_error: 0.530678, mean_q: 1.826640
  46274/1000000: episode: 184, duration: 1.779s, episode steps: 240, steps per second: 135, episode reward: -31.110, mean reward: -0.130 [-11.084, 0.270], mean action: 0.457 [-1.099, 1.157], mean observation: 0.168 [-7.899, 6.042], loss: 0.302899, mean_absolute_error: 0.540193, mean_q: 1.837360
  46462/1000000: episode: 185, duration: 1.384s, episode steps: 188, steps per second: 136, episode reward: -35.344, mean reward: -0.188 [-10.954, 0.051], mean action: 0.493 [-1.072, 1.107], mean observation: 0.228 [-6.386, 6.062], loss: 0.328204, mean_absolute_error: 0.543635, mean_q: 1.857084
  46725/1000000: episode: 186, duration: 1.941s, episode steps: 263, steps per second: 135, episode reward: 15.381, mean reward: 0.058 [-10.751, 0.593], mean action: 0.496 [-1.093, 1.152], mean observation: 0.093 [-7.923, 6.095], loss: 0.326103, mean_absolute_error: 0.539897, mean_q: 1.844342
  46978/1000000: episode: 187, duration: 1.873s, episode steps: 253, steps per second: 135, episode reward: 0.594, mean reward: 0.002 [-11.121, 0.539], mean action: 0.466 [-1.040, 1.084], mean observation: 0.147 [-7.783, 5.970], loss: 0.323045, mean_absolute_error: 0.543629, mean_q: 1.813379
  47133/1000000: episode: 188, duration: 1.143s, episode steps: 155, steps per second: 136, episode reward: 6.345, mean reward: 0.041 [-10.240, 0.171], mean action: 0.499 [-1.045, 1.115], mean observation: 0.260 [-3.436, 6.007], loss: 0.297351, mean_absolute_error: 0.539922, mean_q: 1.822963
  47387/1000000: episode: 189, duration: 1.889s, episode steps: 254, steps per second: 134, episode reward: -45.387, mean reward: -0.179 [-11.231, 0.337], mean action: 0.492 [-1.029, 1.140], mean observation: 0.163 [-7.783, 5.975], loss: 0.321676, mean_absolute_error: 0.547712, mean_q: 1.850076
  47630/1000000: episode: 190, duration: 1.804s, episode steps: 243, steps per second: 135, episode reward: 13.196, mean reward: 0.054 [-11.057, 0.523], mean action: 0.474 [-1.058, 1.089], mean observation: 0.160 [-7.882, 6.016], loss: 0.336586, mean_absolute_error: 0.550680, mean_q: 1.833214
  47872/1000000: episode: 191, duration: 1.819s, episode steps: 242, steps per second: 133, episode reward: -61.891, mean reward: -0.256 [-11.439, 0.248], mean action: 0.498 [-1.052, 1.112], mean observation: 0.164 [-7.863, 5.991], loss: 0.314894, mean_absolute_error: 0.546674, mean_q: 1.777917
  48078/1000000: episode: 192, duration: 1.540s, episode steps: 206, steps per second: 134, episode reward: 8.052, mean reward: 0.039 [-10.834, 0.306], mean action: 0.478 [-1.050, 1.143], mean observation: 0.195 [-7.598, 6.043], loss: 0.333416, mean_absolute_error: 0.554125, mean_q: 1.868770
  48309/1000000: episode: 193, duration: 1.738s, episode steps: 231, steps per second: 133, episode reward: -24.771, mean reward: -0.107 [-11.292, 0.380], mean action: 0.454 [-1.184, 1.085], mean observation: 0.174 [-7.948, 6.083], loss: 0.347547, mean_absolute_error: 0.563247, mean_q: 1.748003
  48547/1000000: episode: 194, duration: 1.783s, episode steps: 238, steps per second: 133, episode reward: -36.482, mean reward: -0.153 [-11.020, 0.208], mean action: 0.486 [-1.052, 1.125], mean observation: 0.166 [-7.869, 6.037], loss: 0.348657, mean_absolute_error: 0.560754, mean_q: 1.749133
  48784/1000000: episode: 195, duration: 1.792s, episode steps: 237, steps per second: 132, episode reward: -49.218, mean reward: -0.208 [-10.979, 0.115], mean action: 0.481 [-1.072, 1.151], mean observation: 0.124 [-7.891, 6.025], loss: 0.319375, mean_absolute_error: 0.553496, mean_q: 1.779510
  48969/1000000: episode: 196, duration: 1.370s, episode steps: 185, steps per second: 135, episode reward: 1.490, mean reward: 0.008 [-10.734, 0.240], mean action: 0.486 [-1.078, 1.111], mean observation: 0.233 [-6.184, 6.064], loss: 0.342870, mean_absolute_error: 0.564413, mean_q: 1.742659
  49186/1000000: episode: 197, duration: 1.622s, episode steps: 217, steps per second: 134, episode reward: 5.076, mean reward: 0.023 [-10.828, 0.298], mean action: 0.426 [-1.160, 1.166], mean observation: 0.185 [-7.759, 6.047], loss: 0.354613, mean_absolute_error: 0.567667, mean_q: 1.800171
  49434/1000000: episode: 198, duration: 1.847s, episode steps: 248, steps per second: 134, episode reward: -10.418, mean reward: -0.042 [-11.215, 0.489], mean action: 0.457 [-1.102, 1.126], mean observation: 0.150 [-7.810, 6.002], loss: 0.341954, mean_absolute_error: 0.567509, mean_q: 1.711651
  49675/1000000: episode: 199, duration: 1.793s, episode steps: 241, steps per second: 134, episode reward: -78.921, mean reward: -0.327 [-11.482, 0.166], mean action: 0.424 [-1.023, 1.090], mean observation: 0.203 [-7.815, 6.005], loss: 0.329693, mean_absolute_error: 0.560615, mean_q: 1.736979
  49885/1000000: episode: 200, duration: 1.550s, episode steps: 210, steps per second: 136, episode reward: 22.559, mean reward: 0.107 [-10.890, 0.418], mean action: 0.469 [-1.097, 1.139], mean observation: 0.199 [-7.698, 6.065], loss: 0.360936, mean_absolute_error: 0.567366, mean_q: 1.730087
  50150/1000000: episode: 201, duration: 1.970s, episode steps: 265, steps per second: 135, episode reward: -28.244, mean reward: -0.107 [-11.121, 0.528], mean action: 0.325 [-1.093, 1.081], mean observation: 0.122 [-7.833, 6.012], loss: 0.348065, mean_absolute_error: 0.565843, mean_q: 1.687524
  50369/1000000: episode: 202, duration: 1.631s, episode steps: 219, steps per second: 134, episode reward: -26.642, mean reward: -0.122 [-10.895, 0.142], mean action: 0.391 [-1.144, 1.107], mean observation: 0.160 [-7.912, 6.065], loss: 0.369159, mean_absolute_error: 0.579566, mean_q: 1.694685
  50629/1000000: episode: 203, duration: 1.938s, episode steps: 260, steps per second: 134, episode reward: -4.490, mean reward: -0.017 [-10.875, 0.478], mean action: 0.353 [-1.072, 1.138], mean observation: 0.117 [-7.868, 6.024], loss: 0.355122, mean_absolute_error: 0.573623, mean_q: 1.642108
  50891/1000000: episode: 204, duration: 1.950s, episode steps: 262, steps per second: 134, episode reward: -8.592, mean reward: -0.033 [-11.110, 0.590], mean action: 0.313 [-1.079, 1.091], mean observation: 0.119 [-7.844, 6.021], loss: 0.350053, mean_absolute_error: 0.577134, mean_q: 1.595945
  51150/1000000: episode: 205, duration: 1.923s, episode steps: 259, steps per second: 135, episode reward: 13.360, mean reward: 0.052 [-10.782, 0.446], mean action: 0.306 [-1.084, 1.091], mean observation: 0.112 [-7.837, 5.975], loss: 0.339142, mean_absolute_error: 0.573685, mean_q: 1.720192
  51370/1000000: episode: 206, duration: 1.650s, episode steps: 220, steps per second: 133, episode reward: 23.536, mean reward: 0.107 [-10.895, 0.430], mean action: 0.388 [-1.122, 1.175], mean observation: 0.186 [-7.959, 6.053], loss: 0.376342, mean_absolute_error: 0.590459, mean_q: 1.631909
  51635/1000000: episode: 207, duration: 2.224s, episode steps: 265, steps per second: 119, episode reward: -45.603, mean reward: -0.172 [-11.096, 0.368], mean action: 0.390 [-1.026, 1.070], mean observation: 0.134 [-7.810, 5.951], loss: 0.348006, mean_absolute_error: 0.584114, mean_q: 1.604095
  51907/1000000: episode: 208, duration: 2.053s, episode steps: 272, steps per second: 133, episode reward: -0.654, mean reward: -0.002 [-10.597, 0.444], mean action: 0.321 [-1.108, 1.076], mean observation: 0.063 [-7.930, 6.029], loss: 0.361897, mean_absolute_error: 0.581902, mean_q: 1.688910
  52159/1000000: episode: 209, duration: 1.919s, episode steps: 252, steps per second: 131, episode reward: -52.565, mean reward: -0.209 [-11.334, 0.358], mean action: 0.334 [-1.112, 1.102], mean observation: 0.144 [-7.826, 6.000], loss: 0.348157, mean_absolute_error: 0.577026, mean_q: 1.672894
  52385/1000000: episode: 210, duration: 1.699s, episode steps: 226, steps per second: 133, episode reward: -19.322, mean reward: -0.085 [-10.878, 0.186], mean action: 0.371 [-1.061, 1.147], mean observation: 0.167 [-7.796, 5.982], loss: 0.363732, mean_absolute_error: 0.587465, mean_q: 1.599589
  52516/1000000: episode: 211, duration: 0.991s, episode steps: 131, steps per second: 132, episode reward: 13.158, mean reward: 0.100 [-9.952, 0.200], mean action: 0.500 [-1.004, 1.070], mean observation: 0.213 [-1.716, 5.155], loss: 0.334968, mean_absolute_error: 0.581468, mean_q: 1.555028
  52749/1000000: episode: 212, duration: 1.769s, episode steps: 233, steps per second: 132, episode reward: -25.306, mean reward: -0.109 [-11.182, 0.292], mean action: 0.376 [-1.018, 1.123], mean observation: 0.187 [-7.846, 5.997], loss: 0.374720, mean_absolute_error: 0.588459, mean_q: 1.595712
  52977/1000000: episode: 213, duration: 1.705s, episode steps: 228, steps per second: 134, episode reward: 24.221, mean reward: 0.106 [-11.027, 0.490], mean action: 0.369 [-1.064, 1.106], mean observation: 0.180 [-7.861, 6.001], loss: 0.378712, mean_absolute_error: 0.583669, mean_q: 1.562482
  53203/1000000: episode: 214, duration: 1.707s, episode steps: 226, steps per second: 132, episode reward: -9.859, mean reward: -0.044 [-10.929, 0.257], mean action: 0.435 [-1.081, 1.076], mean observation: 0.160 [-7.903, 6.018], loss: 0.346073, mean_absolute_error: 0.586568, mean_q: 1.594044
  53373/1000000: episode: 215, duration: 1.295s, episode steps: 170, steps per second: 131, episode reward: -13.336, mean reward: -0.078 [-10.506, 0.079], mean action: 0.476 [-1.071, 1.075], mean observation: 0.232 [-4.119, 5.895], loss: 0.374896, mean_absolute_error: 0.584983, mean_q: 1.490304
  53616/1000000: episode: 216, duration: 1.820s, episode steps: 243, steps per second: 134, episode reward: -33.862, mean reward: -0.139 [-10.933, 0.206], mean action: 0.438 [-1.091, 1.082], mean observation: 0.150 [-7.854, 5.981], loss: 0.349605, mean_absolute_error: 0.581431, mean_q: 1.596133
  53865/1000000: episode: 217, duration: 1.834s, episode steps: 249, steps per second: 136, episode reward: 17.434, mean reward: 0.070 [-11.016, 0.569], mean action: 0.343 [-1.064, 1.096], mean observation: 0.138 [-7.923, 6.004], loss: 0.367589, mean_absolute_error: 0.594019, mean_q: 1.532737
  54115/1000000: episode: 218, duration: 1.860s, episode steps: 250, steps per second: 134, episode reward: -50.410, mean reward: -0.202 [-10.962, 0.194], mean action: 0.445 [-1.063, 1.121], mean observation: 0.104 [-7.956, 6.047], loss: 0.382145, mean_absolute_error: 0.600261, mean_q: 1.600891
  54332/1000000: episode: 219, duration: 1.629s, episode steps: 217, steps per second: 133, episode reward: -62.235, mean reward: -0.287 [-11.397, 0.100], mean action: 0.462 [-1.161, 1.140], mean observation: 0.198 [-8.029, 6.068], loss: 0.380982, mean_absolute_error: 0.605869, mean_q: 1.600404
  54583/1000000: episode: 220, duration: 1.858s, episode steps: 251, steps per second: 135, episode reward: -63.335, mean reward: -0.252 [-10.932, 0.096], mean action: 0.408 [-1.156, 1.108], mean observation: 0.098 [-7.928, 5.988], loss: 0.407296, mean_absolute_error: 0.603514, mean_q: 1.569304
  54843/1000000: episode: 221, duration: 1.919s, episode steps: 260, steps per second: 136, episode reward: 1.245, mean reward: 0.005 [-10.680, 0.382], mean action: 0.310 [-1.091, 1.085], mean observation: 0.100 [-7.912, 6.010], loss: 0.401681, mean_absolute_error: 0.614413, mean_q: 1.438111
  55027/1000000: episode: 222, duration: 1.362s, episode steps: 184, steps per second: 135, episode reward: 1.643, mean reward: 0.009 [-10.688, 0.227], mean action: 0.469 [-0.973, 1.149], mean observation: 0.251 [-5.775, 6.003], loss: 0.382300, mean_absolute_error: 0.602552, mean_q: 1.494706
  55224/1000000: episode: 223, duration: 1.450s, episode steps: 197, steps per second: 136, episode reward: -26.666, mean reward: -0.135 [-11.088, 0.164], mean action: 0.397 [-1.128, 1.169], mean observation: 0.238 [-7.219, 6.060], loss: 0.416642, mean_absolute_error: 0.613716, mean_q: 1.380252
  55438/1000000: episode: 224, duration: 1.579s, episode steps: 214, steps per second: 136, episode reward: -0.079, mean reward: -0.000 [-10.856, 0.265], mean action: 0.398 [-1.093, 1.099], mean observation: 0.179 [-7.811, 6.014], loss: 0.404039, mean_absolute_error: 0.615429, mean_q: 1.475703
  55650/1000000: episode: 225, duration: 1.576s, episode steps: 212, steps per second: 135, episode reward: -0.688, mean reward: -0.003 [-11.033, 0.325], mean action: 0.446 [-1.053, 1.104], mean observation: 0.223 [-7.732, 6.021], loss: 0.412696, mean_absolute_error: 0.614351, mean_q: 1.414353
  55915/1000000: episode: 226, duration: 1.972s, episode steps: 265, steps per second: 134, episode reward: -15.695, mean reward: -0.059 [-10.967, 0.557], mean action: 0.320 [-1.124, 1.118], mean observation: 0.101 [-8.014, 6.073], loss: 0.417747, mean_absolute_error: 0.620253, mean_q: 1.471548
  56138/1000000: episode: 227, duration: 1.644s, episode steps: 223, steps per second: 136, episode reward: 12.491, mean reward: 0.056 [-10.869, 0.370], mean action: 0.419 [-1.122, 1.121], mean observation: 0.173 [-7.992, 6.058], loss: 0.385938, mean_absolute_error: 0.607062, mean_q: 1.402076
  56382/1000000: episode: 228, duration: 1.807s, episode steps: 244, steps per second: 135, episode reward: -34.093, mean reward: -0.140 [-11.277, 0.379], mean action: 0.351 [-1.113, 1.082], mean observation: 0.144 [-7.913, 6.035], loss: 0.385702, mean_absolute_error: 0.611401, mean_q: 1.520821
  56653/1000000: episode: 229, duration: 1.996s, episode steps: 271, steps per second: 136, episode reward: -0.160, mean reward: -0.001 [-10.751, 0.490], mean action: 0.293 [-1.071, 1.130], mean observation: 0.071 [-7.921, 6.053], loss: 0.386856, mean_absolute_error: 0.610167, mean_q: 1.486900
  56861/1000000: episode: 230, duration: 1.539s, episode steps: 208, steps per second: 135, episode reward: -29.612, mean reward: -0.142 [-11.152, 0.175], mean action: 0.381 [-1.044, 1.114], mean observation: 0.204 [-7.515, 5.985], loss: 0.388072, mean_absolute_error: 0.613559, mean_q: 1.347933
  57116/1000000: episode: 231, duration: 1.885s, episode steps: 255, steps per second: 135, episode reward: -50.007, mean reward: -0.196 [-11.249, 0.398], mean action: 0.308 [-1.083, 1.110], mean observation: 0.132 [-7.986, 6.063], loss: 0.405596, mean_absolute_error: 0.609856, mean_q: 1.452696
  57351/1000000: episode: 232, duration: 1.739s, episode steps: 235, steps per second: 135, episode reward: -56.092, mean reward: -0.239 [-11.417, 0.249], mean action: 0.301 [-1.162, 1.069], mean observation: 0.176 [-7.971, 6.040], loss: 0.421515, mean_absolute_error: 0.626066, mean_q: 1.315673
  57549/1000000: episode: 233, duration: 1.457s, episode steps: 198, steps per second: 136, episode reward: 26.512, mean reward: 0.134 [-10.734, 0.394], mean action: 0.365 [-1.092, 1.142], mean observation: 0.216 [-7.087, 6.011], loss: 0.426303, mean_absolute_error: 0.630603, mean_q: 1.357725
  57763/1000000: episode: 234, duration: 1.585s, episode steps: 214, steps per second: 135, episode reward: 25.528, mean reward: 0.119 [-10.867, 0.421], mean action: 0.362 [-1.063, 1.146], mean observation: 0.201 [-7.843, 6.007], loss: 0.410848, mean_absolute_error: 0.629846, mean_q: 1.353639
  57981/1000000: episode: 235, duration: 1.611s, episode steps: 218, steps per second: 135, episode reward: 7.948, mean reward: 0.036 [-11.004, 0.376], mean action: 0.361 [-1.114, 1.117], mean observation: 0.204 [-7.978, 6.053], loss: 0.408369, mean_absolute_error: 0.620286, mean_q: 1.486464
  58253/1000000: episode: 236, duration: 2.028s, episode steps: 272, steps per second: 134, episode reward: -37.902, mean reward: -0.139 [-10.661, 0.283], mean action: 0.288 [-1.122, 1.134], mean observation: 0.053 [-8.012, 6.084], loss: 0.423124, mean_absolute_error: 0.622542, mean_q: 1.441414
  58402/1000000: episode: 237, duration: 1.100s, episode steps: 149, steps per second: 135, episode reward: -8.939, mean reward: -0.060 [-10.213, 0.043], mean action: 0.485 [-1.115, 1.073], mean observation: 0.240 [-3.064, 5.983], loss: 0.431882, mean_absolute_error: 0.628181, mean_q: 1.381226
  58666/1000000: episode: 238, duration: 1.952s, episode steps: 264, steps per second: 135, episode reward: -43.572, mean reward: -0.165 [-10.688, 0.204], mean action: 0.291 [-1.070, 1.089], mean observation: 0.097 [-7.963, 6.031], loss: 0.411355, mean_absolute_error: 0.620158, mean_q: 1.335405
  58785/1000000: episode: 239, duration: 0.881s, episode steps: 119, steps per second: 135, episode reward: 9.418, mean reward: 0.079 [-9.922, 0.179], mean action: 0.490 [-1.049, 1.038], mean observation: 0.209 [-1.266, 4.626], loss: 0.430591, mean_absolute_error: 0.625696, mean_q: 1.381710
  59024/1000000: episode: 240, duration: 1.788s, episode steps: 239, steps per second: 134, episode reward: 1.327, mean reward: 0.006 [-10.889, 0.359], mean action: 0.317 [-1.092, 1.126], mean observation: 0.148 [-7.994, 6.031], loss: 0.420543, mean_absolute_error: 0.632329, mean_q: 1.379500
  59280/1000000: episode: 241, duration: 1.887s, episode steps: 256, steps per second: 136, episode reward: -44.839, mean reward: -0.175 [-11.315, 0.405], mean action: 0.279 [-1.122, 1.058], mean observation: 0.145 [-7.869, 5.966], loss: 0.461336, mean_absolute_error: 0.639881, mean_q: 1.375245
  59486/1000000: episode: 242, duration: 1.521s, episode steps: 206, steps per second: 135, episode reward: 1.479, mean reward: 0.007 [-11.011, 0.325], mean action: 0.344 [-1.087, 1.113], mean observation: 0.225 [-7.530, 6.005], loss: 0.396633, mean_absolute_error: 0.626679, mean_q: 1.313011
  59695/1000000: episode: 243, duration: 1.550s, episode steps: 209, steps per second: 135, episode reward: 7.660, mean reward: 0.037 [-10.958, 0.351], mean action: 0.343 [-1.108, 1.095], mean observation: 0.212 [-7.730, 6.013], loss: 0.423841, mean_absolute_error: 0.638318, mean_q: 1.343702
  59936/1000000: episode: 244, duration: 1.797s, episode steps: 241, steps per second: 134, episode reward: -50.630, mean reward: -0.210 [-11.298, 0.276], mean action: 0.301 [-1.147, 1.146], mean observation: 0.141 [-7.972, 6.051], loss: 0.424916, mean_absolute_error: 0.633017, mean_q: 1.363066
  60150/1000000: episode: 245, duration: 1.626s, episode steps: 214, steps per second: 132, episode reward: 12.201, mean reward: 0.057 [-11.012, 0.404], mean action: 0.358 [-1.071, 1.100], mean observation: 0.195 [-7.884, 6.017], loss: 0.420997, mean_absolute_error: 0.629291, mean_q: 1.429331
  60347/1000000: episode: 246, duration: 1.457s, episode steps: 197, steps per second: 135, episode reward: 19.136, mean reward: 0.097 [-10.745, 0.352], mean action: 0.438 [-1.100, 1.113], mean observation: 0.228 [-7.154, 6.053], loss: 0.448307, mean_absolute_error: 0.637531, mean_q: 1.284409
  60585/1000000: episode: 247, duration: 1.759s, episode steps: 238, steps per second: 135, episode reward: -31.140, mean reward: -0.131 [-11.262, 0.326], mean action: 0.465 [-1.031, 1.061], mean observation: 0.157 [-7.904, 5.987], loss: 0.426150, mean_absolute_error: 0.634084, mean_q: 1.328663
  60829/1000000: episode: 248, duration: 1.801s, episode steps: 244, steps per second: 135, episode reward: -20.258, mean reward: -0.083 [-10.931, 0.291], mean action: 0.335 [-1.095, 1.144], mean observation: 0.114 [-7.911, 6.019], loss: 0.415672, mean_absolute_error: 0.642059, mean_q: 1.300016
  61062/1000000: episode: 249, duration: 1.722s, episode steps: 233, steps per second: 135, episode reward: -73.862, mean reward: -0.317 [-11.442, 0.166], mean action: 0.393 [-1.103, 1.168], mean observation: 0.197 [-8.007, 6.100], loss: 0.431482, mean_absolute_error: 0.638174, mean_q: 1.251132
  61327/1000000: episode: 250, duration: 1.985s, episode steps: 265, steps per second: 133, episode reward: -50.740, mean reward: -0.191 [-11.069, 0.389], mean action: 0.481 [-1.070, 1.123], mean observation: 0.136 [-7.944, 6.015], loss: 0.446749, mean_absolute_error: 0.647488, mean_q: 1.329383
  61558/1000000: episode: 251, duration: 1.713s, episode steps: 231, steps per second: 135, episode reward: 8.146, mean reward: 0.035 [-10.912, 0.361], mean action: 0.455 [-1.111, 1.163], mean observation: 0.154 [-7.893, 5.945], loss: 0.461273, mean_absolute_error: 0.649008, mean_q: 1.277438
  61826/1000000: episode: 252, duration: 1.975s, episode steps: 268, steps per second: 136, episode reward: -56.587, mean reward: -0.211 [-11.005, 0.396], mean action: 0.326 [-1.114, 1.141], mean observation: 0.111 [-8.046, 6.099], loss: 0.457875, mean_absolute_error: 0.653896, mean_q: 1.188507
  62036/1000000: episode: 253, duration: 1.549s, episode steps: 210, steps per second: 136, episode reward: -21.580, mean reward: -0.103 [-10.850, 0.136], mean action: 0.381 [-1.095, 1.110], mean observation: 0.199 [-7.719, 6.014], loss: 0.420392, mean_absolute_error: 0.645745, mean_q: 1.219626
  62311/1000000: episode: 254, duration: 2.035s, episode steps: 275, steps per second: 135, episode reward: -7.486, mean reward: -0.027 [-10.783, 0.534], mean action: 0.313 [-1.045, 1.127], mean observation: 0.071 [-7.919, 6.000], loss: 0.439772, mean_absolute_error: 0.649647, mean_q: 1.223514
  62571/1000000: episode: 255, duration: 1.928s, episode steps: 260, steps per second: 135, episode reward: 15.271, mean reward: 0.059 [-10.756, 0.481], mean action: 0.297 [-1.158, 1.137], mean observation: 0.106 [-8.050, 6.072], loss: 0.452613, mean_absolute_error: 0.656627, mean_q: 1.284827
  62833/1000000: episode: 256, duration: 1.939s, episode steps: 262, steps per second: 135, episode reward: 4.545, mean reward: 0.017 [-10.779, 0.478], mean action: 0.279 [-1.150, 1.092], mean observation: 0.109 [-7.980, 6.017], loss: 0.433923, mean_absolute_error: 0.648076, mean_q: 1.187730
  63072/1000000: episode: 257, duration: 1.758s, episode steps: 239, steps per second: 136, episode reward: 0.312, mean reward: 0.001 [-10.854, 0.362], mean action: 0.299 [-1.128, 1.110], mean observation: 0.124 [-7.992, 6.079], loss: 0.466665, mean_absolute_error: 0.660627, mean_q: 1.232367
  63335/1000000: episode: 258, duration: 1.936s, episode steps: 263, steps per second: 136, episode reward: 20.914, mean reward: 0.080 [-10.756, 0.508], mean action: 0.270 [-1.126, 1.079], mean observation: 0.094 [-7.784, 5.977], loss: 0.470404, mean_absolute_error: 0.657955, mean_q: 1.190392
  63604/1000000: episode: 259, duration: 2.008s, episode steps: 269, steps per second: 134, episode reward: -14.553, mean reward: -0.054 [-10.917, 0.503], mean action: 0.252 [-1.152, 1.130], mean observation: 0.082 [-7.895, 6.059], loss: 0.474461, mean_absolute_error: 0.670214, mean_q: 1.151549
  63761/1000000: episode: 260, duration: 1.175s, episode steps: 157, steps per second: 134, episode reward: -17.686, mean reward: -0.113 [-10.289, -0.011], mean action: 0.487 [-1.080, 1.110], mean observation: 0.197 [-3.426, 6.001], loss: 0.447996, mean_absolute_error: 0.653049, mean_q: 1.203730
  63996/1000000: episode: 261, duration: 1.902s, episode steps: 235, steps per second: 124, episode reward: -42.972, mean reward: -0.183 [-11.443, 0.305], mean action: 0.308 [-1.047, 1.103], mean observation: 0.201 [-7.770, 5.973], loss: 0.483001, mean_absolute_error: 0.671898, mean_q: 1.088716
  64245/1000000: episode: 262, duration: 1.875s, episode steps: 249, steps per second: 133, episode reward: -9.473, mean reward: -0.038 [-10.922, 0.337], mean action: 0.264 [-1.089, 1.081], mean observation: 0.132 [-7.777, 5.990], loss: 0.498405, mean_absolute_error: 0.677817, mean_q: 1.160232
  64484/1000000: episode: 263, duration: 1.806s, episode steps: 239, steps per second: 132, episode reward: -76.014, mean reward: -0.318 [-11.533, 0.165], mean action: 0.293 [-1.137, 1.083], mean observation: 0.171 [-7.666, 5.993], loss: 0.447066, mean_absolute_error: 0.656224, mean_q: 1.191957
  64706/1000000: episode: 264, duration: 1.697s, episode steps: 222, steps per second: 131, episode reward: -2.385, mean reward: -0.011 [-11.139, 0.364], mean action: 0.294 [-1.071, 1.116], mean observation: 0.210 [-7.672, 6.005], loss: 0.462035, mean_absolute_error: 0.667064, mean_q: 1.144785
  64850/1000000: episode: 265, duration: 1.091s, episode steps: 144, steps per second: 132, episode reward: 18.290, mean reward: 0.127 [-10.038, 0.239], mean action: 0.478 [-1.066, 1.065], mean observation: 0.244 [-2.612, 5.805], loss: 0.486777, mean_absolute_error: 0.668229, mean_q: 1.168499
  65111/1000000: episode: 266, duration: 1.991s, episode steps: 261, steps per second: 131, episode reward: -7.725, mean reward: -0.030 [-10.939, 0.458], mean action: 0.334 [-1.079, 1.106], mean observation: 0.093 [-7.825, 6.062], loss: 0.494504, mean_absolute_error: 0.674046, mean_q: 1.107099
  65376/1000000: episode: 267, duration: 2.001s, episode steps: 265, steps per second: 132, episode reward: -84.472, mean reward: -0.319 [-11.107, 0.255], mean action: 0.489 [-1.109, 1.149], mean observation: 0.139 [-7.974, 6.040], loss: 0.512253, mean_absolute_error: 0.683127, mean_q: 1.042537
  65560/1000000: episode: 268, duration: 1.403s, episode steps: 184, steps per second: 131, episode reward: 11.187, mean reward: 0.061 [-10.601, 0.268], mean action: 0.486 [-1.077, 1.147], mean observation: 0.229 [-5.722, 5.972], loss: 0.503459, mean_absolute_error: 0.680310, mean_q: 1.048293
  65793/1000000: episode: 269, duration: 1.795s, episode steps: 233, steps per second: 130, episode reward: -41.602, mean reward: -0.179 [-11.042, 0.158], mean action: 0.432 [-1.041, 1.125], mean observation: 0.172 [-7.897, 6.032], loss: 0.491636, mean_absolute_error: 0.677368, mean_q: 1.114137
  66008/1000000: episode: 270, duration: 1.588s, episode steps: 215, steps per second: 135, episode reward: -20.002, mean reward: -0.093 [-11.158, 0.251], mean action: 0.468 [-1.011, 1.098], mean observation: 0.199 [-7.709, 5.962], loss: 0.497073, mean_absolute_error: 0.679986, mean_q: 1.080817
  66258/1000000: episode: 271, duration: 1.861s, episode steps: 250, steps per second: 134, episode reward: -29.139, mean reward: -0.117 [-10.825, 0.245], mean action: 0.480 [-1.052, 1.158], mean observation: 0.100 [-7.893, 6.020], loss: 0.503259, mean_absolute_error: 0.682991, mean_q: 1.080383
  66520/1000000: episode: 272, duration: 1.932s, episode steps: 262, steps per second: 136, episode reward: -28.105, mean reward: -0.107 [-10.621, 0.252], mean action: 0.455 [-1.117, 1.128], mean observation: 0.086 [-7.943, 6.028], loss: 0.493928, mean_absolute_error: 0.681525, mean_q: 1.105534
  66785/1000000: episode: 273, duration: 1.956s, episode steps: 265, steps per second: 136, episode reward: -53.701, mean reward: -0.203 [-10.744, 0.172], mean action: 0.451 [-1.095, 1.080], mean observation: 0.113 [-7.842, 5.962], loss: 0.528575, mean_absolute_error: 0.697376, mean_q: 1.016949
  67022/1000000: episode: 274, duration: 1.753s, episode steps: 237, steps per second: 135, episode reward: -6.895, mean reward: -0.029 [-11.041, 0.380], mean action: 0.473 [-1.119, 1.129], mean observation: 0.141 [-7.959, 6.050], loss: 0.564228, mean_absolute_error: 0.701573, mean_q: 0.953543
  67299/1000000: episode: 275, duration: 2.043s, episode steps: 277, steps per second: 136, episode reward: -25.144, mean reward: -0.091 [-10.688, 0.455], mean action: 0.489 [-1.112, 1.133], mean observation: 0.074 [-7.895, 6.031], loss: 0.501344, mean_absolute_error: 0.686785, mean_q: 1.035855
  67538/1000000: episode: 276, duration: 1.766s, episode steps: 239, steps per second: 135, episode reward: -12.625, mean reward: -0.053 [-10.991, 0.324], mean action: 0.465 [-1.122, 1.085], mean observation: 0.164 [-7.930, 6.008], loss: 0.502345, mean_absolute_error: 0.694692, mean_q: 1.074798
  67743/1000000: episode: 277, duration: 1.522s, episode steps: 205, steps per second: 135, episode reward: -11.821, mean reward: -0.058 [-10.984, 0.230], mean action: 0.419 [-1.058, 1.114], mean observation: 0.230 [-7.434, 6.013], loss: 0.518589, mean_absolute_error: 0.697269, mean_q: 1.048987
  68011/1000000: episode: 278, duration: 1.994s, episode steps: 268, steps per second: 134, episode reward: -61.970, mean reward: -0.231 [-10.915, 0.203], mean action: 0.301 [-1.111, 1.114], mean observation: 0.104 [-7.900, 6.036], loss: 0.541124, mean_absolute_error: 0.701357, mean_q: 0.938925
  68278/1000000: episode: 279, duration: 1.982s, episode steps: 267, steps per second: 135, episode reward: -67.070, mean reward: -0.251 [-11.311, 0.351], mean action: 0.312 [-1.069, 1.053], mean observation: 0.107 [-7.801, 5.974], loss: 0.513951, mean_absolute_error: 0.697971, mean_q: 1.035852
  68544/1000000: episode: 280, duration: 1.978s, episode steps: 266, steps per second: 134, episode reward: -33.962, mean reward: -0.128 [-10.949, 0.449], mean action: 0.442 [-1.187, 1.130], mean observation: 0.118 [-7.881, 6.069], loss: 0.500576, mean_absolute_error: 0.696652, mean_q: 1.050706
  68778/1000000: episode: 281, duration: 1.790s, episode steps: 234, steps per second: 131, episode reward: 22.598, mean reward: 0.097 [-10.983, 0.480], mean action: 0.487 [-1.042, 1.130], mean observation: 0.173 [-7.820, 5.975], loss: 0.537819, mean_absolute_error: 0.701804, mean_q: 0.913655
  69048/1000000: episode: 282, duration: 2.015s, episode steps: 270, steps per second: 134, episode reward: -9.256, mean reward: -0.034 [-10.453, 0.328], mean action: 0.498 [-1.086, 1.134], mean observation: 0.089 [-7.925, 6.032], loss: 0.503967, mean_absolute_error: 0.692805, mean_q: 1.001836
  69257/1000000: episode: 283, duration: 1.552s, episode steps: 209, steps per second: 135, episode reward: -22.603, mean reward: -0.108 [-11.128, 0.223], mean action: 0.467 [-1.121, 1.135], mean observation: 0.201 [-7.771, 6.057], loss: 0.532746, mean_absolute_error: 0.703276, mean_q: 0.888475
  69457/1000000: episode: 284, duration: 1.493s, episode steps: 200, steps per second: 134, episode reward: 5.594, mean reward: 0.028 [-10.676, 0.244], mean action: 0.464 [-1.107, 1.146], mean observation: 0.194 [-7.260, 6.074], loss: 0.547027, mean_absolute_error: 0.707287, mean_q: 0.947610
  69699/1000000: episode: 285, duration: 1.794s, episode steps: 242, steps per second: 135, episode reward: -9.329, mean reward: -0.039 [-11.201, 0.477], mean action: 0.502 [-1.073, 1.126], mean observation: 0.167 [-7.890, 6.007], loss: 0.554161, mean_absolute_error: 0.709885, mean_q: 0.879733
  69935/1000000: episode: 286, duration: 1.735s, episode steps: 236, steps per second: 136, episode reward: -46.773, mean reward: -0.198 [-11.282, 0.235], mean action: 0.477 [-1.045, 1.186], mean observation: 0.189 [-7.861, 6.022], loss: 0.540375, mean_absolute_error: 0.716014, mean_q: 0.826034
  70203/1000000: episode: 287, duration: 1.971s, episode steps: 268, steps per second: 136, episode reward: -28.592, mean reward: -0.107 [-10.643, 0.312], mean action: 0.475 [-1.071, 1.119], mean observation: 0.106 [-7.892, 6.017], loss: 0.528215, mean_absolute_error: 0.711371, mean_q: 0.899022
  70475/1000000: episode: 288, duration: 1.998s, episode steps: 272, steps per second: 136, episode reward: -58.856, mean reward: -0.216 [-10.899, 0.339], mean action: 0.471 [-1.118, 1.153], mean observation: 0.088 [-7.875, 6.016], loss: 0.583861, mean_absolute_error: 0.726101, mean_q: 0.913553
  70745/1000000: episode: 289, duration: 1.986s, episode steps: 270, steps per second: 136, episode reward: -13.731, mean reward: -0.051 [-10.765, 0.508], mean action: 0.470 [-1.106, 1.087], mean observation: 0.111 [-7.909, 6.063], loss: 0.533351, mean_absolute_error: 0.712089, mean_q: 0.808945
  71023/1000000: episode: 290, duration: 2.049s, episode steps: 278, steps per second: 136, episode reward: -8.809, mean reward: -0.032 [-10.471, 0.355], mean action: 0.485 [-1.051, 1.106], mean observation: 0.072 [-7.799, 6.003], loss: 0.532309, mean_absolute_error: 0.714718, mean_q: 0.781708
  71287/1000000: episode: 291, duration: 1.949s, episode steps: 264, steps per second: 135, episode reward: 10.205, mean reward: 0.039 [-10.920, 0.559], mean action: 0.493 [-1.035, 1.140], mean observation: 0.128 [-7.796, 5.957], loss: 0.583125, mean_absolute_error: 0.724051, mean_q: 0.834600
  71460/1000000: episode: 292, duration: 1.277s, episode steps: 173, steps per second: 135, episode reward: -4.042, mean reward: -0.023 [-10.570, 0.161], mean action: 0.461 [-1.106, 1.123], mean observation: 0.252 [-4.854, 6.034], loss: 0.530095, mean_absolute_error: 0.716775, mean_q: 0.871350
  71660/1000000: episode: 293, duration: 1.475s, episode steps: 200, steps per second: 136, episode reward: -1.447, mean reward: -0.007 [-10.958, 0.289], mean action: 0.478 [-1.131, 1.136], mean observation: 0.219 [-7.145, 6.018], loss: 0.538438, mean_absolute_error: 0.718011, mean_q: 0.925910
  71841/1000000: episode: 294, duration: 1.332s, episode steps: 181, steps per second: 136, episode reward: 4.171, mean reward: 0.023 [-10.605, 0.223], mean action: 0.472 [-1.034, 1.097], mean observation: 0.257 [-5.514, 5.995], loss: 0.509867, mean_absolute_error: 0.706634, mean_q: 0.942813
  72102/1000000: episode: 295, duration: 1.915s, episode steps: 261, steps per second: 136, episode reward: -1.652, mean reward: -0.006 [-10.581, 0.358], mean action: 0.480 [-1.084, 1.087], mean observation: 0.100 [-7.928, 6.028], loss: 0.544217, mean_absolute_error: 0.715862, mean_q: 0.866778
  72378/1000000: episode: 296, duration: 2.043s, episode steps: 276, steps per second: 135, episode reward: -8.245, mean reward: -0.030 [-10.423, 0.378], mean action: 0.490 [-1.110, 1.113], mean observation: 0.066 [-7.955, 6.028], loss: 0.557592, mean_absolute_error: 0.727518, mean_q: 0.924803
  72631/1000000: episode: 297, duration: 1.916s, episode steps: 253, steps per second: 132, episode reward: -60.444, mean reward: -0.239 [-11.329, 0.322], mean action: 0.439 [-1.156, 1.094], mean observation: 0.162 [-7.899, 6.021], loss: 0.561257, mean_absolute_error: 0.725562, mean_q: 0.903772
  72894/1000000: episode: 298, duration: 1.955s, episode steps: 263, steps per second: 135, episode reward: -27.969, mean reward: -0.106 [-11.108, 0.420], mean action: 0.302 [-1.064, 1.170], mean observation: 0.131 [-7.921, 5.972], loss: 0.578012, mean_absolute_error: 0.732074, mean_q: 0.717116
  73087/1000000: episode: 299, duration: 1.437s, episode steps: 193, steps per second: 134, episode reward: 14.392, mean reward: 0.075 [-10.730, 0.320], mean action: 0.383 [-1.058, 1.109], mean observation: 0.229 [-6.427, 5.964], loss: 0.575361, mean_absolute_error: 0.739596, mean_q: 0.708370
  73343/1000000: episode: 300, duration: 1.879s, episode steps: 256, steps per second: 136, episode reward: -0.196, mean reward: -0.001 [-11.057, 0.486], mean action: 0.258 [-1.131, 1.079], mean observation: 0.134 [-7.873, 6.001], loss: 0.554847, mean_absolute_error: 0.737046, mean_q: 0.782024
  73526/1000000: episode: 301, duration: 1.350s, episode steps: 183, steps per second: 136, episode reward: 2.400, mean reward: 0.013 [-10.673, 0.227], mean action: 0.398 [-1.081, 1.098], mean observation: 0.255 [-5.863, 6.038], loss: 0.576926, mean_absolute_error: 0.727838, mean_q: 0.727186
  73804/1000000: episode: 302, duration: 2.104s, episode steps: 278, steps per second: 132, episode reward: 4.447, mean reward: 0.016 [-10.529, 0.393], mean action: 0.284 [-1.096, 1.090], mean observation: 0.069 [-7.913, 5.988], loss: 0.530135, mean_absolute_error: 0.720062, mean_q: 0.811470
  74041/1000000: episode: 303, duration: 1.804s, episode steps: 237, steps per second: 131, episode reward: 6.069, mean reward: 0.026 [-10.995, 0.385], mean action: 0.285 [-1.056, 1.096], mean observation: 0.165 [-7.826, 5.959], loss: 0.543447, mean_absolute_error: 0.722839, mean_q: 0.821759
  74305/1000000: episode: 304, duration: 1.997s, episode steps: 264, steps per second: 132, episode reward: 8.549, mean reward: 0.032 [-11.060, 0.655], mean action: 0.259 [-1.113, 1.084], mean observation: 0.104 [-7.992, 6.053], loss: 0.548350, mean_absolute_error: 0.735791, mean_q: 0.909031
  74559/1000000: episode: 305, duration: 1.880s, episode steps: 254, steps per second: 135, episode reward: -42.625, mean reward: -0.168 [-11.030, 0.231], mean action: 0.277 [-1.098, 1.125], mean observation: 0.103 [-7.878, 5.968], loss: 0.552606, mean_absolute_error: 0.734899, mean_q: 0.754154
  74810/1000000: episode: 306, duration: 1.863s, episode steps: 251, steps per second: 135, episode reward: -51.102, mean reward: -0.204 [-11.406, 0.367], mean action: 0.304 [-1.056, 1.118], mean observation: 0.155 [-7.899, 5.988], loss: 0.581452, mean_absolute_error: 0.745844, mean_q: 0.781616
  75009/1000000: episode: 307, duration: 1.515s, episode steps: 199, steps per second: 131, episode reward: 21.537, mean reward: 0.108 [-10.634, 0.327], mean action: 0.375 [-1.069, 1.104], mean observation: 0.207 [-7.107, 6.016], loss: 0.616333, mean_absolute_error: 0.747331, mean_q: 0.683989
  75289/1000000: episode: 308, duration: 2.069s, episode steps: 280, steps per second: 135, episode reward: 12.246, mean reward: 0.044 [-10.843, 0.597], mean action: 0.271 [-1.145, 1.107], mean observation: 0.086 [-7.923, 5.961], loss: 0.599401, mean_absolute_error: 0.745128, mean_q: 0.679492
  75547/1000000: episode: 309, duration: 1.927s, episode steps: 258, steps per second: 134, episode reward: -14.748, mean reward: -0.057 [-11.065, 0.477], mean action: 0.275 [-1.066, 1.083], mean observation: 0.101 [-7.926, 6.007], loss: 0.578984, mean_absolute_error: 0.743982, mean_q: 0.669745
  75768/1000000: episode: 310, duration: 1.658s, episode steps: 221, steps per second: 133, episode reward: -24.879, mean reward: -0.113 [-11.276, 0.297], mean action: 0.322 [-1.126, 1.141], mean observation: 0.206 [-8.018, 6.070], loss: 0.597765, mean_absolute_error: 0.746762, mean_q: 0.912478
  76015/1000000: episode: 311, duration: 1.852s, episode steps: 247, steps per second: 133, episode reward: -50.627, mean reward: -0.205 [-11.397, 0.307], mean action: 0.293 [-1.091, 1.102], mean observation: 0.143 [-7.905, 5.968], loss: 0.578763, mean_absolute_error: 0.746219, mean_q: 0.724218
  76235/1000000: episode: 312, duration: 1.639s, episode steps: 220, steps per second: 134, episode reward: -27.361, mean reward: -0.124 [-11.290, 0.277], mean action: 0.326 [-1.108, 1.093], mean observation: 0.210 [-7.990, 6.016], loss: 0.544814, mean_absolute_error: 0.729460, mean_q: 0.774575
  76412/1000000: episode: 313, duration: 1.323s, episode steps: 177, steps per second: 134, episode reward: 2.308, mean reward: 0.013 [-10.429, 0.161], mean action: 0.388 [-1.123, 1.125], mean observation: 0.231 [-5.405, 6.045], loss: 0.544560, mean_absolute_error: 0.734482, mean_q: 0.768068
  76578/1000000: episode: 314, duration: 1.233s, episode steps: 166, steps per second: 135, episode reward: -4.124, mean reward: -0.025 [-10.440, 0.127], mean action: 0.402 [-1.179, 1.086], mean observation: 0.221 [-4.524, 6.062], loss: 0.604658, mean_absolute_error: 0.755344, mean_q: 0.866253
  76794/1000000: episode: 315, duration: 1.624s, episode steps: 216, steps per second: 133, episode reward: -25.825, mean reward: -0.120 [-11.147, 0.205], mean action: 0.340 [-1.070, 1.122], mean observation: 0.226 [-7.756, 5.955], loss: 0.577745, mean_absolute_error: 0.747110, mean_q: 0.679777
  77052/1000000: episode: 316, duration: 2.053s, episode steps: 258, steps per second: 126, episode reward: -8.080, mean reward: -0.031 [-11.100, 0.544], mean action: 0.286 [-1.138, 1.105], mean observation: 0.124 [-7.982, 6.042], loss: 0.588082, mean_absolute_error: 0.753063, mean_q: 0.695085
  77331/1000000: episode: 317, duration: 2.140s, episode steps: 279, steps per second: 130, episode reward: 19.354, mean reward: 0.069 [-10.914, 0.737], mean action: 0.288 [-1.086, 1.099], mean observation: 0.091 [-7.926, 5.963], loss: 0.604216, mean_absolute_error: 0.750897, mean_q: 0.784959
  77553/1000000: episode: 318, duration: 1.697s, episode steps: 222, steps per second: 131, episode reward: 9.927, mean reward: 0.045 [-11.116, 0.441], mean action: 0.386 [-1.040, 1.105], mean observation: 0.196 [-7.933, 6.022], loss: 0.613758, mean_absolute_error: 0.764974, mean_q: 0.633820
  77772/1000000: episode: 319, duration: 1.678s, episode steps: 219, steps per second: 131, episode reward: -23.091, mean reward: -0.105 [-11.040, 0.208], mean action: 0.383 [-1.080, 1.111], mean observation: 0.204 [-7.953, 6.030], loss: 0.589296, mean_absolute_error: 0.752362, mean_q: 0.770486
  78043/1000000: episode: 320, duration: 2.205s, episode steps: 271, steps per second: 123, episode reward: -13.745, mean reward: -0.051 [-10.595, 0.337], mean action: 0.323 [-1.119, 1.128], mean observation: 0.065 [-7.984, 6.041], loss: 0.579952, mean_absolute_error: 0.761755, mean_q: 0.637270
  78277/1000000: episode: 321, duration: 2.207s, episode steps: 234, steps per second: 106, episode reward: -17.013, mean reward: -0.073 [-11.145, 0.322], mean action: 0.365 [-1.084, 1.184], mean observation: 0.186 [-7.908, 5.973], loss: 0.594637, mean_absolute_error: 0.759488, mean_q: 0.616874
  78467/1000000: episode: 322, duration: 1.506s, episode steps: 190, steps per second: 126, episode reward: 1.612, mean reward: 0.008 [-10.544, 0.179], mean action: 0.453 [-1.086, 1.123], mean observation: 0.217 [-6.390, 6.010], loss: 0.581719, mean_absolute_error: 0.753452, mean_q: 0.681350
  78705/1000000: episode: 323, duration: 2.005s, episode steps: 238, steps per second: 119, episode reward: -0.991, mean reward: -0.004 [-11.148, 0.463], mean action: 0.344 [-1.115, 1.083], mean observation: 0.164 [-7.961, 6.024], loss: 0.612316, mean_absolute_error: 0.764548, mean_q: 0.714317
  78922/1000000: episode: 324, duration: 1.830s, episode steps: 217, steps per second: 119, episode reward: 12.012, mean reward: 0.055 [-10.971, 0.384], mean action: 0.355 [-1.088, 1.116], mean observation: 0.182 [-7.898, 6.003], loss: 0.612877, mean_absolute_error: 0.771115, mean_q: 0.661786
  79169/1000000: episode: 325, duration: 1.913s, episode steps: 247, steps per second: 129, episode reward: -33.701, mean reward: -0.136 [-11.232, 0.401], mean action: 0.332 [-1.106, 1.155], mean observation: 0.134 [-8.048, 6.075], loss: 0.603351, mean_absolute_error: 0.763726, mean_q: 0.606637
  79343/1000000: episode: 326, duration: 1.339s, episode steps: 174, steps per second: 130, episode reward: -24.929, mean reward: -0.143 [-10.691, 0.041], mean action: 0.444 [-1.051, 1.135], mean observation: 0.271 [-5.020, 6.012], loss: 0.605267, mean_absolute_error: 0.761710, mean_q: 0.666191
  79578/1000000: episode: 327, duration: 1.984s, episode steps: 235, steps per second: 118, episode reward: -62.613, mean reward: -0.266 [-11.420, 0.189], mean action: 0.329 [-1.091, 1.079], mean observation: 0.155 [-7.961, 6.003], loss: 0.610988, mean_absolute_error: 0.766117, mean_q: 0.670372
  79848/1000000: episode: 328, duration: 2.135s, episode steps: 270, steps per second: 126, episode reward: -58.995, mean reward: -0.219 [-10.768, 0.175], mean action: 0.274 [-1.088, 1.103], mean observation: 0.090 [-8.000, 6.044], loss: 0.617300, mean_absolute_error: 0.766894, mean_q: 0.601746
  80022/1000000: episode: 329, duration: 1.305s, episode steps: 174, steps per second: 133, episode reward: 8.730, mean reward: 0.050 [-10.367, 0.194], mean action: 0.430 [-1.117, 1.077], mean observation: 0.210 [-5.006, 6.031], loss: 0.593638, mean_absolute_error: 0.763014, mean_q: 0.648288
  80259/1000000: episode: 330, duration: 1.759s, episode steps: 237, steps per second: 135, episode reward: -17.770, mean reward: -0.075 [-10.892, 0.261], mean action: 0.472 [-1.048, 1.097], mean observation: 0.132 [-7.961, 6.026], loss: 0.599939, mean_absolute_error: 0.766120, mean_q: 0.663218
  80508/1000000: episode: 331, duration: 1.848s, episode steps: 249, steps per second: 135, episode reward: -28.681, mean reward: -0.115 [-11.097, 0.380], mean action: 0.503 [-1.087, 1.136], mean observation: 0.153 [-8.002, 6.053], loss: 0.587463, mean_absolute_error: 0.766882, mean_q: 0.597024
  80739/1000000: episode: 332, duration: 1.706s, episode steps: 231, steps per second: 135, episode reward: -59.147, mean reward: -0.256 [-11.411, 0.201], mean action: 0.501 [-1.103, 1.175], mean observation: 0.167 [-7.963, 6.031], loss: 0.624370, mean_absolute_error: 0.776976, mean_q: 0.663454
  80936/1000000: episode: 333, duration: 1.686s, episode steps: 197, steps per second: 117, episode reward: -6.085, mean reward: -0.031 [-10.699, 0.175], mean action: 0.493 [-1.133, 1.089], mean observation: 0.194 [-7.238, 6.055], loss: 0.626121, mean_absolute_error: 0.775713, mean_q: 0.521085
  81189/1000000: episode: 334, duration: 1.867s, episode steps: 253, steps per second: 135, episode reward: -47.970, mean reward: -0.190 [-11.046, 0.284], mean action: 0.498 [-1.109, 1.120], mean observation: 0.147 [-8.001, 6.027], loss: 0.621323, mean_absolute_error: 0.765803, mean_q: 0.584685
  81374/1000000: episode: 335, duration: 1.567s, episode steps: 185, steps per second: 118, episode reward: -20.226, mean reward: -0.109 [-10.788, 0.103], mean action: 0.490 [-1.034, 1.103], mean observation: 0.264 [-5.789, 5.983], loss: 0.629709, mean_absolute_error: 0.785025, mean_q: 0.568519
  81607/1000000: episode: 336, duration: 1.801s, episode steps: 233, steps per second: 129, episode reward: -49.415, mean reward: -0.212 [-11.395, 0.275], mean action: 0.480 [-1.150, 1.118], mean observation: 0.190 [-7.983, 6.029], loss: 0.617441, mean_absolute_error: 0.784352, mean_q: 0.531358
  81716/1000000: episode: 337, duration: 0.864s, episode steps: 109, steps per second: 126, episode reward: -6.172, mean reward: -0.057 [-9.996, 0.040], mean action: 0.500 [-1.090, 1.136], mean observation: 0.171 [-1.000, 4.217], loss: 0.587623, mean_absolute_error: 0.765789, mean_q: 0.589942
  81963/1000000: episode: 338, duration: 1.987s, episode steps: 247, steps per second: 124, episode reward: -47.344, mean reward: -0.192 [-10.986, 0.218], mean action: 0.465 [-1.114, 1.071], mean observation: 0.103 [-8.003, 6.056], loss: 0.626706, mean_absolute_error: 0.784619, mean_q: 0.544673
  82230/1000000: episode: 339, duration: 2.124s, episode steps: 267, steps per second: 126, episode reward: 17.988, mean reward: 0.067 [-10.681, 0.526], mean action: 0.502 [-1.082, 1.115], mean observation: 0.113 [-7.943, 5.985], loss: 0.633625, mean_absolute_error: 0.782822, mean_q: 0.534433
  82464/1000000: episode: 340, duration: 1.831s, episode steps: 234, steps per second: 128, episode reward: -21.395, mean reward: -0.091 [-11.093, 0.292], mean action: 0.496 [-1.021, 1.102], mean observation: 0.179 [-7.915, 6.006], loss: 0.673490, mean_absolute_error: 0.793042, mean_q: 0.605505
  82733/1000000: episode: 341, duration: 2.229s, episode steps: 269, steps per second: 121, episode reward: -16.268, mean reward: -0.060 [-10.574, 0.298], mean action: 0.310 [-1.135, 1.134], mean observation: 0.068 [-7.998, 6.046], loss: 0.624192, mean_absolute_error: 0.780660, mean_q: 0.546412
  82987/1000000: episode: 342, duration: 1.908s, episode steps: 254, steps per second: 133, episode reward: 25.308, mean reward: 0.100 [-10.945, 0.568], mean action: 0.345 [-1.062, 1.149], mean observation: 0.126 [-7.943, 5.989], loss: 0.636532, mean_absolute_error: 0.781132, mean_q: 0.566793
  83218/1000000: episode: 343, duration: 1.860s, episode steps: 231, steps per second: 124, episode reward: 16.420, mean reward: 0.071 [-10.912, 0.415], mean action: 0.345 [-1.063, 1.135], mean observation: 0.159 [-7.940, 6.016], loss: 0.637733, mean_absolute_error: 0.787567, mean_q: 0.589819
  83451/1000000: episode: 344, duration: 1.887s, episode steps: 233, steps per second: 124, episode reward: -32.310, mean reward: -0.139 [-11.274, 0.255], mean action: 0.141 [-1.068, 1.100], mean observation: 0.157 [-7.091, 5.951], loss: 0.623972, mean_absolute_error: 0.782671, mean_q: 0.679639
  83686/1000000: episode: 345, duration: 2.395s, episode steps: 235, steps per second: 98, episode reward: -33.872, mean reward: -0.144 [-11.340, 0.307], mean action: 0.247 [-1.046, 1.099], mean observation: 0.163 [-7.708, 5.965], loss: 0.656705, mean_absolute_error: 0.798085, mean_q: 0.404899
  83895/1000000: episode: 346, duration: 1.663s, episode steps: 209, steps per second: 126, episode reward: -100.306, mean reward: -0.480 [-12.141, 0.095], mean action: -0.166 [-1.066, 1.067], mean observation: -0.046 [-6.944, 3.889], loss: 0.687924, mean_absolute_error: 0.803246, mean_q: 0.566319
  84153/1000000: episode: 347, duration: 1.930s, episode steps: 258, steps per second: 134, episode reward: -117.276, mean reward: -0.455 [-11.740, 0.325], mean action: 0.010 [-1.089, 1.103], mean observation: -0.129 [-7.280, 3.974], loss: 0.663072, mean_absolute_error: 0.793959, mean_q: 0.552439
  84284/1000000: episode: 348, duration: 0.982s, episode steps: 131, steps per second: 133, episode reward: -18.311, mean reward: -0.140 [-10.396, 0.001], mean action: 0.009 [-1.019, 1.068], mean observation: 0.110 [-5.039, 3.935], loss: 0.609438, mean_absolute_error: 0.779955, mean_q: 0.506463
  84448/1000000: episode: 349, duration: 1.234s, episode steps: 164, steps per second: 133, episode reward: 13.728, mean reward: 0.084 [-10.643, 0.299], mean action: -0.016 [-1.149, 1.075], mean observation: 0.055 [-6.469, 4.037], loss: 0.683294, mean_absolute_error: 0.801562, mean_q: 0.352115
  84692/1000000: episode: 350, duration: 1.837s, episode steps: 244, steps per second: 133, episode reward: -102.803, mean reward: -0.421 [-12.151, 0.412], mean action: -0.027 [-1.151, 1.067], mean observation: -0.147 [-7.089, 4.056], loss: 0.646783, mean_absolute_error: 0.790391, mean_q: 0.453394
  84867/1000000: episode: 351, duration: 1.338s, episode steps: 175, steps per second: 131, episode reward: -20.780, mean reward: -0.119 [-11.182, 0.197], mean action: 0.009 [-1.077, 1.082], mean observation: 0.045 [-6.982, 3.936], loss: 0.688535, mean_absolute_error: 0.804907, mean_q: 0.438362
  85049/1000000: episode: 352, duration: 1.383s, episode steps: 182, steps per second: 132, episode reward: -14.223, mean reward: -0.078 [-11.339, 0.302], mean action: 0.007 [-1.156, 1.081], mean observation: 0.011 [-7.103, 3.938], loss: 0.651815, mean_absolute_error: 0.793512, mean_q: 0.491066
  85264/1000000: episode: 353, duration: 1.611s, episode steps: 215, steps per second: 133, episode reward: -39.992, mean reward: -0.186 [-11.903, 0.419], mean action: 0.151 [-1.060, 1.104], mean observation: -0.047 [-7.060, 3.892], loss: 0.688089, mean_absolute_error: 0.808408, mean_q: 0.459502
  85519/1000000: episode: 354, duration: 1.910s, episode steps: 255, steps per second: 133, episode reward: -56.910, mean reward: -0.223 [-12.080, 0.666], mean action: 0.262 [-1.040, 1.122], mean observation: -0.135 [-6.996, 3.978], loss: 0.674946, mean_absolute_error: 0.811608, mean_q: 0.490735
  85744/1000000: episode: 355, duration: 1.693s, episode steps: 225, steps per second: 133, episode reward: -68.122, mean reward: -0.303 [-12.117, 0.394], mean action: 0.219 [-1.073, 1.161], mean observation: -0.092 [-6.955, 3.851], loss: 0.653012, mean_absolute_error: 0.797802, mean_q: 0.517948
  85916/1000000: episode: 356, duration: 1.289s, episode steps: 172, steps per second: 133, episode reward: -18.826, mean reward: -0.109 [-10.848, 0.118], mean action: 0.140 [-1.065, 1.071], mean observation: 0.050 [-6.581, 4.063], loss: 0.604972, mean_absolute_error: 0.781961, mean_q: 0.536421
  86150/1000000: episode: 357, duration: 1.738s, episode steps: 234, steps per second: 135, episode reward: -100.232, mean reward: -0.428 [-12.274, 0.337], mean action: 0.234 [-1.092, 1.048], mean observation: -0.065 [-6.903, 3.950], loss: 0.671656, mean_absolute_error: 0.812071, mean_q: 0.478210
  86353/1000000: episode: 358, duration: 1.516s, episode steps: 203, steps per second: 134, episode reward: -56.702, mean reward: -0.279 [-11.635, 0.162], mean action: 0.248 [-1.028, 1.132], mean observation: -0.058 [-6.917, 3.994], loss: 0.652431, mean_absolute_error: 0.808117, mean_q: 0.392816
  86611/1000000: episode: 359, duration: 1.921s, episode steps: 258, steps per second: 134, episode reward: -119.554, mean reward: -0.463 [-12.126, 0.358], mean action: 0.298 [-1.060, 1.132], mean observation: -0.140 [-6.962, 4.063], loss: 0.679087, mean_absolute_error: 0.807417, mean_q: 0.525301
  86830/1000000: episode: 360, duration: 1.631s, episode steps: 219, steps per second: 134, episode reward: -18.606, mean reward: -0.085 [-11.620, 0.447], mean action: 0.246 [-1.101, 1.080], mean observation: -0.046 [-6.913, 3.992], loss: 0.661287, mean_absolute_error: 0.795484, mean_q: 0.513491
  87055/1000000: episode: 361, duration: 1.674s, episode steps: 225, steps per second: 134, episode reward: -68.588, mean reward: -0.305 [-12.066, 0.377], mean action: 0.248 [-1.109, 1.117], mean observation: -0.079 [-6.957, 3.999], loss: 0.694372, mean_absolute_error: 0.816544, mean_q: 0.434974
  87272/1000000: episode: 362, duration: 1.622s, episode steps: 217, steps per second: 134, episode reward: -22.049, mean reward: -0.102 [-11.683, 0.448], mean action: 0.234 [-1.053, 1.074], mean observation: -0.058 [-6.885, 4.017], loss: 0.697743, mean_absolute_error: 0.818977, mean_q: 0.327250
  87532/1000000: episode: 363, duration: 1.944s, episode steps: 260, steps per second: 134, episode reward: -84.895, mean reward: -0.327 [-12.237, 0.624], mean action: 0.278 [-1.076, 1.098], mean observation: -0.106 [-6.890, 3.967], loss: 0.658543, mean_absolute_error: 0.812740, mean_q: 0.445306
  87729/1000000: episode: 364, duration: 1.495s, episode steps: 197, steps per second: 132, episode reward: -19.734, mean reward: -0.100 [-11.455, 0.330], mean action: 0.169 [-1.126, 1.100], mean observation: -0.001 [-6.867, 3.907], loss: 0.674673, mean_absolute_error: 0.816740, mean_q: 0.337652
  87986/1000000: episode: 365, duration: 1.913s, episode steps: 257, steps per second: 134, episode reward: -134.486, mean reward: -0.523 [-12.412, 0.431], mean action: 0.263 [-1.094, 1.111], mean observation: -0.119 [-6.945, 3.972], loss: 0.717731, mean_absolute_error: 0.827043, mean_q: 0.382582
  88240/1000000: episode: 366, duration: 1.892s, episode steps: 254, steps per second: 134, episode reward: -91.712, mean reward: -0.361 [-11.945, 0.350], mean action: 0.262 [-1.103, 1.088], mean observation: -0.130 [-6.921, 3.945], loss: 0.649807, mean_absolute_error: 0.808370, mean_q: 0.421840
  88375/1000000: episode: 367, duration: 1.016s, episode steps: 135, steps per second: 133, episode reward: -7.235, mean reward: -0.054 [-10.307, 0.081], mean action: 0.103 [-1.051, 1.091], mean observation: 0.136 [-5.039, 4.049], loss: 0.653047, mean_absolute_error: 0.811825, mean_q: 0.446966
  88622/1000000: episode: 368, duration: 1.847s, episode steps: 247, steps per second: 134, episode reward: -140.756, mean reward: -0.570 [-12.451, 0.305], mean action: 0.269 [-1.086, 1.120], mean observation: -0.094 [-6.902, 4.098], loss: 0.711257, mean_absolute_error: 0.828079, mean_q: 0.400134
  88852/1000000: episode: 369, duration: 1.725s, episode steps: 230, steps per second: 133, episode reward: -70.873, mean reward: -0.308 [-12.085, 0.396], mean action: 0.269 [-1.048, 1.118], mean observation: -0.045 [-6.928, 4.063], loss: 0.676589, mean_absolute_error: 0.820879, mean_q: 0.424489
  89103/1000000: episode: 370, duration: 1.874s, episode steps: 251, steps per second: 134, episode reward: -129.488, mean reward: -0.516 [-12.424, 0.422], mean action: 0.202 [-1.065, 1.109], mean observation: -0.127 [-7.005, 4.001], loss: 0.674448, mean_absolute_error: 0.817820, mean_q: 0.412559
  89292/1000000: episode: 371, duration: 1.414s, episode steps: 189, steps per second: 134, episode reward: -58.969, mean reward: -0.312 [-11.525, 0.063], mean action: 0.012 [-1.152, 1.084], mean observation: 0.037 [-7.065, 4.104], loss: 0.708434, mean_absolute_error: 0.832167, mean_q: 0.350240
  89517/1000000: episode: 372, duration: 1.679s, episode steps: 225, steps per second: 134, episode reward: -57.090, mean reward: -0.254 [-11.738, 0.295], mean action: 0.026 [-1.099, 1.137], mean observation: -0.098 [-7.102, 3.967], loss: 0.683065, mean_absolute_error: 0.821743, mean_q: 0.333587
  89771/1000000: episode: 373, duration: 1.900s, episode steps: 254, steps per second: 134, episode reward: -123.177, mean reward: -0.485 [-12.143, 0.445], mean action: 0.008 [-1.055, 1.090], mean observation: -0.143 [-7.090, 3.893], loss: 0.717434, mean_absolute_error: 0.830087, mean_q: 0.370588
  89955/1000000: episode: 374, duration: 1.383s, episode steps: 184, steps per second: 133, episode reward: -16.363, mean reward: -0.089 [-11.388, 0.305], mean action: -0.011 [-1.133, 1.063], mean observation: -0.006 [-7.082, 4.052], loss: 0.712269, mean_absolute_error: 0.831962, mean_q: 0.293928
  90136/1000000: episode: 375, duration: 1.351s, episode steps: 181, steps per second: 134, episode reward: -7.715, mean reward: -0.043 [-11.013, 0.235], mean action: 0.022 [-1.036, 1.086], mean observation: -0.002 [-7.030, 3.829], loss: 0.716098, mean_absolute_error: 0.831261, mean_q: 0.311159
  90384/1000000: episode: 376, duration: 1.852s, episode steps: 248, steps per second: 134, episode reward: -160.057, mean reward: -0.645 [-12.081, 0.124], mean action: -0.020 [-1.142, 1.088], mean observation: -0.097 [-7.096, 4.021], loss: 0.679794, mean_absolute_error: 0.825751, mean_q: 0.316770
  90628/1000000: episode: 377, duration: 1.823s, episode steps: 244, steps per second: 134, episode reward: -131.087, mean reward: -0.537 [-12.027, 0.209], mean action: 0.022 [-1.121, 1.163], mean observation: -0.102 [-7.153, 3.888], loss: 0.658380, mean_absolute_error: 0.821915, mean_q: 0.299495
  90818/1000000: episode: 378, duration: 1.417s, episode steps: 190, steps per second: 134, episode reward: -32.449, mean reward: -0.171 [-11.382, 0.198], mean action: -0.008 [-1.104, 1.102], mean observation: 0.011 [-7.120, 3.958], loss: 0.713801, mean_absolute_error: 0.836348, mean_q: 0.359228
  91024/1000000: episode: 379, duration: 1.548s, episode steps: 206, steps per second: 133, episode reward: -1.343, mean reward: -0.007 [-11.574, 0.492], mean action: -0.018 [-1.109, 1.068], mean observation: -0.035 [-7.092, 4.006], loss: 0.708854, mean_absolute_error: 0.825442, mean_q: 0.321834
  91266/1000000: episode: 380, duration: 1.822s, episode steps: 242, steps per second: 133, episode reward: -107.465, mean reward: -0.444 [-12.188, 0.415], mean action: 0.008 [-1.137, 1.122], mean observation: -0.150 [-7.133, 4.004], loss: 0.686372, mean_absolute_error: 0.828649, mean_q: 0.421594
  91516/1000000: episode: 381, duration: 1.871s, episode steps: 250, steps per second: 134, episode reward: -126.590, mean reward: -0.506 [-12.138, 0.370], mean action: -0.002 [-1.111, 1.085], mean observation: -0.123 [-7.101, 3.983], loss: 0.730044, mean_absolute_error: 0.841256, mean_q: 0.242479
  91754/1000000: episode: 382, duration: 1.782s, episode steps: 238, steps per second: 134, episode reward: -93.400, mean reward: -0.392 [-12.211, 0.413], mean action: -0.003 [-1.089, 1.058], mean observation: -0.128 [-7.098, 4.027], loss: 0.716849, mean_absolute_error: 0.840109, mean_q: 0.395209
  91981/1000000: episode: 383, duration: 1.703s, episode steps: 227, steps per second: 133, episode reward: -62.530, mean reward: -0.275 [-11.711, 0.257], mean action: 0.017 [-1.108, 1.130], mean observation: -0.066 [-7.070, 4.020], loss: 0.712265, mean_absolute_error: 0.848209, mean_q: 0.203227
  92113/1000000: episode: 384, duration: 0.995s, episode steps: 132, steps per second: 133, episode reward: -6.132, mean reward: -0.046 [-10.250, 0.077], mean action: -0.019 [-1.144, 1.060], mean observation: 0.084 [-5.014, 4.075], loss: 0.676278, mean_absolute_error: 0.829780, mean_q: 0.365762
  92313/1000000: episode: 385, duration: 1.503s, episode steps: 200, steps per second: 133, episode reward: -34.564, mean reward: -0.173 [-11.731, 0.327], mean action: 0.003 [-1.093, 1.100], mean observation: -0.033 [-7.056, 3.970], loss: 0.688419, mean_absolute_error: 0.835644, mean_q: 0.337865
  92471/1000000: episode: 386, duration: 1.184s, episode steps: 158, steps per second: 133, episode reward: 12.260, mean reward: 0.078 [-10.527, 0.267], mean action: -0.006 [-1.109, 1.084], mean observation: 0.056 [-5.918, 4.002], loss: 0.721060, mean_absolute_error: 0.839410, mean_q: 0.333711
  92657/1000000: episode: 387, duration: 1.392s, episode steps: 186, steps per second: 134, episode reward: -38.631, mean reward: -0.208 [-11.474, 0.175], mean action: 0.040 [-1.053, 1.153], mean observation: 0.024 [-7.133, 3.961], loss: 0.704495, mean_absolute_error: 0.842944, mean_q: 0.331604
  92842/1000000: episode: 388, duration: 1.379s, episode steps: 185, steps per second: 134, episode reward: -39.490, mean reward: -0.213 [-11.509, 0.178], mean action: -0.025 [-1.104, 1.066], mean observation: -0.023 [-7.085, 3.883], loss: 0.727379, mean_absolute_error: 0.846829, mean_q: 0.274954
  93065/1000000: episode: 389, duration: 1.712s, episode steps: 223, steps per second: 130, episode reward: -78.723, mean reward: -0.353 [-12.111, 0.306], mean action: 0.030 [-1.071, 1.110], mean observation: -0.102 [-7.089, 3.975], loss: 0.731441, mean_absolute_error: 0.849253, mean_q: 0.382034
  93275/1000000: episode: 390, duration: 1.615s, episode steps: 210, steps per second: 130, episode reward: -5.217, mean reward: -0.025 [-11.648, 0.507], mean action: -0.017 [-1.067, 1.058], mean observation: -0.047 [-7.070, 4.022], loss: 0.742990, mean_absolute_error: 0.853485, mean_q: 0.308694
  93505/1000000: episode: 391, duration: 1.795s, episode steps: 230, steps per second: 128, episode reward: -56.161, mean reward: -0.244 [-12.103, 0.498], mean action: 0.001 [-1.098, 1.080], mean observation: -0.096 [-7.061, 4.049], loss: 0.745008, mean_absolute_error: 0.858401, mean_q: 0.307300
  93744/1000000: episode: 392, duration: 1.844s, episode steps: 239, steps per second: 130, episode reward: -164.395, mean reward: -0.688 [-12.472, 0.162], mean action: -0.015 [-1.119, 1.157], mean observation: -0.126 [-7.126, 3.955], loss: 0.718382, mean_absolute_error: 0.855758, mean_q: 0.340062
  93955/1000000: episode: 393, duration: 1.629s, episode steps: 211, steps per second: 129, episode reward: -61.646, mean reward: -0.292 [-11.957, 0.282], mean action: -0.023 [-1.085, 1.043], mean observation: -0.079 [-7.083, 3.937], loss: 0.723738, mean_absolute_error: 0.847402, mean_q: 0.236064
  94120/1000000: episode: 394, duration: 1.342s, episode steps: 165, steps per second: 123, episode reward: 14.187, mean reward: 0.086 [-10.776, 0.340], mean action: -0.025 [-1.148, 1.070], mean observation: 0.043 [-6.562, 4.038], loss: 0.712568, mean_absolute_error: 0.849054, mean_q: 0.255604
  94307/1000000: episode: 395, duration: 1.507s, episode steps: 187, steps per second: 124, episode reward: -4.197, mean reward: -0.022 [-11.226, 0.330], mean action: -0.003 [-1.102, 1.103], mean observation: -0.018 [-7.105, 3.974], loss: 0.716379, mean_absolute_error: 0.853805, mean_q: 0.317454
  94514/1000000: episode: 396, duration: 1.561s, episode steps: 207, steps per second: 133, episode reward: -22.045, mean reward: -0.106 [-11.768, 0.454], mean action: 0.027 [-1.139, 1.153], mean observation: -0.042 [-7.132, 3.949], loss: 0.717005, mean_absolute_error: 0.853272, mean_q: 0.233494
  94735/1000000: episode: 397, duration: 1.687s, episode steps: 221, steps per second: 131, episode reward: -33.594, mean reward: -0.152 [-11.908, 0.477], mean action: -0.016 [-1.084, 1.097], mean observation: -0.075 [-7.061, 4.071], loss: 0.721722, mean_absolute_error: 0.853281, mean_q: 0.379218
  94933/1000000: episode: 398, duration: 1.565s, episode steps: 198, steps per second: 127, episode reward: -29.202, mean reward: -0.147 [-11.354, 0.224], mean action: 0.003 [-1.078, 1.114], mean observation: -0.030 [-7.093, 3.959], loss: 0.690375, mean_absolute_error: 0.847370, mean_q: 0.255290
  95075/1000000: episode: 399, duration: 1.112s, episode steps: 142, steps per second: 128, episode reward: 5.866, mean reward: 0.041 [-10.397, 0.211], mean action: 0.010 [-1.098, 1.148], mean observation: 0.096 [-5.419, 3.951], loss: 0.734828, mean_absolute_error: 0.861937, mean_q: 0.286696
  95288/1000000: episode: 400, duration: 1.643s, episode steps: 213, steps per second: 130, episode reward: -49.472, mean reward: -0.232 [-11.689, 0.271], mean action: -0.001 [-1.119, 1.114], mean observation: -0.041 [-7.124, 3.941], loss: 0.720080, mean_absolute_error: 0.855649, mean_q: 0.309132
  95444/1000000: episode: 401, duration: 1.182s, episode steps: 156, steps per second: 132, episode reward: 10.930, mean reward: 0.070 [-10.613, 0.287], mean action: -0.008 [-1.145, 1.125], mean observation: 0.058 [-5.802, 3.832], loss: 0.714404, mean_absolute_error: 0.853607, mean_q: 0.292224
  95696/1000000: episode: 402, duration: 1.976s, episode steps: 252, steps per second: 128, episode reward: -64.354, mean reward: -0.255 [-11.922, 0.670], mean action: -0.021 [-1.137, 1.137], mean observation: -0.154 [-7.117, 3.997], loss: 0.755900, mean_absolute_error: 0.856851, mean_q: 0.378879
  95943/1000000: episode: 403, duration: 2.278s, episode steps: 247, steps per second: 108, episode reward: -44.883, mean reward: -0.182 [-11.846, 0.595], mean action: 0.018 [-1.110, 1.158], mean observation: -0.125 [-7.104, 3.938], loss: 0.734862, mean_absolute_error: 0.854948, mean_q: 0.336775
  96202/1000000: episode: 404, duration: 2.027s, episode steps: 259, steps per second: 128, episode reward: -80.150, mean reward: -0.309 [-11.486, 0.370], mean action: 0.020 [-1.063, 1.091], mean observation: -0.158 [-7.372, 3.858], loss: 0.717584, mean_absolute_error: 0.850597, mean_q: 0.425432
  96369/1000000: episode: 405, duration: 1.257s, episode steps: 167, steps per second: 133, episode reward: -35.617, mean reward: -0.213 [-11.054, 0.039], mean action: 0.001 [-1.129, 1.139], mean observation: 0.065 [-6.802, 3.808], loss: 0.740413, mean_absolute_error: 0.856215, mean_q: 0.309768
  96536/1000000: episode: 406, duration: 1.289s, episode steps: 167, steps per second: 130, episode reward: 3.765, mean reward: 0.023 [-10.894, 0.295], mean action: 0.008 [-1.054, 1.079], mean observation: 0.050 [-6.658, 3.936], loss: 0.795251, mean_absolute_error: 0.873280, mean_q: 0.230565
  96767/1000000: episode: 407, duration: 1.800s, episode steps: 231, steps per second: 128, episode reward: -113.634, mean reward: -0.492 [-12.133, 0.188], mean action: 0.016 [-1.116, 1.124], mean observation: -0.056 [-7.099, 4.116], loss: 0.730287, mean_absolute_error: 0.858597, mean_q: 0.338679
  96923/1000000: episode: 408, duration: 1.185s, episode steps: 156, steps per second: 132, episode reward: 0.003, mean reward: 0.000 [-10.693, 0.220], mean action: -0.024 [-1.090, 1.146], mean observation: 0.065 [-5.837, 4.198], loss: 0.746546, mean_absolute_error: 0.858380, mean_q: 0.271991
  97178/1000000: episode: 409, duration: 1.910s, episode steps: 255, steps per second: 134, episode reward: -49.409, mean reward: -0.194 [-11.808, 0.760], mean action: -0.011 [-1.130, 1.152], mean observation: -0.159 [-7.141, 4.020], loss: 0.757026, mean_absolute_error: 0.864807, mean_q: 0.278206
  97381/1000000: episode: 410, duration: 1.545s, episode steps: 203, steps per second: 131, episode reward: -69.752, mean reward: -0.344 [-11.986, 0.210], mean action: 0.005 [-1.133, 1.109], mean observation: -0.047 [-7.101, 4.009], loss: 0.732479, mean_absolute_error: 0.867619, mean_q: 0.389819
  97630/1000000: episode: 411, duration: 1.896s, episode steps: 249, steps per second: 131, episode reward: -47.547, mean reward: -0.191 [-11.834, 0.574], mean action: -0.017 [-1.097, 1.094], mean observation: -0.148 [-7.116, 3.878], loss: 0.732810, mean_absolute_error: 0.863825, mean_q: 0.297877
  97858/1000000: episode: 412, duration: 1.720s, episode steps: 228, steps per second: 133, episode reward: -42.986, mean reward: -0.189 [-11.776, 0.407], mean action: 0.001 [-1.117, 1.107], mean observation: -0.073 [-7.117, 4.012], loss: 0.728696, mean_absolute_error: 0.864385, mean_q: 0.292286
  97990/1000000: episode: 413, duration: 1.010s, episode steps: 132, steps per second: 131, episode reward: -6.821, mean reward: -0.052 [-10.262, 0.073], mean action: 0.007 [-1.097, 1.121], mean observation: 0.139 [-5.215, 3.920], loss: 0.783607, mean_absolute_error: 0.870703, mean_q: 0.318233
  98211/1000000: episode: 414, duration: 1.683s, episode steps: 221, steps per second: 131, episode reward: -49.684, mean reward: -0.225 [-11.726, 0.309], mean action: -0.018 [-1.105, 1.102], mean observation: -0.052 [-7.064, 3.989], loss: 0.732028, mean_absolute_error: 0.869078, mean_q: 0.266892
  98444/1000000: episode: 415, duration: 1.793s, episode steps: 233, steps per second: 130, episode reward: -107.986, mean reward: -0.463 [-12.078, 0.211], mean action: 0.044 [-1.036, 1.142], mean observation: -0.069 [-7.087, 3.933], loss: 0.759899, mean_absolute_error: 0.874233, mean_q: 0.353529
  98637/1000000: episode: 416, duration: 1.451s, episode steps: 193, steps per second: 133, episode reward: 0.424, mean reward: 0.002 [-11.271, 0.376], mean action: 0.002 [-1.052, 1.053], mean observation: -0.019 [-7.066, 3.973], loss: 0.745647, mean_absolute_error: 0.863116, mean_q: 0.311686
  98886/1000000: episode: 417, duration: 1.865s, episode steps: 249, steps per second: 133, episode reward: -42.655, mean reward: -0.171 [-11.791, 0.584], mean action: 0.001 [-1.080, 1.114], mean observation: -0.123 [-7.068, 4.113], loss: 0.761228, mean_absolute_error: 0.870877, mean_q: 0.341677
  99006/1000000: episode: 418, duration: 0.917s, episode steps: 120, steps per second: 131, episode reward: 13.483, mean reward: 0.112 [-10.021, 0.236], mean action: 0.023 [-1.055, 1.086], mean observation: 0.126 [-4.628, 3.972], loss: 0.762371, mean_absolute_error: 0.868340, mean_q: 0.311907
  99217/1000000: episode: 419, duration: 1.606s, episode steps: 211, steps per second: 131, episode reward: -39.305, mean reward: -0.186 [-11.586, 0.282], mean action: 0.004 [-1.145, 1.077], mean observation: -0.032 [-7.101, 4.118], loss: 0.737061, mean_absolute_error: 0.868378, mean_q: 0.294611
  99459/1000000: episode: 420, duration: 1.842s, episode steps: 242, steps per second: 131, episode reward: -164.045, mean reward: -0.678 [-12.514, 0.179], mean action: -0.004 [-1.081, 1.105], mean observation: -0.116 [-7.059, 4.077], loss: 0.745225, mean_absolute_error: 0.870952, mean_q: 0.317294
  99697/1000000: episode: 421, duration: 1.789s, episode steps: 238, steps per second: 133, episode reward: -82.623, mean reward: -0.347 [-12.078, 0.407], mean action: -0.002 [-1.127, 1.070], mean observation: -0.091 [-7.088, 3.914], loss: 0.738256, mean_absolute_error: 0.868078, mean_q: 0.217026
  99929/1000000: episode: 422, duration: 1.730s, episode steps: 232, steps per second: 134, episode reward: -105.462, mean reward: -0.455 [-12.273, 0.303], mean action: 0.011 [-1.074, 1.111], mean observation: -0.127 [-7.110, 3.905], loss: 0.709619, mean_absolute_error: 0.865835, mean_q: 0.286946
 100169/1000000: episode: 423, duration: 1.802s, episode steps: 240, steps per second: 133, episode reward: -84.010, mean reward: -0.350 [-12.048, 0.400], mean action: -0.012 [-1.150, 1.080], mean observation: -0.087 [-7.075, 3.998], loss: 0.747036, mean_absolute_error: 0.881099, mean_q: 0.335870
 100396/1000000: episode: 424, duration: 1.696s, episode steps: 227, steps per second: 134, episode reward: -107.675, mean reward: -0.474 [-12.066, 0.179], mean action: -0.003 [-1.127, 1.144], mean observation: -0.057 [-7.122, 4.063], loss: 0.737802, mean_absolute_error: 0.871989, mean_q: 0.275169
 100642/1000000: episode: 425, duration: 1.835s, episode steps: 246, steps per second: 134, episode reward: -125.751, mean reward: -0.511 [-12.213, 0.325], mean action: 0.005 [-1.111, 1.170], mean observation: -0.156 [-7.106, 3.989], loss: 0.757319, mean_absolute_error: 0.877198, mean_q: 0.248925
 100848/1000000: episode: 426, duration: 1.553s, episode steps: 206, steps per second: 133, episode reward: -61.566, mean reward: -0.299 [-11.773, 0.182], mean action: -0.004 [-1.113, 1.118], mean observation: -0.066 [-7.106, 4.115], loss: 0.768300, mean_absolute_error: 0.881934, mean_q: 0.182545
 101074/1000000: episode: 427, duration: 1.714s, episode steps: 226, steps per second: 132, episode reward: -54.877, mean reward: -0.243 [-11.832, 0.341], mean action: -0.024 [-1.156, 1.101], mean observation: -0.057 [-7.077, 3.980], loss: 0.747644, mean_absolute_error: 0.881005, mean_q: 0.359696
 101318/1000000: episode: 428, duration: 1.851s, episode steps: 244, steps per second: 132, episode reward: -132.358, mean reward: -0.542 [-12.082, 0.219], mean action: -0.003 [-1.077, 1.125], mean observation: -0.099 [-7.117, 3.933], loss: 0.797655, mean_absolute_error: 0.899822, mean_q: 0.139144
 101564/1000000: episode: 429, duration: 1.853s, episode steps: 246, steps per second: 133, episode reward: -72.706, mean reward: -0.296 [-11.903, 0.524], mean action: -0.001 [-1.116, 1.127], mean observation: -0.123 [-7.121, 4.038], loss: 0.758174, mean_absolute_error: 0.883882, mean_q: 0.269830
 101701/1000000: episode: 430, duration: 1.052s, episode steps: 137, steps per second: 130, episode reward: -12.972, mean reward: -0.095 [-10.467, 0.066], mean action: 0.022 [-1.048, 1.140], mean observation: 0.100 [-5.298, 4.012], loss: 0.754711, mean_absolute_error: 0.878644, mean_q: 0.248271
 101943/1000000: episode: 431, duration: 1.831s, episode steps: 242, steps per second: 132, episode reward: -144.451, mean reward: -0.597 [-12.380, 0.273], mean action: -0.014 [-1.131, 1.126], mean observation: -0.134 [-7.131, 3.976], loss: 0.755453, mean_absolute_error: 0.879993, mean_q: 0.261461
 102151/1000000: episode: 432, duration: 1.568s, episode steps: 208, steps per second: 133, episode reward: -26.553, mean reward: -0.128 [-11.566, 0.347], mean action: 0.006 [-1.083, 1.117], mean observation: -0.038 [-7.126, 3.997], loss: 0.790716, mean_absolute_error: 0.891643, mean_q: 0.286732
 102402/1000000: episode: 433, duration: 1.908s, episode steps: 251, steps per second: 132, episode reward: -121.028, mean reward: -0.482 [-12.132, 0.416], mean action: 0.025 [-1.093, 1.159], mean observation: -0.124 [-7.092, 4.086], loss: 0.763114, mean_absolute_error: 0.875155, mean_q: 0.219678
 102618/1000000: episode: 434, duration: 1.608s, episode steps: 216, steps per second: 134, episode reward: -107.502, mean reward: -0.498 [-12.284, 0.160], mean action: 0.002 [-1.108, 1.138], mean observation: -0.063 [-7.089, 4.024], loss: 0.787069, mean_absolute_error: 0.898392, mean_q: 0.138477
 102840/1000000: episode: 435, duration: 1.673s, episode steps: 222, steps per second: 133, episode reward: -74.588, mean reward: -0.336 [-11.918, 0.253], mean action: -0.031 [-1.139, 1.073], mean observation: -0.054 [-7.109, 3.818], loss: 0.809620, mean_absolute_error: 0.903426, mean_q: 0.124786
 103064/1000000: episode: 436, duration: 1.698s, episode steps: 224, steps per second: 132, episode reward: -34.221, mean reward: -0.153 [-11.735, 0.428], mean action: 0.003 [-1.091, 1.104], mean observation: -0.080 [-7.097, 3.911], loss: 0.743290, mean_absolute_error: 0.883353, mean_q: 0.271276
 103259/1000000: episode: 437, duration: 1.475s, episode steps: 195, steps per second: 132, episode reward: -56.302, mean reward: -0.289 [-11.795, 0.190], mean action: -0.009 [-1.100, 1.090], mean observation: -0.023 [-7.138, 3.850], loss: 0.779359, mean_absolute_error: 0.890547, mean_q: 0.085638
 103483/1000000: episode: 438, duration: 1.683s, episode steps: 224, steps per second: 133, episode reward: -61.686, mean reward: -0.275 [-11.879, 0.340], mean action: 0.012 [-1.096, 1.122], mean observation: -0.061 [-7.129, 4.017], loss: 0.797284, mean_absolute_error: 0.901374, mean_q: 0.131887
 103730/1000000: episode: 439, duration: 1.857s, episode steps: 247, steps per second: 133, episode reward: -76.903, mean reward: -0.311 [-11.616, 0.313], mean action: -0.014 [-1.103, 1.101], mean observation: -0.137 [-7.111, 3.955], loss: 0.774524, mean_absolute_error: 0.891504, mean_q: 0.261971
 103929/1000000: episode: 440, duration: 1.485s, episode steps: 199, steps per second: 134, episode reward: -55.911, mean reward: -0.281 [-11.762, 0.194], mean action: -0.015 [-1.089, 1.076], mean observation: 0.002 [-7.082, 3.985], loss: 0.824227, mean_absolute_error: 0.900978, mean_q: 0.197832
 104176/1000000: episode: 441, duration: 1.910s, episode steps: 247, steps per second: 129, episode reward: -93.782, mean reward: -0.380 [-12.141, 0.492], mean action: -0.004 [-1.118, 1.090], mean observation: -0.127 [-7.109, 3.858], loss: 0.794397, mean_absolute_error: 0.900316, mean_q: 0.179947
 104399/1000000: episode: 442, duration: 1.682s, episode steps: 223, steps per second: 133, episode reward: -26.950, mean reward: -0.121 [-11.929, 0.542], mean action: 0.005 [-1.058, 1.124], mean observation: -0.082 [-7.073, 3.885], loss: 0.772708, mean_absolute_error: 0.896728, mean_q: 0.168231
 104565/1000000: episode: 443, duration: 1.242s, episode steps: 166, steps per second: 134, episode reward: -25.284, mean reward: -0.152 [-11.084, 0.128], mean action: -0.015 [-1.082, 1.122], mean observation: 0.051 [-6.588, 4.008], loss: 0.747953, mean_absolute_error: 0.892067, mean_q: 0.007107
 104683/1000000: episode: 444, duration: 0.914s, episode steps: 118, steps per second: 129, episode reward: -1.647, mean reward: -0.014 [-10.038, 0.087], mean action: 0.006 [-1.069, 1.059], mean observation: 0.128 [-4.283, 3.924], loss: 0.764661, mean_absolute_error: 0.894298, mean_q: 0.010671
 104824/1000000: episode: 445, duration: 1.102s, episode steps: 141, steps per second: 128, episode reward: -23.088, mean reward: -0.164 [-10.592, 0.006], mean action: -0.016 [-1.113, 1.069], mean observation: 0.088 [-5.435, 3.890], loss: 0.794283, mean_absolute_error: 0.901702, mean_q: 0.243778
 105014/1000000: episode: 446, duration: 1.450s, episode steps: 190, steps per second: 131, episode reward: -48.522, mean reward: -0.255 [-11.644, 0.175], mean action: -0.008 [-1.088, 1.127], mean observation: -0.049 [-7.144, 3.832], loss: 0.802831, mean_absolute_error: 0.907997, mean_q: 0.083596
 105173/1000000: episode: 447, duration: 1.187s, episode steps: 159, steps per second: 134, episode reward: -20.778, mean reward: -0.131 [-10.671, 0.043], mean action: -0.033 [-1.149, 1.042], mean observation: 0.056 [-6.085, 3.871], loss: 0.811913, mean_absolute_error: 0.906650, mean_q: 0.148803
 105351/1000000: episode: 448, duration: 1.348s, episode steps: 178, steps per second: 132, episode reward: 14.081, mean reward: 0.079 [-10.955, 0.375], mean action: -0.006 [-1.090, 1.078], mean observation: 0.020 [-7.009, 3.968], loss: 0.787396, mean_absolute_error: 0.899112, mean_q: 0.036104
 105574/1000000: episode: 449, duration: 1.650s, episode steps: 223, steps per second: 135, episode reward: -19.336, mean reward: -0.087 [-11.855, 0.555], mean action: 0.001 [-1.083, 1.131], mean observation: -0.079 [-7.103, 4.012], loss: 0.843549, mean_absolute_error: 0.914364, mean_q: 0.101929
 105794/1000000: episode: 450, duration: 1.692s, episode steps: 220, steps per second: 130, episode reward: -64.513, mean reward: -0.293 [-12.115, 0.412], mean action: 0.000 [-1.107, 1.111], mean observation: -0.083 [-7.135, 4.008], loss: 0.767087, mean_absolute_error: 0.890316, mean_q: 0.260979
 106048/1000000: episode: 451, duration: 1.934s, episode steps: 254, steps per second: 131, episode reward: -39.858, mean reward: -0.157 [-11.797, 0.685], mean action: -0.005 [-1.093, 1.097], mean observation: -0.144 [-7.068, 4.020], loss: 0.773030, mean_absolute_error: 0.903596, mean_q: 0.158786
 106172/1000000: episode: 452, duration: 0.932s, episode steps: 124, steps per second: 133, episode reward: 0.672, mean reward: 0.005 [-10.090, 0.113], mean action: -0.011 [-1.109, 1.090], mean observation: 0.096 [-4.897, 3.790], loss: 0.810567, mean_absolute_error: 0.904765, mean_q: 0.181833
 106389/1000000: episode: 453, duration: 1.631s, episode steps: 217, steps per second: 133, episode reward: -89.341, mean reward: -0.412 [-12.176, 0.236], mean action: 0.004 [-1.069, 1.113], mean observation: -0.056 [-7.098, 3.888], loss: 0.760479, mean_absolute_error: 0.900859, mean_q: 0.024446
 106550/1000000: episode: 454, duration: 1.225s, episode steps: 161, steps per second: 131, episode reward: 18.314, mean reward: 0.114 [-10.649, 0.346], mean action: 0.007 [-1.055, 1.072], mean observation: 0.058 [-6.167, 3.961], loss: 0.794084, mean_absolute_error: 0.909997, mean_q: 0.193333
 106766/1000000: episode: 455, duration: 1.631s, episode steps: 216, steps per second: 132, episode reward: -38.579, mean reward: -0.179 [-11.606, 0.305], mean action: -0.004 [-1.080, 1.071], mean observation: -0.069 [-7.096, 3.917], loss: 0.831052, mean_absolute_error: 0.927770, mean_q: 0.121568
 106906/1000000: episode: 456, duration: 1.062s, episode steps: 140, steps per second: 132, episode reward: -20.329, mean reward: -0.145 [-10.542, 0.019], mean action: 0.014 [-1.035, 1.134], mean observation: 0.102 [-5.367, 3.919], loss: 0.850937, mean_absolute_error: 0.929020, mean_q: 0.099052
 107156/1000000: episode: 457, duration: 1.862s, episode steps: 250, steps per second: 134, episode reward: -64.678, mean reward: -0.259 [-12.050, 0.570], mean action: -0.251 [-1.134, 1.132], mean observation: -0.094 [-6.976, 4.000], loss: 0.824399, mean_absolute_error: 0.914335, mean_q: 0.092615
 107392/1000000: episode: 458, duration: 1.764s, episode steps: 236, steps per second: 134, episode reward: -11.478, mean reward: -0.049 [-11.632, 0.598], mean action: -0.335 [-1.086, 1.113], mean observation: -0.012 [-7.124, 4.569], loss: 0.801734, mean_absolute_error: 0.907501, mean_q: 0.233975
 107667/1000000: episode: 459, duration: 2.054s, episode steps: 275, steps per second: 134, episode reward: 1.436, mean reward: 0.005 [-10.705, 0.437], mean action: -0.568 [-1.111, 1.079], mean observation: 0.102 [-7.751, 5.862], loss: 0.829922, mean_absolute_error: 0.915057, mean_q: 0.147684
 107946/1000000: episode: 460, duration: 2.168s, episode steps: 279, steps per second: 129, episode reward: 8.359, mean reward: 0.030 [-11.080, 0.457], mean action: -0.616 [-1.123, 1.096], mean observation: 0.127 [-6.857, 5.525], loss: 0.800114, mean_absolute_error: 0.907824, mean_q: 0.048702
 108157/1000000: episode: 461, duration: 1.565s, episode steps: 211, steps per second: 135, episode reward: 5.358, mean reward: 0.025 [-10.742, 0.259], mean action: -0.587 [-1.122, 1.141], mean observation: 0.195 [-6.846, 5.706], loss: 0.822964, mean_absolute_error: 0.919306, mean_q: 0.171785
 108435/1000000: episode: 462, duration: 2.125s, episode steps: 278, steps per second: 131, episode reward: -35.684, mean reward: -0.128 [-11.255, 0.469], mean action: -0.554 [-1.063, 1.040], mean observation: 0.118 [-7.354, 5.613], loss: 0.800784, mean_absolute_error: 0.918076, mean_q: 0.060075
 108692/1000000: episode: 463, duration: 1.928s, episode steps: 257, steps per second: 133, episode reward: -35.417, mean reward: -0.138 [-11.541, 0.411], mean action: -0.626 [-1.111, 1.143], mean observation: 0.198 [-6.964, 5.525], loss: 0.826581, mean_absolute_error: 0.925921, mean_q: 0.080077
 108970/1000000: episode: 464, duration: 2.071s, episode steps: 278, steps per second: 134, episode reward: -58.484, mean reward: -0.210 [-11.269, 0.265], mean action: -0.655 [-1.081, 1.100], mean observation: 0.110 [-6.559, 5.782], loss: 0.809983, mean_absolute_error: 0.918709, mean_q: 0.304038
 109216/1000000: episode: 465, duration: 1.898s, episode steps: 246, steps per second: 130, episode reward: -2.648, mean reward: -0.011 [-11.055, 0.329], mean action: -0.755 [-1.115, 1.103], mean observation: 0.214 [-5.983, 5.455], loss: 0.788973, mean_absolute_error: 0.907728, mean_q: 0.193187
 109457/1000000: episode: 466, duration: 1.895s, episode steps: 241, steps per second: 127, episode reward: -53.175, mean reward: -0.221 [-11.340, 0.180], mean action: -0.806 [-1.141, 1.027], mean observation: 0.261 [-4.986, 5.287], loss: 0.790167, mean_absolute_error: 0.914745, mean_q: 0.280080
 109929/1000000: episode: 467, duration: 3.671s, episode steps: 472, steps per second: 129, episode reward: 6.109, mean reward: 0.013 [-11.194, 0.280], mean action: -0.935 [-1.121, 0.606], mean observation: 0.207 [-2.502, 3.458], loss: 0.784244, mean_absolute_error: 0.909754, mean_q: 0.200173
 110884/1000000: episode: 468, duration: 7.379s, episode steps: 955, steps per second: 129, episode reward: 203.676, mean reward: 0.213 [-10.473, 0.556], mean action: -0.997 [-1.145, -0.826], mean observation: 0.063 [-2.495, 1.035], loss: 0.802447, mean_absolute_error: 0.912925, mean_q: 0.278029
 111427/1000000: episode: 469, duration: 4.194s, episode steps: 543, steps per second: 129, episode reward: 142.038, mean reward: 0.262 [-10.047, 0.327], mean action: -0.985 [-1.133, -0.738], mean observation: 0.212 [-0.330, 1.726], loss: 0.810980, mean_absolute_error: 0.914212, mean_q: 0.287377
 112164/1000000: episode: 470, duration: 5.830s, episode steps: 737, steps per second: 126, episode reward: 129.537, mean reward: 0.176 [-10.745, 0.362], mean action: -0.982 [-1.157, 0.926], mean observation: 0.244 [-0.822, 1.911], loss: 0.800647, mean_absolute_error: 0.916364, mean_q: 0.431293
 112866/1000000: episode: 471, duration: 5.448s, episode steps: 702, steps per second: 129, episode reward: 228.301, mean reward: 0.325 [-11.454, 0.579], mean action: -0.914 [-1.183, 1.004], mean observation: 0.229 [-4.949, 2.756], loss: 0.807707, mean_absolute_error: 0.919709, mean_q: 0.452097
 113071/1000000: episode: 472, duration: 1.607s, episode steps: 205, steps per second: 128, episode reward: 18.258, mean reward: 0.089 [-10.237, 0.216], mean action: -0.791 [-1.151, 1.020], mean observation: 0.227 [-3.145, 4.868], loss: 0.848901, mean_absolute_error: 0.930754, mean_q: 0.393567
 113333/1000000: episode: 473, duration: 1.980s, episode steps: 262, steps per second: 132, episode reward: -62.310, mean reward: -0.238 [-11.510, 0.274], mean action: -0.617 [-1.121, 1.107], mean observation: 0.194 [-7.093, 5.514], loss: 0.786367, mean_absolute_error: 0.904583, mean_q: 0.520382
 113589/1000000: episode: 474, duration: 2.043s, episode steps: 256, steps per second: 125, episode reward: -128.537, mean reward: -0.502 [-12.408, 0.271], mean action: -0.425 [-1.094, 1.106], mean observation: -0.122 [-6.212, 3.706], loss: 0.799262, mean_absolute_error: 0.908338, mean_q: 0.549629
 113844/1000000: episode: 475, duration: 2.056s, episode steps: 255, steps per second: 124, episode reward: -133.991, mean reward: -0.525 [-12.517, 0.395], mean action: -0.287 [-1.105, 1.109], mean observation: -0.113 [-6.743, 3.890], loss: 0.858166, mean_absolute_error: 0.929658, mean_q: 0.487408
 114064/1000000: episode: 476, duration: 1.718s, episode steps: 220, steps per second: 128, episode reward: -54.966, mean reward: -0.250 [-11.892, 0.324], mean action: -0.298 [-1.099, 1.090], mean observation: -0.110 [-6.903, 3.499], loss: 0.791506, mean_absolute_error: 0.912176, mean_q: 0.465234
 114287/1000000: episode: 477, duration: 1.711s, episode steps: 223, steps per second: 130, episode reward: -39.929, mean reward: -0.179 [-11.756, 0.357], mean action: -0.123 [-1.090, 1.085], mean observation: -0.096 [-6.909, 3.739], loss: 0.788107, mean_absolute_error: 0.911433, mean_q: 0.433133
 114549/1000000: episode: 478, duration: 1.959s, episode steps: 262, steps per second: 134, episode reward: -96.767, mean reward: -0.369 [-12.050, 0.442], mean action: -0.127 [-1.127, 1.047], mean observation: -0.132 [-6.893, 3.650], loss: 0.781078, mean_absolute_error: 0.904112, mean_q: 0.461826
 114771/1000000: episode: 479, duration: 1.659s, episode steps: 222, steps per second: 134, episode reward: -21.997, mean reward: -0.099 [-11.839, 0.520], mean action: -0.215 [-1.136, 1.063], mean observation: -0.066 [-6.800, 3.771], loss: 0.811665, mean_absolute_error: 0.919402, mean_q: 0.412992
 114911/1000000: episode: 480, duration: 1.129s, episode steps: 140, steps per second: 124, episode reward: -0.660, mean reward: -0.005 [-10.296, 0.137], mean action: -0.242 [-1.111, 1.068], mean observation: 0.123 [-5.334, 3.694], loss: 0.820761, mean_absolute_error: 0.922588, mean_q: 0.508816
 115145/1000000: episode: 481, duration: 1.842s, episode steps: 234, steps per second: 127, episode reward: -112.245, mean reward: -0.480 [-12.120, 0.148], mean action: -0.313 [-1.093, 1.052], mean observation: -0.052 [-6.675, 3.771], loss: 0.769587, mean_absolute_error: 0.904530, mean_q: 0.487141
 115274/1000000: episode: 482, duration: 1.095s, episode steps: 129, steps per second: 118, episode reward: 8.294, mean reward: 0.064 [-10.123, 0.195], mean action: -0.211 [-1.063, 1.022], mean observation: 0.111 [-4.786, 3.715], loss: 0.821285, mean_absolute_error: 0.920490, mean_q: 0.443155
 115521/1000000: episode: 483, duration: 1.957s, episode steps: 247, steps per second: 126, episode reward: -39.614, mean reward: -0.160 [-12.040, 0.642], mean action: -0.120 [-1.081, 1.063], mean observation: -0.120 [-6.919, 3.663], loss: 0.790602, mean_absolute_error: 0.920578, mean_q: 0.379157
 115687/1000000: episode: 484, duration: 1.722s, episode steps: 166, steps per second: 96, episode reward: -7.667, mean reward: -0.046 [-10.779, 0.184], mean action: -0.155 [-1.095, 1.092], mean observation: 0.070 [-6.103, 3.851], loss: 0.804477, mean_absolute_error: 0.927382, mean_q: 0.364229
 115942/1000000: episode: 485, duration: 2.073s, episode steps: 255, steps per second: 123, episode reward: -120.597, mean reward: -0.473 [-12.123, 0.410], mean action: -0.035 [-1.080, 1.113], mean observation: -0.136 [-6.994, 3.807], loss: 0.775314, mean_absolute_error: 0.915394, mean_q: 0.625678
 116139/1000000: episode: 486, duration: 1.555s, episode steps: 197, steps per second: 127, episode reward: -37.206, mean reward: -0.189 [-11.629, 0.267], mean action: -0.039 [-1.130, 1.113], mean observation: -0.051 [-7.077, 4.045], loss: 0.762731, mean_absolute_error: 0.911396, mean_q: 0.603913
 116332/1000000: episode: 487, duration: 1.585s, episode steps: 193, steps per second: 122, episode reward: -8.708, mean reward: -0.045 [-11.491, 0.407], mean action: -0.046 [-1.100, 1.149], mean observation: -0.037 [-7.095, 3.773], loss: 0.785885, mean_absolute_error: 0.919401, mean_q: 0.502053
 116581/1000000: episode: 488, duration: 2.136s, episode steps: 249, steps per second: 117, episode reward: -39.114, mean reward: -0.157 [-11.941, 0.683], mean action: -0.033 [-1.102, 1.088], mean observation: -0.142 [-7.029, 3.926], loss: 0.787666, mean_absolute_error: 0.918213, mean_q: 0.460104
 116840/1000000: episode: 489, duration: 2.205s, episode steps: 259, steps per second: 117, episode reward: -95.806, mean reward: -0.370 [-11.927, 0.546], mean action: -0.006 [-1.069, 1.126], mean observation: -0.145 [-7.220, 3.955], loss: 0.800225, mean_absolute_error: 0.910175, mean_q: 0.508232
 117093/1000000: episode: 490, duration: 2.105s, episode steps: 253, steps per second: 120, episode reward: -158.565, mean reward: -0.627 [-12.133, 0.190], mean action: -0.013 [-1.100, 1.078], mean observation: -0.181 [-7.061, 4.000], loss: 0.790186, mean_absolute_error: 0.915677, mean_q: 0.536416
 117296/1000000: episode: 491, duration: 1.575s, episode steps: 203, steps per second: 129, episode reward: -20.131, mean reward: -0.099 [-11.721, 0.439], mean action: 0.009 [-1.094, 1.154], mean observation: -0.040 [-7.104, 3.973], loss: 0.787263, mean_absolute_error: 0.918489, mean_q: 0.497566
 117472/1000000: episode: 492, duration: 1.377s, episode steps: 176, steps per second: 128, episode reward: -22.186, mean reward: -0.126 [-11.247, 0.207], mean action: 0.011 [-1.110, 1.122], mean observation: -0.004 [-7.033, 3.941], loss: 0.779026, mean_absolute_error: 0.919685, mean_q: 0.518439
 117638/1000000: episode: 493, duration: 1.278s, episode steps: 166, steps per second: 130, episode reward: -35.130, mean reward: -0.212 [-11.009, 0.028], mean action: -0.001 [-1.084, 1.114], mean observation: 0.078 [-6.618, 3.894], loss: 0.820264, mean_absolute_error: 0.910870, mean_q: 0.429586
 117883/1000000: episode: 494, duration: 1.838s, episode steps: 245, steps per second: 133, episode reward: -109.200, mean reward: -0.446 [-12.127, 0.394], mean action: 0.018 [-1.089, 1.156], mean observation: -0.169 [-7.149, 3.808], loss: 0.764783, mean_absolute_error: 0.906329, mean_q: 0.515302
 118102/1000000: episode: 495, duration: 1.675s, episode steps: 219, steps per second: 131, episode reward: -86.137, mean reward: -0.393 [-12.204, 0.291], mean action: 0.003 [-1.136, 1.118], mean observation: -0.071 [-7.124, 4.057], loss: 0.798181, mean_absolute_error: 0.916063, mean_q: 0.484375
 118315/1000000: episode: 496, duration: 1.624s, episode steps: 213, steps per second: 131, episode reward: -35.197, mean reward: -0.165 [-11.634, 0.331], mean action: -0.021 [-1.171, 1.080], mean observation: -0.071 [-7.129, 4.028], loss: 0.821826, mean_absolute_error: 0.922200, mean_q: 0.606488
 118545/1000000: episode: 497, duration: 1.735s, episode steps: 230, steps per second: 133, episode reward: -85.278, mean reward: -0.371 [-12.210, 0.400], mean action: 0.015 [-1.104, 1.137], mean observation: -0.096 [-7.119, 3.868], loss: 0.810902, mean_absolute_error: 0.922223, mean_q: 0.505017
 118783/1000000: episode: 498, duration: 1.808s, episode steps: 238, steps per second: 132, episode reward: -129.692, mean reward: -0.545 [-12.356, 0.301], mean action: -0.021 [-1.121, 1.077], mean observation: -0.134 [-7.122, 4.026], loss: 0.815537, mean_absolute_error: 0.916668, mean_q: 0.528550
 119032/1000000: episode: 499, duration: 1.864s, episode steps: 249, steps per second: 134, episode reward: -72.547, mean reward: -0.291 [-11.669, 0.374], mean action: 0.011 [-1.097, 1.083], mean observation: -0.136 [-7.077, 3.877], loss: 0.791931, mean_absolute_error: 0.920925, mean_q: 0.437143
 119265/1000000: episode: 500, duration: 1.741s, episode steps: 233, steps per second: 134, episode reward: -42.772, mean reward: -0.184 [-11.782, 0.438], mean action: -0.010 [-1.094, 1.109], mean observation: -0.090 [-7.105, 3.964], loss: 0.787066, mean_absolute_error: 0.916451, mean_q: 0.606324
 119454/1000000: episode: 501, duration: 1.423s, episode steps: 189, steps per second: 133, episode reward: -13.575, mean reward: -0.072 [-11.293, 0.294], mean action: -0.011 [-1.061, 1.077], mean observation: 0.009 [-7.092, 3.959], loss: 0.779351, mean_absolute_error: 0.919481, mean_q: 0.647336
 119711/1000000: episode: 502, duration: 1.924s, episode steps: 257, steps per second: 134, episode reward: -135.325, mean reward: -0.527 [-11.884, 0.341], mean action: 0.029 [-1.071, 1.159], mean observation: -0.135 [-7.376, 4.010], loss: 0.798623, mean_absolute_error: 0.920540, mean_q: 0.589016
 119942/1000000: episode: 503, duration: 1.742s, episode steps: 231, steps per second: 133, episode reward: -117.020, mean reward: -0.507 [-12.369, 0.250], mean action: 0.006 [-1.083, 1.094], mean observation: -0.079 [-7.072, 4.013], loss: 0.782937, mean_absolute_error: 0.917419, mean_q: 0.488131
 120154/1000000: episode: 504, duration: 1.605s, episode steps: 212, steps per second: 132, episode reward: -27.976, mean reward: -0.132 [-11.554, 0.336], mean action: 0.007 [-1.081, 1.077], mean observation: -0.053 [-7.093, 3.887], loss: 0.784243, mean_absolute_error: 0.920976, mean_q: 0.763471
 120382/1000000: episode: 505, duration: 1.727s, episode steps: 228, steps per second: 132, episode reward: -73.324, mean reward: -0.322 [-11.540, 0.123], mean action: -0.303 [-1.098, 1.019], mean observation: -0.041 [-6.787, 4.041], loss: 0.782224, mean_absolute_error: 0.921109, mean_q: 0.422714
 120639/1000000: episode: 506, duration: 1.961s, episode steps: 257, steps per second: 131, episode reward: 9.821, mean reward: 0.038 [-10.787, 0.492], mean action: -0.511 [-1.174, 1.094], mean observation: 0.104 [-7.967, 6.011], loss: 0.803110, mean_absolute_error: 0.926776, mean_q: 0.591278
 120816/1000000: episode: 507, duration: 1.339s, episode steps: 177, steps per second: 132, episode reward: -5.980, mean reward: -0.034 [-10.598, 0.151], mean action: -0.479 [-1.077, 1.122], mean observation: 0.222 [-5.208, 5.990], loss: 0.786262, mean_absolute_error: 0.916436, mean_q: 0.585416
 121071/1000000: episode: 508, duration: 1.913s, episode steps: 255, steps per second: 133, episode reward: -36.440, mean reward: -0.143 [-11.152, 0.410], mean action: -0.523 [-1.120, 1.096], mean observation: 0.148 [-7.914, 5.986], loss: 0.804576, mean_absolute_error: 0.926430, mean_q: 0.506463
 121283/1000000: episode: 509, duration: 1.582s, episode steps: 212, steps per second: 134, episode reward: -30.140, mean reward: -0.142 [-10.881, 0.094], mean action: -0.502 [-1.123, 1.060], mean observation: 0.171 [-7.736, 6.004], loss: 0.816024, mean_absolute_error: 0.933261, mean_q: 0.521636
 121533/1000000: episode: 510, duration: 1.868s, episode steps: 250, steps per second: 134, episode reward: -6.040, mean reward: -0.024 [-11.073, 0.472], mean action: -0.505 [-1.096, 1.043], mean observation: 0.155 [-7.896, 5.974], loss: 0.803238, mean_absolute_error: 0.927016, mean_q: 0.549095
 121799/1000000: episode: 511, duration: 2.009s, episode steps: 266, steps per second: 132, episode reward: -43.493, mean reward: -0.164 [-11.063, 0.457], mean action: -0.515 [-1.113, 1.080], mean observation: 0.127 [-7.923, 6.002], loss: 0.778694, mean_absolute_error: 0.922300, mean_q: 0.645107
 122066/1000000: episode: 512, duration: 1.984s, episode steps: 267, steps per second: 135, episode reward: -33.543, mean reward: -0.126 [-10.683, 0.316], mean action: -0.505 [-1.119, 1.069], mean observation: 0.111 [-7.932, 6.020], loss: 0.769533, mean_absolute_error: 0.914922, mean_q: 0.702505
 122299/1000000: episode: 513, duration: 1.754s, episode steps: 233, steps per second: 133, episode reward: -34.526, mean reward: -0.148 [-11.069, 0.208], mean action: -0.500 [-1.079, 1.058], mean observation: 0.137 [-7.877, 6.002], loss: 0.784620, mean_absolute_error: 0.920278, mean_q: 0.478151
 122471/1000000: episode: 514, duration: 1.297s, episode steps: 172, steps per second: 133, episode reward: 6.026, mean reward: 0.035 [-10.350, 0.173], mean action: -0.516 [-1.177, 1.064], mean observation: 0.208 [-4.566, 5.978], loss: 0.795999, mean_absolute_error: 0.924996, mean_q: 0.649873
 122652/1000000: episode: 515, duration: 1.363s, episode steps: 181, steps per second: 133, episode reward: -13.385, mean reward: -0.074 [-10.747, 0.139], mean action: -0.484 [-1.081, 1.111], mean observation: 0.237 [-5.699, 6.032], loss: 0.810362, mean_absolute_error: 0.920265, mean_q: 0.619524
 122853/1000000: episode: 516, duration: 1.538s, episode steps: 201, steps per second: 131, episode reward: -22.901, mean reward: -0.114 [-10.913, 0.140], mean action: -0.487 [-1.120, 1.174], mean observation: 0.179 [-7.745, 6.135], loss: 0.819268, mean_absolute_error: 0.921917, mean_q: 0.676494
 123120/1000000: episode: 517, duration: 2.017s, episode steps: 267, steps per second: 132, episode reward: -4.461, mean reward: -0.017 [-10.780, 0.618], mean action: -0.491 [-1.150, 1.166], mean observation: 0.104 [-8.032, 6.093], loss: 0.820862, mean_absolute_error: 0.932075, mean_q: 0.561054
 123272/1000000: episode: 518, duration: 1.171s, episode steps: 152, steps per second: 130, episode reward: 9.254, mean reward: 0.061 [-10.161, 0.178], mean action: -0.502 [-1.110, 1.092], mean observation: 0.218 [-3.182, 5.981], loss: 0.801871, mean_absolute_error: 0.920090, mean_q: 0.610345
 123540/1000000: episode: 519, duration: 2.009s, episode steps: 268, steps per second: 133, episode reward: -33.212, mean reward: -0.124 [-10.650, 0.334], mean action: -0.523 [-1.165, 1.135], mean observation: 0.107 [-7.990, 6.062], loss: 0.797232, mean_absolute_error: 0.920732, mean_q: 0.537280
 123816/1000000: episode: 520, duration: 2.060s, episode steps: 276, steps per second: 134, episode reward: -61.316, mean reward: -0.222 [-10.881, 0.401], mean action: -0.497 [-1.121, 1.081], mean observation: 0.085 [-7.964, 6.042], loss: 0.797973, mean_absolute_error: 0.926023, mean_q: 0.645418
 124078/1000000: episode: 521, duration: 1.970s, episode steps: 262, steps per second: 133, episode reward: -49.914, mean reward: -0.191 [-10.700, 0.203], mean action: -0.491 [-1.107, 1.119], mean observation: 0.111 [-7.959, 6.029], loss: 0.814691, mean_absolute_error: 0.927685, mean_q: 0.522306
 124299/1000000: episode: 522, duration: 1.649s, episode steps: 221, steps per second: 134, episode reward: 13.630, mean reward: 0.062 [-10.855, 0.352], mean action: -0.500 [-1.123, 1.075], mean observation: 0.185 [-7.886, 5.963], loss: 0.821686, mean_absolute_error: 0.933507, mean_q: 0.580703
 124574/1000000: episode: 523, duration: 2.053s, episode steps: 275, steps per second: 134, episode reward: 30.354, mean reward: 0.110 [-10.478, 0.578], mean action: -0.491 [-1.102, 1.094], mean observation: 0.082 [-7.960, 6.018], loss: 0.818373, mean_absolute_error: 0.924188, mean_q: 0.663034
 124847/1000000: episode: 524, duration: 2.048s, episode steps: 273, steps per second: 133, episode reward: -4.494, mean reward: -0.016 [-10.841, 0.603], mean action: -0.528 [-1.168, 1.048], mean observation: 0.103 [-7.918, 5.946], loss: 0.805544, mean_absolute_error: 0.931570, mean_q: 0.547092
 125118/1000000: episode: 525, duration: 2.035s, episode steps: 271, steps per second: 133, episode reward: -52.822, mean reward: -0.195 [-10.967, 0.434], mean action: -0.495 [-1.103, 1.084], mean observation: 0.118 [-7.934, 6.045], loss: 0.804639, mean_absolute_error: 0.929356, mean_q: 0.563424
 125312/1000000: episode: 526, duration: 1.453s, episode steps: 194, steps per second: 133, episode reward: 13.452, mean reward: 0.069 [-10.793, 0.333], mean action: -0.488 [-1.125, 1.108], mean observation: 0.225 [-6.797, 6.003], loss: 0.783166, mean_absolute_error: 0.920423, mean_q: 0.655847
 125453/1000000: episode: 527, duration: 1.063s, episode steps: 141, steps per second: 133, episode reward: -7.879, mean reward: -0.056 [-10.445, 0.102], mean action: -0.008 [-1.102, 1.113], mean observation: 0.122 [-5.439, 3.944], loss: 0.756198, mean_absolute_error: 0.908997, mean_q: 0.511328
 125682/1000000: episode: 528, duration: 1.719s, episode steps: 229, steps per second: 133, episode reward: -83.007, mean reward: -0.362 [-12.032, 0.292], mean action: -0.016 [-1.103, 1.073], mean observation: -0.125 [-7.107, 3.895], loss: 0.786365, mean_absolute_error: 0.919792, mean_q: 0.518903
 125919/1000000: episode: 529, duration: 1.777s, episode steps: 237, steps per second: 133, episode reward: -59.473, mean reward: -0.251 [-11.796, 0.394], mean action: 0.009 [-1.085, 1.118], mean observation: -0.095 [-7.090, 3.985], loss: 0.800610, mean_absolute_error: 0.930333, mean_q: 0.484783
 126115/1000000: episode: 530, duration: 1.466s, episode steps: 196, steps per second: 134, episode reward: -30.484, mean reward: -0.156 [-11.469, 0.259], mean action: 0.015 [-1.074, 1.159], mean observation: -0.004 [-7.117, 4.056], loss: 0.815622, mean_absolute_error: 0.921024, mean_q: 0.619302
 126360/1000000: episode: 531, duration: 1.831s, episode steps: 245, steps per second: 134, episode reward: -101.178, mean reward: -0.413 [-12.173, 0.448], mean action: -0.013 [-1.164, 1.077], mean observation: -0.119 [-7.110, 3.928], loss: 0.782490, mean_absolute_error: 0.919599, mean_q: 0.673484
 126481/1000000: episode: 532, duration: 0.908s, episode steps: 121, steps per second: 133, episode reward: 3.339, mean reward: 0.028 [-10.053, 0.136], mean action: -0.021 [-1.127, 1.075], mean observation: 0.146 [-4.663, 3.902], loss: 0.775635, mean_absolute_error: 0.924688, mean_q: 0.574494
 126738/1000000: episode: 533, duration: 1.932s, episode steps: 257, steps per second: 133, episode reward: -143.059, mean reward: -0.557 [-12.033, 0.374], mean action: -0.005 [-1.096, 1.105], mean observation: -0.149 [-7.391, 3.898], loss: 0.831114, mean_absolute_error: 0.937363, mean_q: 0.657592
 126980/1000000: episode: 534, duration: 1.822s, episode steps: 242, steps per second: 133, episode reward: -91.975, mean reward: -0.380 [-12.056, 0.398], mean action: 0.001 [-1.086, 1.120], mean observation: -0.099 [-7.127, 3.961], loss: 0.746182, mean_absolute_error: 0.913567, mean_q: 0.623054
 127236/1000000: episode: 535, duration: 1.956s, episode steps: 256, steps per second: 131, episode reward: -66.156, mean reward: -0.258 [-11.746, 0.556], mean action: -0.004 [-1.121, 1.110], mean observation: -0.161 [-7.087, 4.091], loss: 0.791712, mean_absolute_error: 0.919624, mean_q: 0.732559
 127457/1000000: episode: 536, duration: 1.683s, episode steps: 221, steps per second: 131, episode reward: -35.678, mean reward: -0.161 [-11.733, 0.380], mean action: 0.002 [-1.147, 1.068], mean observation: -0.084 [-7.069, 3.913], loss: 0.776446, mean_absolute_error: 0.926875, mean_q: 0.588334
 127664/1000000: episode: 537, duration: 1.557s, episode steps: 207, steps per second: 133, episode reward: -55.769, mean reward: -0.269 [-11.714, 0.211], mean action: 0.006 [-1.082, 1.085], mean observation: -0.019 [-7.094, 3.919], loss: 0.762245, mean_absolute_error: 0.915014, mean_q: 0.743324
 127872/1000000: episode: 538, duration: 1.561s, episode steps: 208, steps per second: 133, episode reward: -21.095, mean reward: -0.101 [-11.627, 0.395], mean action: 0.004 [-1.082, 1.073], mean observation: -0.026 [-7.083, 3.990], loss: 0.791810, mean_absolute_error: 0.922669, mean_q: 0.711435
 128039/1000000: episode: 539, duration: 1.256s, episode steps: 167, steps per second: 133, episode reward: -6.024, mean reward: -0.036 [-10.714, 0.166], mean action: -0.010 [-1.107, 1.070], mean observation: 0.058 [-6.476, 4.086], loss: 0.780644, mean_absolute_error: 0.914808, mean_q: 0.631217
 128294/1000000: episode: 540, duration: 1.903s, episode steps: 255, steps per second: 134, episode reward: -128.138, mean reward: -0.503 [-11.904, 0.277], mean action: -0.019 [-1.132, 1.052], mean observation: -0.114 [-7.075, 3.973], loss: 0.779265, mean_absolute_error: 0.921229, mean_q: 0.673787
 128548/1000000: episode: 541, duration: 1.907s, episode steps: 254, steps per second: 133, episode reward: -102.043, mean reward: -0.402 [-11.564, 0.245], mean action: 0.007 [-1.097, 1.103], mean observation: -0.165 [-7.128, 3.891], loss: 0.783395, mean_absolute_error: 0.923277, mean_q: 0.709131
 128786/1000000: episode: 542, duration: 1.782s, episode steps: 238, steps per second: 134, episode reward: -80.866, mean reward: -0.340 [-12.155, 0.463], mean action: 0.011 [-1.064, 1.103], mean observation: -0.130 [-7.119, 3.990], loss: 0.792379, mean_absolute_error: 0.927102, mean_q: 0.641368
 129039/1000000: episode: 543, duration: 1.891s, episode steps: 253, steps per second: 134, episode reward: -99.935, mean reward: -0.395 [-12.074, 0.536], mean action: 0.006 [-1.089, 1.071], mean observation: -0.142 [-7.096, 3.979], loss: 0.771570, mean_absolute_error: 0.920239, mean_q: 0.643845
 129286/1000000: episode: 544, duration: 1.859s, episode steps: 247, steps per second: 133, episode reward: -59.968, mean reward: -0.243 [-11.849, 0.572], mean action: 0.006 [-1.153, 1.105], mean observation: -0.126 [-7.112, 4.060], loss: 0.802155, mean_absolute_error: 0.928641, mean_q: 0.593040
 129403/1000000: episode: 545, duration: 0.889s, episode steps: 117, steps per second: 132, episode reward: -3.747, mean reward: -0.032 [-10.142, 0.091], mean action: 0.026 [-1.053, 1.098], mean observation: 0.134 [-4.500, 3.893], loss: 0.775106, mean_absolute_error: 0.920135, mean_q: 0.552259
 129655/1000000: episode: 546, duration: 1.899s, episode steps: 252, steps per second: 133, episode reward: -47.530, mean reward: -0.189 [-11.892, 0.700], mean action: -0.020 [-1.087, 1.132], mean observation: -0.141 [-7.088, 3.935], loss: 0.794221, mean_absolute_error: 0.922339, mean_q: 0.561432
 129868/1000000: episode: 547, duration: 1.591s, episode steps: 213, steps per second: 134, episode reward: -55.949, mean reward: -0.263 [-11.850, 0.287], mean action: -0.007 [-1.148, 1.094], mean observation: -0.085 [-7.111, 4.024], loss: 0.823501, mean_absolute_error: 0.932972, mean_q: 0.562318
 130114/1000000: episode: 548, duration: 1.833s, episode steps: 246, steps per second: 134, episode reward: -81.194, mean reward: -0.330 [-11.618, 0.285], mean action: 0.007 [-1.115, 1.148], mean observation: -0.139 [-7.128, 3.925], loss: 0.802232, mean_absolute_error: 0.928631, mean_q: 0.694132
 130305/1000000: episode: 549, duration: 1.439s, episode steps: 191, steps per second: 133, episode reward: -43.073, mean reward: -0.226 [-11.508, 0.159], mean action: -0.002 [-1.101, 1.077], mean observation: -0.048 [-7.135, 3.865], loss: 0.797461, mean_absolute_error: 0.929033, mean_q: 0.751735
 130498/1000000: episode: 550, duration: 1.450s, episode steps: 193, steps per second: 133, episode reward: -14.525, mean reward: -0.075 [-11.434, 0.333], mean action: -0.001 [-1.085, 1.059], mean observation: -0.027 [-7.056, 3.997], loss: 0.760020, mean_absolute_error: 0.916419, mean_q: 0.680756
 130741/1000000: episode: 551, duration: 1.831s, episode steps: 243, steps per second: 133, episode reward: -113.796, mean reward: -0.468 [-12.230, 0.328], mean action: 0.024 [-1.071, 1.119], mean observation: -0.141 [-7.060, 4.020], loss: 0.776431, mean_absolute_error: 0.913101, mean_q: 0.698120
 130962/1000000: episode: 552, duration: 1.750s, episode steps: 221, steps per second: 126, episode reward: -56.987, mean reward: -0.258 [-12.008, 0.386], mean action: -0.012 [-1.113, 1.078], mean observation: -0.092 [-7.103, 3.987], loss: 0.778027, mean_absolute_error: 0.920313, mean_q: 0.731632
 131138/1000000: episode: 553, duration: 1.397s, episode steps: 176, steps per second: 126, episode reward: -40.942, mean reward: -0.233 [-11.243, 0.068], mean action: 0.009 [-1.097, 1.094], mean observation: 0.049 [-7.055, 3.896], loss: 0.780536, mean_absolute_error: 0.926559, mean_q: 0.613938
 131347/1000000: episode: 554, duration: 1.956s, episode steps: 209, steps per second: 107, episode reward: -43.438, mean reward: -0.208 [-11.741, 0.314], mean action: -0.008 [-1.163, 1.114], mean observation: -0.035 [-7.161, 3.879], loss: 0.768696, mean_absolute_error: 0.913301, mean_q: 0.732185
 131601/1000000: episode: 555, duration: 2.064s, episode steps: 254, steps per second: 123, episode reward: -113.481, mean reward: -0.447 [-11.960, 0.377], mean action: -0.004 [-1.091, 1.085], mean observation: -0.177 [-7.093, 3.952], loss: 0.820647, mean_absolute_error: 0.927222, mean_q: 0.618038
 131789/1000000: episode: 556, duration: 1.456s, episode steps: 188, steps per second: 129, episode reward: 4.797, mean reward: 0.026 [-11.300, 0.424], mean action: 0.003 [-1.079, 1.097], mean observation: -0.010 [-7.109, 3.862], loss: 0.789594, mean_absolute_error: 0.930521, mean_q: 0.703732
 132043/1000000: episode: 557, duration: 2.168s, episode steps: 254, steps per second: 117, episode reward: -78.550, mean reward: -0.309 [-11.503, 0.382], mean action: -0.004 [-1.150, 1.121], mean observation: -0.145 [-7.160, 4.048], loss: 0.775145, mean_absolute_error: 0.923354, mean_q: 0.733624
 132182/1000000: episode: 558, duration: 1.097s, episode steps: 139, steps per second: 127, episode reward: 17.759, mean reward: 0.128 [-10.225, 0.278], mean action: -0.020 [-1.135, 1.154], mean observation: 0.116 [-5.274, 4.103], loss: 0.784564, mean_absolute_error: 0.934053, mean_q: 0.716575
 132412/1000000: episode: 559, duration: 1.774s, episode steps: 230, steps per second: 130, episode reward: -24.832, mean reward: -0.108 [-11.854, 0.564], mean action: 0.022 [-1.105, 1.174], mean observation: -0.102 [-7.116, 3.947], loss: 0.799677, mean_absolute_error: 0.929265, mean_q: 0.756886
 132658/1000000: episode: 560, duration: 2.011s, episode steps: 246, steps per second: 122, episode reward: -121.622, mean reward: -0.494 [-12.141, 0.309], mean action: -0.009 [-1.139, 1.114], mean observation: -0.166 [-7.109, 3.902], loss: 0.750881, mean_absolute_error: 0.916533, mean_q: 0.724701
 132786/1000000: episode: 561, duration: 0.996s, episode steps: 128, steps per second: 129, episode reward: 8.590, mean reward: 0.067 [-10.107, 0.189], mean action: -0.019 [-1.076, 1.114], mean observation: 0.087 [-5.091, 3.800], loss: 0.763749, mean_absolute_error: 0.911226, mean_q: 0.703405
 132986/1000000: episode: 562, duration: 1.512s, episode steps: 200, steps per second: 132, episode reward: -60.862, mean reward: -0.304 [-11.860, 0.202], mean action: 0.003 [-1.133, 1.122], mean observation: -0.065 [-7.151, 3.868], loss: 0.742655, mean_absolute_error: 0.909045, mean_q: 0.737984
 133204/1000000: episode: 563, duration: 1.622s, episode steps: 218, steps per second: 134, episode reward: -64.104, mean reward: -0.294 [-11.777, 0.228], mean action: -0.266 [-1.110, 1.058], mean observation: -0.061 [-6.867, 4.127], loss: 0.778122, mean_absolute_error: 0.924579, mean_q: 0.729788
 133476/1000000: episode: 564, duration: 2.046s, episode steps: 272, steps per second: 133, episode reward: -25.240, mean reward: -0.093 [-10.588, 0.350], mean action: -0.486 [-1.063, 1.070], mean observation: 0.070 [-7.938, 6.024], loss: 0.773593, mean_absolute_error: 0.919201, mean_q: 0.790872
 133729/1000000: episode: 565, duration: 2.002s, episode steps: 253, steps per second: 126, episode reward: -82.571, mean reward: -0.326 [-11.317, 0.223], mean action: -0.497 [-1.119, 1.117], mean observation: 0.126 [-7.970, 6.045], loss: 0.781721, mean_absolute_error: 0.919159, mean_q: 0.777411
 133843/1000000: episode: 566, duration: 0.854s, episode steps: 114, steps per second: 133, episode reward: -0.950, mean reward: -0.008 [-10.039, 0.099], mean action: 0.002 [-1.030, 1.045], mean observation: 0.102 [-4.187, 3.710], loss: 0.763635, mean_absolute_error: 0.913038, mean_q: 0.801087
 134088/1000000: episode: 567, duration: 1.821s, episode steps: 245, steps per second: 135, episode reward: -84.817, mean reward: -0.346 [-11.861, 0.338], mean action: 0.014 [-1.044, 1.115], mean observation: -0.096 [-7.039, 4.017], loss: 0.798623, mean_absolute_error: 0.925501, mean_q: 0.676366
 134330/1000000: episode: 568, duration: 1.800s, episode steps: 242, steps per second: 134, episode reward: -70.777, mean reward: -0.292 [-11.717, 0.292], mean action: -0.023 [-1.094, 1.038], mean observation: -0.110 [-7.035, 3.909], loss: 0.772886, mean_absolute_error: 0.915434, mean_q: 0.781702
 134543/1000000: episode: 569, duration: 1.596s, episode steps: 213, steps per second: 133, episode reward: -51.708, mean reward: -0.243 [-11.548, 0.193], mean action: -0.007 [-1.078, 1.068], mean observation: -0.041 [-7.064, 4.044], loss: 0.772601, mean_absolute_error: 0.924248, mean_q: 0.763536
 134766/1000000: episode: 570, duration: 1.688s, episode steps: 223, steps per second: 132, episode reward: -67.923, mean reward: -0.305 [-12.038, 0.333], mean action: -0.026 [-1.127, 1.057], mean observation: -0.093 [-7.067, 4.052], loss: 0.745473, mean_absolute_error: 0.908390, mean_q: 0.819288
 134989/1000000: episode: 571, duration: 1.752s, episode steps: 223, steps per second: 127, episode reward: -30.738, mean reward: -0.138 [-11.910, 0.533], mean action: 0.019 [-1.105, 1.107], mean observation: -0.096 [-7.117, 3.969], loss: 0.776533, mean_absolute_error: 0.922448, mean_q: 0.784270
 135204/1000000: episode: 572, duration: 1.605s, episode steps: 215, steps per second: 134, episode reward: -68.890, mean reward: -0.320 [-11.997, 0.261], mean action: 0.001 [-1.065, 1.111], mean observation: -0.088 [-7.097, 3.883], loss: 0.818860, mean_absolute_error: 0.937356, mean_q: 0.797571
 135365/1000000: episode: 573, duration: 1.205s, episode steps: 161, steps per second: 134, episode reward: 0.771, mean reward: 0.005 [-10.666, 0.212], mean action: -0.040 [-1.094, 1.045], mean observation: 0.092 [-5.995, 4.139], loss: 0.799169, mean_absolute_error: 0.925604, mean_q: 0.836283
 135623/1000000: episode: 574, duration: 1.927s, episode steps: 258, steps per second: 134, episode reward: -32.654, mean reward: -0.127 [-11.994, 0.669], mean action: -0.148 [-1.084, 1.060], mean observation: -0.092 [-6.932, 4.353], loss: 0.806037, mean_absolute_error: 0.932608, mean_q: 0.794986
 135885/1000000: episode: 575, duration: 1.954s, episode steps: 262, steps per second: 134, episode reward: -45.449, mean reward: -0.173 [-11.507, 0.470], mean action: -0.248 [-1.105, 1.110], mean observation: 0.067 [-7.754, 5.642], loss: 0.802979, mean_absolute_error: 0.931835, mean_q: 0.717448
 136073/1000000: episode: 576, duration: 1.414s, episode steps: 188, steps per second: 133, episode reward: 5.337, mean reward: 0.028 [-11.198, 0.388], mean action: -0.036 [-1.127, 1.076], mean observation: 0.022 [-7.066, 4.229], loss: 0.751734, mean_absolute_error: 0.921350, mean_q: 0.727118
 136300/1000000: episode: 577, duration: 1.690s, episode steps: 227, steps per second: 134, episode reward: -65.658, mean reward: -0.289 [-11.724, 0.229], mean action: -0.023 [-1.100, 1.118], mean observation: -0.081 [-7.070, 4.111], loss: 0.770217, mean_absolute_error: 0.913777, mean_q: 0.841411
 136483/1000000: episode: 578, duration: 1.360s, episode steps: 183, steps per second: 135, episode reward: -10.273, mean reward: -0.056 [-11.111, 0.249], mean action: -0.018 [-1.082, 1.082], mean observation: 0.027 [-7.088, 3.968], loss: 0.796323, mean_absolute_error: 0.930898, mean_q: 0.828436
 136704/1000000: episode: 579, duration: 1.751s, episode steps: 221, steps per second: 126, episode reward: -90.039, mean reward: -0.407 [-11.993, 0.193], mean action: -0.013 [-1.187, 1.119], mean observation: -0.049 [-7.150, 3.882], loss: 0.769408, mean_absolute_error: 0.914781, mean_q: 0.889675
 136906/1000000: episode: 580, duration: 1.561s, episode steps: 202, steps per second: 129, episode reward: -4.829, mean reward: -0.024 [-11.545, 0.458], mean action: 0.023 [-1.116, 1.108], mean observation: -0.019 [-7.090, 4.081], loss: 0.734666, mean_absolute_error: 0.899839, mean_q: 0.911942
 137073/1000000: episode: 581, duration: 1.248s, episode steps: 167, steps per second: 134, episode reward: 8.799, mean reward: 0.053 [-10.880, 0.331], mean action: -0.015 [-1.128, 1.105], mean observation: 0.034 [-6.676, 3.989], loss: 0.762889, mean_absolute_error: 0.908405, mean_q: 0.819271
 137302/1000000: episode: 582, duration: 1.701s, episode steps: 229, steps per second: 135, episode reward: -70.099, mean reward: -0.306 [-12.155, 0.448], mean action: -0.011 [-1.112, 1.128], mean observation: -0.112 [-7.136, 3.839], loss: 0.759850, mean_absolute_error: 0.915155, mean_q: 0.781129
 137415/1000000: episode: 583, duration: 0.859s, episode steps: 113, steps per second: 132, episode reward: -0.677, mean reward: -0.006 [-10.033, 0.101], mean action: -0.003 [-1.055, 1.052], mean observation: 0.103 [-4.196, 3.750], loss: 0.784441, mean_absolute_error: 0.921785, mean_q: 1.036279
 137615/1000000: episode: 584, duration: 1.486s, episode steps: 200, steps per second: 135, episode reward: -19.971, mean reward: -0.100 [-11.499, 0.335], mean action: -0.020 [-1.083, 1.036], mean observation: -0.012 [-7.066, 3.880], loss: 0.737635, mean_absolute_error: 0.907878, mean_q: 0.971182
 137840/1000000: episode: 585, duration: 1.664s, episode steps: 225, steps per second: 135, episode reward: -79.179, mean reward: -0.352 [-12.196, 0.369], mean action: 0.008 [-1.106, 1.121], mean observation: -0.076 [-7.091, 3.978], loss: 0.756146, mean_absolute_error: 0.913428, mean_q: 0.981929
 138055/1000000: episode: 586, duration: 1.594s, episode steps: 215, steps per second: 135, episode reward: -60.158, mean reward: -0.280 [-11.583, 0.169], mean action: -0.019 [-1.159, 1.072], mean observation: -0.055 [-7.087, 3.946], loss: 0.791572, mean_absolute_error: 0.923894, mean_q: 0.864669
 138280/1000000: episode: 587, duration: 1.679s, episode steps: 225, steps per second: 134, episode reward: -132.245, mean reward: -0.588 [-12.425, 0.153], mean action: 0.006 [-1.106, 1.128], mean observation: -0.085 [-7.114, 4.011], loss: 0.737157, mean_absolute_error: 0.912112, mean_q: 0.949310
 138414/1000000: episode: 588, duration: 1.001s, episode steps: 134, steps per second: 134, episode reward: -5.588, mean reward: -0.042 [-10.305, 0.096], mean action: -0.023 [-1.063, 1.022], mean observation: 0.131 [-5.209, 3.854], loss: 0.834614, mean_absolute_error: 0.927296, mean_q: 1.092313
 138648/1000000: episode: 589, duration: 1.826s, episode steps: 234, steps per second: 128, episode reward: -107.662, mean reward: -0.460 [-12.252, 0.316], mean action: -0.010 [-1.124, 1.066], mean observation: -0.093 [-7.141, 3.886], loss: 0.771649, mean_absolute_error: 0.913515, mean_q: 0.974205
 138872/1000000: episode: 590, duration: 1.659s, episode steps: 224, steps per second: 135, episode reward: -45.832, mean reward: -0.205 [-11.660, 0.304], mean action: -0.029 [-1.127, 1.116], mean observation: -0.077 [-7.063, 3.895], loss: 0.783775, mean_absolute_error: 0.922715, mean_q: 0.963620
 139014/1000000: episode: 591, duration: 1.068s, episode steps: 142, steps per second: 133, episode reward: 21.139, mean reward: 0.149 [-10.141, 0.288], mean action: -0.206 [-1.070, 1.074], mean observation: 0.109 [-4.215, 4.607], loss: 0.765179, mean_absolute_error: 0.909626, mean_q: 1.052912
 139254/1000000: episode: 592, duration: 1.794s, episode steps: 240, steps per second: 134, episode reward: -104.948, mean reward: -0.437 [-12.094, 0.180], mean action: -0.146 [-1.114, 1.175], mean observation: -0.045 [-7.192, 4.810], loss: 0.769751, mean_absolute_error: 0.918014, mean_q: 1.073570
 139416/1000000: episode: 593, duration: 1.211s, episode steps: 162, steps per second: 134, episode reward: 24.822, mean reward: 0.153 [-10.293, 0.312], mean action: -0.321 [-1.119, 1.061], mean observation: 0.130 [-4.745, 5.064], loss: 0.760358, mean_absolute_error: 0.909712, mean_q: 0.929126
 139675/1000000: episode: 594, duration: 1.923s, episode steps: 259, steps per second: 135, episode reward: -47.651, mean reward: -0.184 [-11.157, 0.277], mean action: -0.242 [-1.081, 1.092], mean observation: 0.125 [-7.971, 6.045], loss: 0.750740, mean_absolute_error: 0.899522, mean_q: 1.118378
 139945/1000000: episode: 595, duration: 2.034s, episode steps: 270, steps per second: 133, episode reward: -48.751, mean reward: -0.181 [-11.710, 0.469], mean action: -0.187 [-1.084, 1.066], mean observation: 0.065 [-7.973, 5.937], loss: 0.768102, mean_absolute_error: 0.915130, mean_q: 1.069256
 140215/1000000: episode: 596, duration: 2.030s, episode steps: 270, steps per second: 133, episode reward: 3.279, mean reward: 0.012 [-11.591, 0.690], mean action: -0.202 [-1.124, 1.106], mean observation: 0.067 [-8.009, 5.930], loss: 0.790218, mean_absolute_error: 0.919133, mean_q: 0.971263
 140442/1000000: episode: 597, duration: 1.704s, episode steps: 227, steps per second: 133, episode reward: 3.269, mean reward: 0.014 [-11.123, 0.358], mean action: -0.224 [-1.124, 1.117], mean observation: 0.146 [-8.008, 5.935], loss: 0.772645, mean_absolute_error: 0.918276, mean_q: 1.033800
 140672/1000000: episode: 598, duration: 1.730s, episode steps: 230, steps per second: 133, episode reward: -44.007, mean reward: -0.191 [-11.724, 0.383], mean action: 0.005 [-1.097, 1.085], mean observation: -0.085 [-7.077, 4.002], loss: 0.736847, mean_absolute_error: 0.901565, mean_q: 1.140161
 140861/1000000: episode: 599, duration: 1.431s, episode steps: 189, steps per second: 132, episode reward: -5.744, mean reward: -0.030 [-11.368, 0.378], mean action: 0.007 [-1.088, 1.132], mean observation: 0.016 [-7.096, 4.214], loss: 0.742716, mean_absolute_error: 0.906291, mean_q: 0.974681
 141113/1000000: episode: 600, duration: 1.951s, episode steps: 252, steps per second: 129, episode reward: -112.868, mean reward: -0.448 [-11.585, 0.134], mean action: -0.037 [-1.145, 1.101], mean observation: -0.121 [-7.060, 4.200], loss: 0.739735, mean_absolute_error: 0.903165, mean_q: 1.089007
 141315/1000000: episode: 601, duration: 1.553s, episode steps: 202, steps per second: 130, episode reward: -8.892, mean reward: -0.044 [-11.327, 0.351], mean action: -0.248 [-1.149, 1.032], mean observation: 0.068 [-7.185, 5.006], loss: 0.773340, mean_absolute_error: 0.914472, mean_q: 1.119546
 141544/1000000: episode: 602, duration: 1.750s, episode steps: 229, steps per second: 131, episode reward: -31.570, mean reward: -0.138 [-11.190, 0.257], mean action: -0.393 [-1.082, 1.142], mean observation: 0.153 [-7.982, 6.049], loss: 0.765868, mean_absolute_error: 0.900562, mean_q: 1.139713
 141798/1000000: episode: 603, duration: 1.939s, episode steps: 254, steps per second: 131, episode reward: -59.386, mean reward: -0.234 [-11.945, 0.630], mean action: -0.036 [-1.113, 1.088], mean observation: -0.141 [-7.040, 4.165], loss: 0.759188, mean_absolute_error: 0.904068, mean_q: 1.078264
 141990/1000000: episode: 604, duration: 1.445s, episode steps: 192, steps per second: 133, episode reward: -16.548, mean reward: -0.086 [-11.422, 0.310], mean action: -0.046 [-1.080, 1.130], mean observation: -0.006 [-7.037, 4.124], loss: 0.775376, mean_absolute_error: 0.909852, mean_q: 1.022252
 142133/1000000: episode: 605, duration: 1.081s, episode steps: 143, steps per second: 132, episode reward: 13.851, mean reward: 0.097 [-10.157, 0.221], mean action: -0.136 [-1.102, 1.081], mean observation: 0.122 [-4.545, 4.466], loss: 0.780052, mean_absolute_error: 0.916605, mean_q: 0.988757
 142363/1000000: episode: 606, duration: 1.717s, episode steps: 230, steps per second: 134, episode reward: -50.922, mean reward: -0.221 [-12.056, 0.458], mean action: -0.090 [-1.083, 1.073], mean observation: -0.026 [-7.078, 4.593], loss: 0.762958, mean_absolute_error: 0.911736, mean_q: 1.180988
 142541/1000000: episode: 607, duration: 1.332s, episode steps: 178, steps per second: 134, episode reward: 17.847, mean reward: 0.100 [-10.888, 0.383], mean action: -0.061 [-1.061, 1.101], mean observation: 0.046 [-6.708, 4.267], loss: 0.760904, mean_absolute_error: 0.909469, mean_q: 1.235365
 142724/1000000: episode: 608, duration: 1.364s, episode steps: 183, steps per second: 134, episode reward: -30.906, mean reward: -0.169 [-11.265, 0.150], mean action: -0.061 [-1.102, 1.080], mean observation: -0.002 [-6.998, 4.213], loss: 0.767898, mean_absolute_error: 0.916353, mean_q: 1.316395
 142954/1000000: episode: 609, duration: 1.726s, episode steps: 230, steps per second: 133, episode reward: -29.693, mean reward: -0.129 [-11.975, 0.542], mean action: -0.080 [-1.090, 1.113], mean observation: -0.035 [-6.968, 4.432], loss: 0.780567, mean_absolute_error: 0.918828, mean_q: 1.236384
 143172/1000000: episode: 610, duration: 1.688s, episode steps: 218, steps per second: 129, episode reward: -60.737, mean reward: -0.279 [-12.000, 0.316], mean action: -0.087 [-1.077, 1.096], mean observation: -0.004 [-7.029, 4.411], loss: 0.771932, mean_absolute_error: 0.915147, mean_q: 1.138914
 143436/1000000: episode: 611, duration: 2.074s, episode steps: 264, steps per second: 127, episode reward: -103.583, mean reward: -0.392 [-11.862, 0.435], mean action: -0.403 [-1.157, 1.149], mean observation: -0.030 [-7.290, 4.791], loss: 0.780576, mean_absolute_error: 0.915069, mean_q: 1.096059
 143576/1000000: episode: 612, duration: 1.070s, episode steps: 140, steps per second: 131, episode reward: 0.265, mean reward: 0.002 [-10.059, 0.093], mean action: -0.486 [-1.106, 1.053], mean observation: 0.194 [-2.400, 5.693], loss: 0.761796, mean_absolute_error: 0.911627, mean_q: 1.119665
 143811/1000000: episode: 613, duration: 1.783s, episode steps: 235, steps per second: 132, episode reward: -19.839, mean reward: -0.084 [-11.157, 0.330], mean action: -0.498 [-1.077, 1.069], mean observation: 0.184 [-7.880, 5.985], loss: 0.788346, mean_absolute_error: 0.912463, mean_q: 1.266356
 144063/1000000: episode: 614, duration: 1.896s, episode steps: 252, steps per second: 133, episode reward: -71.919, mean reward: -0.285 [-11.341, 0.254], mean action: -0.492 [-1.070, 1.114], mean observation: 0.166 [-7.900, 5.994], loss: 0.779650, mean_absolute_error: 0.911152, mean_q: 1.179657
 144262/1000000: episode: 615, duration: 1.505s, episode steps: 199, steps per second: 132, episode reward: -18.008, mean reward: -0.090 [-10.751, 0.114], mean action: -0.527 [-1.155, 1.090], mean observation: 0.185 [-7.292, 6.053], loss: 0.759502, mean_absolute_error: 0.913693, mean_q: 1.158488
 144530/1000000: episode: 616, duration: 2.043s, episode steps: 268, steps per second: 131, episode reward: -42.620, mean reward: -0.159 [-11.012, 0.399], mean action: -0.518 [-1.110, 1.009], mean observation: 0.100 [-7.845, 5.952], loss: 0.765158, mean_absolute_error: 0.896624, mean_q: 1.174491
 144781/1000000: episode: 617, duration: 1.864s, episode steps: 251, steps per second: 135, episode reward: -44.071, mean reward: -0.176 [-11.136, 0.286], mean action: -0.501 [-1.085, 1.097], mean observation: 0.117 [-7.919, 5.960], loss: 0.790091, mean_absolute_error: 0.905769, mean_q: 1.212873
 145019/1000000: episode: 618, duration: 1.767s, episode steps: 238, steps per second: 135, episode reward: -88.002, mean reward: -0.370 [-11.503, 0.101], mean action: -0.506 [-1.074, 1.021], mean observation: 0.168 [-7.901, 5.989], loss: 0.756925, mean_absolute_error: 0.907016, mean_q: 1.285921
 145278/1000000: episode: 619, duration: 1.923s, episode steps: 259, steps per second: 135, episode reward: -14.996, mean reward: -0.058 [-10.603, 0.285], mean action: -0.511 [-1.111, 1.076], mean observation: 0.107 [-7.965, 5.999], loss: 0.756467, mean_absolute_error: 0.903587, mean_q: 1.149081
 145513/1000000: episode: 620, duration: 1.752s, episode steps: 235, steps per second: 134, episode reward: -6.303, mean reward: -0.027 [-11.214, 0.451], mean action: -0.455 [-1.045, 1.131], mean observation: 0.172 [-7.906, 6.017], loss: 0.757364, mean_absolute_error: 0.904360, mean_q: 1.233151
 145702/1000000: episode: 621, duration: 1.408s, episode steps: 189, steps per second: 134, episode reward: 31.476, mean reward: 0.167 [-10.496, 0.364], mean action: -0.491 [-1.099, 1.080], mean observation: 0.231 [-6.019, 5.971], loss: 0.767818, mean_absolute_error: 0.911415, mean_q: 1.305928
 145969/1000000: episode: 622, duration: 1.986s, episode steps: 267, steps per second: 134, episode reward: -7.129, mean reward: -0.027 [-10.931, 0.581], mean action: -0.497 [-1.111, 1.055], mean observation: 0.105 [-7.929, 6.010], loss: 0.751875, mean_absolute_error: 0.902054, mean_q: 1.151013
 146202/1000000: episode: 623, duration: 1.731s, episode steps: 233, steps per second: 135, episode reward: -33.308, mean reward: -0.143 [-11.196, 0.255], mean action: -0.495 [-1.166, 1.087], mean observation: 0.153 [-7.886, 5.975], loss: 0.757746, mean_absolute_error: 0.910166, mean_q: 1.212790
 146457/1000000: episode: 624, duration: 1.893s, episode steps: 255, steps per second: 135, episode reward: 10.623, mean reward: 0.042 [-10.705, 0.434], mean action: -0.505 [-1.110, 1.098], mean observation: 0.116 [-7.932, 6.014], loss: 0.747310, mean_absolute_error: 0.903879, mean_q: 1.150407
 146684/1000000: episode: 625, duration: 1.684s, episode steps: 227, steps per second: 135, episode reward: -2.887, mean reward: -0.013 [-10.941, 0.319], mean action: -0.477 [-1.073, 1.132], mean observation: 0.176 [-7.985, 6.034], loss: 0.736189, mean_absolute_error: 0.893268, mean_q: 1.372201
 146842/1000000: episode: 626, duration: 1.179s, episode steps: 158, steps per second: 134, episode reward: 6.275, mean reward: 0.040 [-10.336, 0.191], mean action: -0.526 [-1.193, 1.082], mean observation: 0.249 [-3.881, 6.081], loss: 0.738752, mean_absolute_error: 0.896561, mean_q: 1.207092
 147054/1000000: episode: 627, duration: 1.583s, episode steps: 212, steps per second: 134, episode reward: 11.430, mean reward: 0.054 [-11.019, 0.405], mean action: -0.507 [-1.118, 1.106], mean observation: 0.212 [-7.918, 6.042], loss: 0.750352, mean_absolute_error: 0.896597, mean_q: 1.344279
 147248/1000000: episode: 628, duration: 1.567s, episode steps: 194, steps per second: 124, episode reward: 4.953, mean reward: 0.026 [-10.596, 0.211], mean action: -0.379 [-1.188, 1.072], mean observation: 0.208 [-6.724, 6.002], loss: 0.758557, mean_absolute_error: 0.902809, mean_q: 1.277276
 147502/1000000: episode: 629, duration: 1.911s, episode steps: 254, steps per second: 133, episode reward: 7.913, mean reward: 0.031 [-11.398, 0.654], mean action: -0.270 [-1.100, 1.111], mean observation: 0.050 [-7.663, 5.552], loss: 0.721676, mean_absolute_error: 0.886830, mean_q: 1.348629
 147733/1000000: episode: 630, duration: 1.733s, episode steps: 231, steps per second: 133, episode reward: -71.706, mean reward: -0.310 [-12.025, 0.344], mean action: -0.008 [-1.147, 1.088], mean observation: -0.093 [-7.060, 4.279], loss: 0.745205, mean_absolute_error: 0.895698, mean_q: 1.418097
 147942/1000000: episode: 631, duration: 1.560s, episode steps: 209, steps per second: 134, episode reward: -46.223, mean reward: -0.221 [-11.874, 0.340], mean action: -0.018 [-1.111, 1.085], mean observation: -0.028 [-7.076, 3.987], loss: 0.738900, mean_absolute_error: 0.895875, mean_q: 1.384722
 148147/1000000: episode: 632, duration: 1.528s, episode steps: 205, steps per second: 134, episode reward: -28.776, mean reward: -0.140 [-11.708, 0.358], mean action: -0.035 [-1.081, 1.097], mean observation: 0.004 [-7.035, 4.305], loss: 0.754764, mean_absolute_error: 0.903742, mean_q: 1.472969
 148284/1000000: episode: 633, duration: 1.025s, episode steps: 137, steps per second: 134, episode reward: 10.701, mean reward: 0.078 [-10.066, 0.185], mean action: -0.129 [-1.151, 1.022], mean observation: 0.140 [-4.300, 4.458], loss: 0.707914, mean_absolute_error: 0.890198, mean_q: 1.483978
 148475/1000000: episode: 634, duration: 1.426s, episode steps: 191, steps per second: 134, episode reward: 12.877, mean reward: 0.067 [-10.821, 0.324], mean action: -0.248 [-1.089, 1.107], mean observation: 0.100 [-6.875, 5.326], loss: 0.749332, mean_absolute_error: 0.897347, mean_q: 1.373654
 148720/1000000: episode: 635, duration: 1.882s, episode steps: 245, steps per second: 130, episode reward: -32.658, mean reward: -0.133 [-11.957, 0.530], mean action: -0.085 [-1.062, 1.108], mean observation: -0.020 [-7.285, 4.907], loss: 0.762715, mean_absolute_error: 0.894107, mean_q: 1.470093
 148933/1000000: episode: 636, duration: 1.576s, episode steps: 213, steps per second: 135, episode reward: -10.873, mean reward: -0.051 [-11.526, 0.428], mean action: -0.234 [-1.164, 1.072], mean observation: 0.054 [-7.320, 4.960], loss: 0.722273, mean_absolute_error: 0.883148, mean_q: 1.467667
 149166/1000000: episode: 637, duration: 1.795s, episode steps: 233, steps per second: 130, episode reward: 9.969, mean reward: 0.043 [-11.065, 0.480], mean action: -0.482 [-1.088, 1.137], mean observation: 0.169 [-8.019, 6.058], loss: 0.766490, mean_absolute_error: 0.896457, mean_q: 1.500797
 149433/1000000: episode: 638, duration: 2.003s, episode steps: 267, steps per second: 133, episode reward: 11.988, mean reward: 0.045 [-10.648, 0.487], mean action: -0.514 [-1.169, 1.122], mean observation: 0.110 [-7.943, 5.968], loss: 0.755769, mean_absolute_error: 0.893583, mean_q: 1.447164
 149627/1000000: episode: 639, duration: 1.450s, episode steps: 194, steps per second: 134, episode reward: 29.727, mean reward: 0.153 [-10.572, 0.367], mean action: -0.508 [-1.100, 1.063], mean observation: 0.221 [-6.588, 5.965], loss: 0.752322, mean_absolute_error: 0.894191, mean_q: 1.377025
 149868/1000000: episode: 640, duration: 1.793s, episode steps: 241, steps per second: 134, episode reward: -8.132, mean reward: -0.034 [-11.134, 0.444], mean action: -0.502 [-1.106, 1.074], mean observation: 0.146 [-7.967, 6.017], loss: 0.749792, mean_absolute_error: 0.890024, mean_q: 1.474997
 150116/1000000: episode: 641, duration: 1.841s, episode steps: 248, steps per second: 135, episode reward: -21.501, mean reward: -0.087 [-10.886, 0.318], mean action: -0.496 [-1.144, 1.098], mean observation: 0.107 [-7.969, 6.048], loss: 0.757222, mean_absolute_error: 0.894740, mean_q: 1.636852
 150264/1000000: episode: 642, duration: 1.101s, episode steps: 148, steps per second: 134, episode reward: 2.566, mean reward: 0.017 [-10.135, 0.122], mean action: -0.507 [-1.131, 1.048], mean observation: 0.246 [-2.738, 5.829], loss: 0.773813, mean_absolute_error: 0.897366, mean_q: 1.430814
 150536/1000000: episode: 643, duration: 2.009s, episode steps: 272, steps per second: 135, episode reward: -27.269, mean reward: -0.100 [-10.682, 0.390], mean action: -0.501 [-1.086, 1.088], mean observation: 0.105 [-7.903, 6.010], loss: 0.712841, mean_absolute_error: 0.874998, mean_q: 1.519122
 150766/1000000: episode: 644, duration: 1.717s, episode steps: 230, steps per second: 134, episode reward: 26.736, mean reward: 0.116 [-10.976, 0.499], mean action: -0.532 [-1.128, 1.057], mean observation: 0.172 [-7.933, 6.007], loss: 0.742495, mean_absolute_error: 0.892177, mean_q: 1.606278
 151029/1000000: episode: 645, duration: 1.953s, episode steps: 263, steps per second: 135, episode reward: 15.798, mean reward: 0.060 [-10.834, 0.597], mean action: -0.507 [-1.116, 1.060], mean observation: 0.120 [-7.921, 5.981], loss: 0.723775, mean_absolute_error: 0.881970, mean_q: 1.598823
 151183/1000000: episode: 646, duration: 1.168s, episode steps: 154, steps per second: 132, episode reward: -2.142, mean reward: -0.014 [-10.208, 0.094], mean action: -0.529 [-1.119, 1.101], mean observation: 0.203 [-3.331, 5.998], loss: 0.753848, mean_absolute_error: 0.886351, mean_q: 1.561128
 151343/1000000: episode: 647, duration: 1.209s, episode steps: 160, steps per second: 132, episode reward: 1.431, mean reward: 0.009 [-10.216, 0.119], mean action: -0.502 [-1.127, 1.069], mean observation: 0.207 [-3.738, 6.009], loss: 0.700012, mean_absolute_error: 0.870451, mean_q: 1.726263
 151608/1000000: episode: 648, duration: 1.976s, episode steps: 265, steps per second: 134, episode reward: -19.152, mean reward: -0.072 [-10.900, 0.483], mean action: -0.498 [-1.110, 1.104], mean observation: 0.127 [-7.969, 6.010], loss: 0.729519, mean_absolute_error: 0.875709, mean_q: 1.593378
 151852/1000000: episode: 649, duration: 1.814s, episode steps: 244, steps per second: 134, episode reward: -21.999, mean reward: -0.090 [-11.035, 0.321], mean action: -0.492 [-1.095, 1.140], mean observation: 0.158 [-7.966, 5.992], loss: 0.742106, mean_absolute_error: 0.887289, mean_q: 1.582987
 152060/1000000: episode: 650, duration: 1.549s, episode steps: 208, steps per second: 134, episode reward: -2.857, mean reward: -0.014 [-11.020, 0.299], mean action: -0.489 [-1.115, 1.060], mean observation: 0.234 [-7.323, 5.937], loss: 0.732122, mean_absolute_error: 0.875440, mean_q: 1.708086
 152293/1000000: episode: 651, duration: 1.728s, episode steps: 233, steps per second: 135, episode reward: -7.816, mean reward: -0.034 [-11.080, 0.329], mean action: -0.317 [-1.133, 1.077], mean observation: 0.130 [-7.860, 5.954], loss: 0.728587, mean_absolute_error: 0.877519, mean_q: 1.565674
 152521/1000000: episode: 652, duration: 1.701s, episode steps: 228, steps per second: 134, episode reward: -136.200, mean reward: -0.597 [-12.455, 0.121], mean action: -0.055 [-1.112, 1.124], mean observation: -0.049 [-7.054, 4.382], loss: 0.741114, mean_absolute_error: 0.880187, mean_q: 1.561564
 152741/1000000: episode: 653, duration: 1.718s, episode steps: 220, steps per second: 128, episode reward: 0.339, mean reward: 0.002 [-11.398, 0.479], mean action: -0.407 [-1.175, 1.039], mean observation: 0.037 [-7.301, 4.941], loss: 0.706900, mean_absolute_error: 0.865437, mean_q: 1.653373
 152964/1000000: episode: 654, duration: 1.707s, episode steps: 223, steps per second: 131, episode reward: -38.181, mean reward: -0.171 [-11.429, 0.265], mean action: -0.412 [-1.088, 1.116], mean observation: 0.106 [-7.553, 5.321], loss: 0.734309, mean_absolute_error: 0.873226, mean_q: 1.677081
 153119/1000000: episode: 655, duration: 1.340s, episode steps: 155, steps per second: 116, episode reward: 24.105, mean reward: 0.156 [-10.094, 0.276], mean action: -0.500 [-1.092, 1.104], mean observation: 0.220 [-3.374, 6.013], loss: 0.728548, mean_absolute_error: 0.875641, mean_q: 1.622652
 153358/1000000: episode: 656, duration: 2.002s, episode steps: 239, steps per second: 119, episode reward: -52.445, mean reward: -0.219 [-10.974, 0.123], mean action: -0.484 [-1.081, 1.139], mean observation: 0.113 [-8.003, 6.030], loss: 0.726033, mean_absolute_error: 0.873800, mean_q: 1.666267
 153544/1000000: episode: 657, duration: 1.456s, episode steps: 186, steps per second: 128, episode reward: 19.870, mean reward: 0.107 [-10.509, 0.295], mean action: -0.487 [-1.066, 1.072], mean observation: 0.216 [-5.787, 5.964], loss: 0.784783, mean_absolute_error: 0.891434, mean_q: 1.671502
 153787/1000000: episode: 658, duration: 1.905s, episode steps: 243, steps per second: 128, episode reward: -32.928, mean reward: -0.136 [-11.073, 0.280], mean action: -0.503 [-1.132, 1.095], mean observation: 0.121 [-7.887, 6.002], loss: 0.725040, mean_absolute_error: 0.871756, mean_q: 1.498370
 154023/1000000: episode: 659, duration: 1.933s, episode steps: 236, steps per second: 122, episode reward: -74.773, mean reward: -0.317 [-11.466, 0.158], mean action: -0.470 [-1.105, 1.119], mean observation: 0.162 [-7.914, 6.011], loss: 0.730697, mean_absolute_error: 0.863019, mean_q: 1.652801
 154225/1000000: episode: 660, duration: 1.553s, episode steps: 202, steps per second: 130, episode reward: 25.646, mean reward: 0.127 [-10.739, 0.388], mean action: -0.499 [-1.128, 1.102], mean observation: 0.205 [-7.488, 6.046], loss: 0.742578, mean_absolute_error: 0.875291, mean_q: 1.648093
 154502/1000000: episode: 661, duration: 2.166s, episode steps: 277, steps per second: 128, episode reward: -48.683, mean reward: -0.176 [-10.420, 0.215], mean action: -0.504 [-1.096, 1.080], mean observation: 0.053 [-7.955, 6.026], loss: 0.726293, mean_absolute_error: 0.864362, mean_q: 1.744981
 154754/1000000: episode: 662, duration: 1.879s, episode steps: 252, steps per second: 134, episode reward: -17.659, mean reward: -0.070 [-11.080, 0.468], mean action: -0.491 [-1.118, 1.103], mean observation: 0.144 [-7.994, 6.044], loss: 0.678552, mean_absolute_error: 0.859543, mean_q: 1.648524
 154986/1000000: episode: 663, duration: 1.736s, episode steps: 232, steps per second: 134, episode reward: 7.317, mean reward: 0.032 [-10.903, 0.365], mean action: -0.509 [-1.118, 1.041], mean observation: 0.157 [-7.931, 5.979], loss: 0.701623, mean_absolute_error: 0.854966, mean_q: 1.716838
 155178/1000000: episode: 664, duration: 1.431s, episode steps: 192, steps per second: 134, episode reward: -37.069, mean reward: -0.193 [-10.981, 0.050], mean action: -0.498 [-1.074, 1.013], mean observation: 0.264 [-6.216, 5.936], loss: 0.714483, mean_absolute_error: 0.861739, mean_q: 1.656700
 155304/1000000: episode: 665, duration: 0.956s, episode steps: 126, steps per second: 132, episode reward: -3.918, mean reward: -0.031 [-10.076, 0.072], mean action: -0.502 [-1.075, 1.055], mean observation: 0.244 [-1.555, 4.969], loss: 0.736996, mean_absolute_error: 0.865819, mean_q: 1.950101
 155539/1000000: episode: 666, duration: 1.788s, episode steps: 235, steps per second: 131, episode reward: -51.771, mean reward: -0.220 [-11.387, 0.261], mean action: -0.499 [-1.121, 1.094], mean observation: 0.209 [-7.903, 6.004], loss: 0.714282, mean_absolute_error: 0.862033, mean_q: 1.730242
 155810/1000000: episode: 667, duration: 2.081s, episode steps: 271, steps per second: 130, episode reward: -59.777, mean reward: -0.221 [-10.564, 0.167], mean action: -0.529 [-1.160, 1.067], mean observation: 0.062 [-7.937, 6.033], loss: 0.728331, mean_absolute_error: 0.860022, mean_q: 1.727890
 156077/1000000: episode: 668, duration: 1.997s, episode steps: 267, steps per second: 134, episode reward: -12.545, mean reward: -0.047 [-10.781, 0.489], mean action: -0.499 [-1.093, 1.074], mean observation: 0.116 [-7.931, 6.029], loss: 0.683684, mean_absolute_error: 0.846979, mean_q: 1.800245
 156316/1000000: episode: 669, duration: 1.818s, episode steps: 239, steps per second: 131, episode reward: -27.925, mean reward: -0.117 [-11.205, 0.330], mean action: -0.505 [-1.105, 1.045], mean observation: 0.183 [-7.866, 5.981], loss: 0.736714, mean_absolute_error: 0.863898, mean_q: 1.735472
 156487/1000000: episode: 670, duration: 1.308s, episode steps: 171, steps per second: 131, episode reward: 24.143, mean reward: 0.141 [-10.316, 0.299], mean action: -0.521 [-1.131, 1.103], mean observation: 0.244 [-4.654, 6.022], loss: 0.724695, mean_absolute_error: 0.862091, mean_q: 1.736598
 156740/1000000: episode: 671, duration: 1.980s, episode steps: 253, steps per second: 128, episode reward: -8.174, mean reward: -0.032 [-10.822, 0.376], mean action: -0.477 [-1.092, 1.107], mean observation: 0.130 [-7.913, 6.010], loss: 0.682247, mean_absolute_error: 0.842994, mean_q: 1.738523
 156970/1000000: episode: 672, duration: 1.929s, episode steps: 230, steps per second: 119, episode reward: -68.208, mean reward: -0.297 [-11.474, 0.140], mean action: -0.516 [-1.167, 1.102], mean observation: 0.189 [-7.860, 5.972], loss: 0.727494, mean_absolute_error: 0.854321, mean_q: 1.893901
 157232/1000000: episode: 673, duration: 2.049s, episode steps: 262, steps per second: 128, episode reward: -72.739, mean reward: -0.278 [-11.106, 0.313], mean action: -0.480 [-1.109, 1.102], mean observation: 0.106 [-7.987, 6.071], loss: 0.708045, mean_absolute_error: 0.847430, mean_q: 1.774479
 157471/1000000: episode: 674, duration: 1.834s, episode steps: 239, steps per second: 130, episode reward: -54.147, mean reward: -0.227 [-10.977, 0.101], mean action: -0.489 [-1.118, 1.096], mean observation: 0.122 [-7.935, 6.011], loss: 0.680745, mean_absolute_error: 0.837377, mean_q: 1.800107
 157735/1000000: episode: 675, duration: 2.318s, episode steps: 264, steps per second: 114, episode reward: -55.839, mean reward: -0.212 [-11.022, 0.317], mean action: -0.509 [-1.130, 1.065], mean observation: 0.097 [-7.901, 5.996], loss: 0.697351, mean_absolute_error: 0.847761, mean_q: 1.985969
 157899/1000000: episode: 676, duration: 1.414s, episode steps: 164, steps per second: 116, episode reward: 12.606, mean reward: 0.077 [-10.342, 0.233], mean action: -0.500 [-1.119, 1.076], mean observation: 0.256 [-4.189, 6.047], loss: 0.691174, mean_absolute_error: 0.841430, mean_q: 1.866778
 158142/1000000: episode: 677, duration: 1.975s, episode steps: 243, steps per second: 123, episode reward: -35.798, mean reward: -0.147 [-11.307, 0.375], mean action: -0.498 [-1.152, 1.098], mean observation: 0.163 [-7.900, 5.985], loss: 0.718021, mean_absolute_error: 0.844545, mean_q: 1.978625
 158411/1000000: episode: 678, duration: 2.077s, episode steps: 269, steps per second: 130, episode reward: 14.395, mean reward: 0.054 [-10.767, 0.637], mean action: -0.519 [-1.140, 1.079], mean observation: 0.110 [-7.910, 6.011], loss: 0.699412, mean_absolute_error: 0.842948, mean_q: 1.925768
 158582/1000000: episode: 679, duration: 1.278s, episode steps: 171, steps per second: 134, episode reward: 3.499, mean reward: 0.020 [-10.348, 0.155], mean action: -0.510 [-1.084, 1.059], mean observation: 0.233 [-4.758, 6.014], loss: 0.705928, mean_absolute_error: 0.848835, mean_q: 2.038099
 158835/1000000: episode: 680, duration: 1.948s, episode steps: 253, steps per second: 130, episode reward: -27.206, mean reward: -0.108 [-11.087, 0.377], mean action: -0.516 [-1.108, 1.037], mean observation: 0.122 [-7.902, 5.958], loss: 0.705250, mean_absolute_error: 0.850554, mean_q: 1.899815
 159080/1000000: episode: 681, duration: 1.849s, episode steps: 245, steps per second: 132, episode reward: -34.469, mean reward: -0.141 [-11.181, 0.350], mean action: -0.518 [-1.153, 1.109], mean observation: 0.137 [-7.943, 6.014], loss: 0.695850, mean_absolute_error: 0.844915, mean_q: 1.814113
 159306/1000000: episode: 682, duration: 1.691s, episode steps: 226, steps per second: 134, episode reward: -43.339, mean reward: -0.192 [-11.009, 0.101], mean action: -0.517 [-1.088, 1.092], mean observation: 0.186 [-7.952, 5.995], loss: 0.673550, mean_absolute_error: 0.840741, mean_q: 2.100308
 159554/1000000: episode: 683, duration: 1.869s, episode steps: 248, steps per second: 133, episode reward: -54.162, mean reward: -0.218 [-11.250, 0.344], mean action: -0.491 [-1.111, 1.153], mean observation: 0.147 [-8.035, 6.090], loss: 0.703379, mean_absolute_error: 0.841948, mean_q: 2.038274
 159792/1000000: episode: 684, duration: 1.807s, episode steps: 238, steps per second: 132, episode reward: -37.177, mean reward: -0.156 [-10.932, 0.179], mean action: -0.483 [-1.137, 1.125], mean observation: 0.116 [-7.941, 6.019], loss: 0.711701, mean_absolute_error: 0.846765, mean_q: 1.988328
 159948/1000000: episode: 685, duration: 1.215s, episode steps: 156, steps per second: 128, episode reward: -0.791, mean reward: -0.005 [-10.512, 0.181], mean action: -0.301 [-1.103, 1.100], mean observation: 0.081 [-4.673, 4.686], loss: 0.703922, mean_absolute_error: 0.851776, mean_q: 1.959015
 160089/1000000: episode: 686, duration: 1.057s, episode steps: 141, steps per second: 133, episode reward: 24.398, mean reward: 0.173 [-10.010, 0.290], mean action: -0.300 [-1.051, 1.037], mean observation: 0.130 [-3.600, 4.804], loss: 0.688110, mean_absolute_error: 0.843708, mean_q: 2.073192
 160354/1000000: episode: 687, duration: 1.980s, episode steps: 265, steps per second: 134, episode reward: -153.160, mean reward: -0.578 [-12.008, 0.158], mean action: -0.387 [-1.128, 1.042], mean observation: -0.087 [-6.976, 4.498], loss: 0.712838, mean_absolute_error: 0.845627, mean_q: 2.125163
 160618/1000000: episode: 688, duration: 1.968s, episode steps: 264, steps per second: 134, episode reward: -100.372, mean reward: -0.380 [-11.656, 0.282], mean action: -0.387 [-1.096, 1.061], mean observation: 0.000 [-7.220, 4.797], loss: 0.716156, mean_absolute_error: 0.843922, mean_q: 2.005337
 160839/1000000: episode: 689, duration: 1.666s, episode steps: 221, steps per second: 133, episode reward: -39.778, mean reward: -0.180 [-11.450, 0.271], mean action: -0.392 [-1.090, 1.075], mean observation: 0.080 [-7.458, 5.155], loss: 0.702442, mean_absolute_error: 0.843216, mean_q: 2.022704
 161043/1000000: episode: 690, duration: 1.542s, episode steps: 204, steps per second: 132, episode reward: -27.236, mean reward: -0.134 [-11.224, 0.209], mean action: -0.373 [-1.104, 1.057], mean observation: 0.083 [-7.117, 4.845], loss: 0.716756, mean_absolute_error: 0.840420, mean_q: 2.136765
 161298/1000000: episode: 691, duration: 1.965s, episode steps: 255, steps per second: 130, episode reward: -103.571, mean reward: -0.406 [-11.920, 0.353], mean action: -0.418 [-1.127, 1.054], mean observation: -0.011 [-7.241, 4.848], loss: 0.692800, mean_absolute_error: 0.833217, mean_q: 2.127313
 161471/1000000: episode: 692, duration: 1.305s, episode steps: 173, steps per second: 133, episode reward: 31.166, mean reward: 0.180 [-10.278, 0.337], mean action: -0.413 [-1.088, 1.033], mean observation: 0.168 [-4.693, 5.614], loss: 0.706243, mean_absolute_error: 0.826794, mean_q: 2.202529
 161736/1000000: episode: 693, duration: 2.047s, episode steps: 265, steps per second: 129, episode reward: -23.144, mean reward: -0.087 [-10.883, 0.458], mean action: -0.495 [-1.114, 1.102], mean observation: 0.099 [-7.937, 6.011], loss: 0.687868, mean_absolute_error: 0.832313, mean_q: 2.187288
 161994/1000000: episode: 694, duration: 1.940s, episode steps: 258, steps per second: 133, episode reward: -3.424, mean reward: -0.013 [-10.921, 0.483], mean action: -0.528 [-1.133, 1.056], mean observation: 0.113 [-7.903, 5.970], loss: 0.672219, mean_absolute_error: 0.829101, mean_q: 2.180956
 162247/1000000: episode: 695, duration: 1.948s, episode steps: 253, steps per second: 130, episode reward: -52.962, mean reward: -0.209 [-10.875, 0.161], mean action: -0.504 [-1.181, 1.086], mean observation: 0.135 [-7.936, 5.997], loss: 0.710131, mean_absolute_error: 0.841359, mean_q: 2.137458
 162503/1000000: episode: 696, duration: 1.930s, episode steps: 256, steps per second: 133, episode reward: 28.352, mean reward: 0.111 [-10.757, 0.560], mean action: -0.504 [-1.088, 1.081], mean observation: 0.111 [-7.924, 6.021], loss: 0.707887, mean_absolute_error: 0.837168, mean_q: 2.149279
 162721/1000000: episode: 697, duration: 1.656s, episode steps: 218, steps per second: 132, episode reward: -14.519, mean reward: -0.067 [-11.180, 0.310], mean action: -0.440 [-1.103, 1.094], mean observation: 0.069 [-7.645, 5.457], loss: 0.703790, mean_absolute_error: 0.843019, mean_q: 2.345862
 162954/1000000: episode: 698, duration: 1.872s, episode steps: 233, steps per second: 124, episode reward: -75.331, mean reward: -0.323 [-12.068, 0.367], mean action: -0.251 [-1.123, 1.128], mean observation: -0.012 [-7.201, 4.669], loss: 0.748448, mean_absolute_error: 0.852083, mean_q: 2.261939
 163184/1000000: episode: 699, duration: 1.740s, episode steps: 230, steps per second: 132, episode reward: -20.422, mean reward: -0.089 [-11.259, 0.347], mean action: -0.395 [-1.108, 1.099], mean observation: 0.028 [-7.368, 4.976], loss: 0.723544, mean_absolute_error: 0.853185, mean_q: 2.237438
 163393/1000000: episode: 700, duration: 1.564s, episode steps: 209, steps per second: 134, episode reward: -22.631, mean reward: -0.108 [-11.082, 0.205], mean action: -0.393 [-1.109, 1.103], mean observation: 0.046 [-7.349, 5.073], loss: 0.735532, mean_absolute_error: 0.860874, mean_q: 2.403060
 163545/1000000: episode: 701, duration: 1.203s, episode steps: 152, steps per second: 126, episode reward: 14.855, mean reward: 0.098 [-10.093, 0.206], mean action: -0.490 [-1.124, 1.032], mean observation: 0.213 [-3.220, 5.899], loss: 0.730126, mean_absolute_error: 0.852779, mean_q: 2.384567
 163776/1000000: episode: 702, duration: 1.773s, episode steps: 231, steps per second: 130, episode reward: -30.660, mean reward: -0.133 [-10.939, 0.172], mean action: -0.525 [-1.123, 1.056], mean observation: 0.136 [-7.938, 6.025], loss: 0.714778, mean_absolute_error: 0.847617, mean_q: 2.389648
 164024/1000000: episode: 703, duration: 1.885s, episode steps: 248, steps per second: 132, episode reward: -29.354, mean reward: -0.118 [-11.580, 0.511], mean action: -0.421 [-1.097, 1.081], mean observation: -0.014 [-7.231, 4.846], loss: 0.706400, mean_absolute_error: 0.839271, mean_q: 2.422076
 164277/1000000: episode: 704, duration: 1.900s, episode steps: 253, steps per second: 133, episode reward: -134.472, mean reward: -0.532 [-12.242, 0.393], mean action: -0.026 [-1.143, 1.078], mean observation: -0.146 [-7.086, 4.075], loss: 0.731818, mean_absolute_error: 0.845841, mean_q: 2.355803
 164461/1000000: episode: 705, duration: 1.421s, episode steps: 184, steps per second: 130, episode reward: -30.708, mean reward: -0.167 [-11.425, 0.210], mean action: -0.025 [-1.103, 1.055], mean observation: -0.017 [-7.075, 3.928], loss: 0.698836, mean_absolute_error: 0.843629, mean_q: 2.482047
 164622/1000000: episode: 706, duration: 1.246s, episode steps: 161, steps per second: 129, episode reward: -33.440, mean reward: -0.208 [-10.906, 0.007], mean action: -0.029 [-1.106, 1.075], mean observation: 0.017 [-6.138, 3.906], loss: 0.682016, mean_absolute_error: 0.821717, mean_q: 2.381862
 164844/1000000: episode: 707, duration: 1.714s, episode steps: 222, steps per second: 130, episode reward: -32.854, mean reward: -0.148 [-11.905, 0.485], mean action: -0.001 [-1.105, 1.072], mean observation: -0.089 [-7.101, 3.873], loss: 0.726646, mean_absolute_error: 0.850246, mean_q: 2.310102
 165070/1000000: episode: 708, duration: 1.723s, episode steps: 226, steps per second: 131, episode reward: -65.257, mean reward: -0.289 [-11.675, 0.222], mean action: 0.019 [-1.091, 1.106], mean observation: -0.068 [-7.062, 4.027], loss: 0.701668, mean_absolute_error: 0.841416, mean_q: 2.385230
 165266/1000000: episode: 709, duration: 1.503s, episode steps: 196, steps per second: 130, episode reward: 3.402, mean reward: 0.017 [-11.330, 0.441], mean action: -0.246 [-1.100, 1.121], mean observation: -0.013 [-6.870, 4.006], loss: 0.690319, mean_absolute_error: 0.835346, mean_q: 2.468253
 165468/1000000: episode: 710, duration: 1.562s, episode steps: 202, steps per second: 129, episode reward: -28.115, mean reward: -0.139 [-11.558, 0.329], mean action: -0.301 [-1.133, 1.051], mean observation: 0.014 [-6.913, 4.270], loss: 0.665749, mean_absolute_error: 0.821014, mean_q: 2.475688
 165716/1000000: episode: 711, duration: 1.902s, episode steps: 248, steps per second: 130, episode reward: -165.817, mean reward: -0.669 [-12.386, 0.143], mean action: -0.363 [-1.115, 1.051], mean observation: -0.047 [-7.035, 4.433], loss: 0.741302, mean_absolute_error: 0.843017, mean_q: 2.543565
 165973/1000000: episode: 712, duration: 1.949s, episode steps: 257, steps per second: 132, episode reward: -165.804, mean reward: -0.645 [-12.291, 0.174], mean action: -0.363 [-1.157, 1.089], mean observation: -0.035 [-7.098, 4.590], loss: 0.707179, mean_absolute_error: 0.840807, mean_q: 2.522348
 166235/1000000: episode: 713, duration: 1.982s, episode steps: 262, steps per second: 132, episode reward: -58.741, mean reward: -0.224 [-11.477, 0.452], mean action: -0.406 [-1.108, 1.098], mean observation: 0.030 [-7.452, 5.192], loss: 0.671435, mean_absolute_error: 0.832994, mean_q: 2.608047
 166474/1000000: episode: 714, duration: 1.807s, episode steps: 239, steps per second: 132, episode reward: 5.494, mean reward: 0.023 [-11.104, 0.527], mean action: -0.490 [-1.121, 1.093], mean observation: 0.149 [-7.989, 6.084], loss: 0.740818, mean_absolute_error: 0.849648, mean_q: 2.545211
 166664/1000000: episode: 715, duration: 1.437s, episode steps: 190, steps per second: 132, episode reward: -22.152, mean reward: -0.117 [-10.655, 0.051], mean action: -0.516 [-1.117, 1.031], mean observation: 0.192 [-6.182, 5.973], loss: 0.746832, mean_absolute_error: 0.838918, mean_q: 2.468166
 166890/1000000: episode: 716, duration: 1.698s, episode steps: 226, steps per second: 133, episode reward: -31.463, mean reward: -0.139 [-11.286, 0.283], mean action: -0.515 [-1.115, 1.052], mean observation: 0.176 [-7.967, 6.024], loss: 0.736661, mean_absolute_error: 0.842393, mean_q: 2.530321
 167127/1000000: episode: 717, duration: 1.806s, episode steps: 237, steps per second: 131, episode reward: -1.428, mean reward: -0.006 [-11.171, 0.485], mean action: -0.485 [-1.091, 1.072], mean observation: 0.165 [-7.943, 6.037], loss: 0.740249, mean_absolute_error: 0.849340, mean_q: 2.497192
 167317/1000000: episode: 718, duration: 1.481s, episode steps: 190, steps per second: 128, episode reward: 15.886, mean reward: 0.084 [-10.514, 0.265], mean action: -0.498 [-1.095, 1.064], mean observation: 0.206 [-6.247, 5.971], loss: 0.743751, mean_absolute_error: 0.835836, mean_q: 2.598780
 167487/1000000: episode: 719, duration: 1.314s, episode steps: 170, steps per second: 129, episode reward: 26.205, mean reward: 0.154 [-10.453, 0.352], mean action: -0.369 [-1.124, 1.075], mean observation: 0.111 [-5.364, 5.068], loss: 0.756470, mean_absolute_error: 0.842476, mean_q: 2.646622
 167740/1000000: episode: 720, duration: 1.930s, episode steps: 253, steps per second: 131, episode reward: -94.308, mean reward: -0.373 [-11.537, 0.192], mean action: -0.406 [-1.097, 1.078], mean observation: -0.030 [-7.355, 5.038], loss: 0.723860, mean_absolute_error: 0.838915, mean_q: 2.616448
 167976/1000000: episode: 721, duration: 1.804s, episode steps: 236, steps per second: 131, episode reward: -26.990, mean reward: -0.114 [-11.463, 0.448], mean action: -0.429 [-1.113, 1.064], mean observation: 0.034 [-7.557, 5.307], loss: 0.733245, mean_absolute_error: 0.849370, mean_q: 2.563624
 168247/1000000: episode: 722, duration: 2.047s, episode steps: 271, steps per second: 132, episode reward: 8.770, mean reward: 0.032 [-10.836, 0.589], mean action: -0.445 [-1.145, 1.108], mean observation: 0.004 [-7.705, 5.471], loss: 0.724822, mean_absolute_error: 0.838484, mean_q: 2.659898
 168519/1000000: episode: 723, duration: 2.081s, episode steps: 272, steps per second: 131, episode reward: -16.470, mean reward: -0.061 [-10.872, 0.477], mean action: -0.452 [-1.114, 1.101], mean observation: 0.043 [-7.689, 5.608], loss: 0.722767, mean_absolute_error: 0.828387, mean_q: 2.744586
 168768/1000000: episode: 724, duration: 1.913s, episode steps: 249, steps per second: 130, episode reward: -37.863, mean reward: -0.152 [-11.181, 0.331], mean action: -0.431 [-1.136, 1.140], mean observation: -0.002 [-7.568, 5.253], loss: 0.731457, mean_absolute_error: 0.835571, mean_q: 2.688922
 169045/1000000: episode: 725, duration: 2.106s, episode steps: 277, steps per second: 132, episode reward: -64.248, mean reward: -0.232 [-10.508, 0.164], mean action: -0.490 [-1.101, 1.063], mean observation: 0.043 [-7.918, 6.012], loss: 0.712888, mean_absolute_error: 0.833916, mean_q: 2.768013
 169294/1000000: episode: 726, duration: 1.886s, episode steps: 249, steps per second: 132, episode reward: -43.688, mean reward: -0.175 [-11.618, 0.540], mean action: -0.421 [-1.124, 1.078], mean observation: 0.010 [-7.435, 5.092], loss: 0.744734, mean_absolute_error: 0.835234, mean_q: 2.747137
 169525/1000000: episode: 727, duration: 1.759s, episode steps: 231, steps per second: 131, episode reward: -0.527, mean reward: -0.002 [-11.229, 0.440], mean action: -0.378 [-1.087, 1.062], mean observation: 0.030 [-7.404, 5.093], loss: 0.719867, mean_absolute_error: 0.834572, mean_q: 2.712521
 169678/1000000: episode: 728, duration: 1.158s, episode steps: 153, steps per second: 132, episode reward: 9.573, mean reward: 0.063 [-10.096, 0.164], mean action: -0.465 [-1.081, 1.048], mean observation: 0.194 [-3.234, 5.891], loss: 0.764089, mean_absolute_error: 0.844328, mean_q: 2.695562
 169914/1000000: episode: 729, duration: 1.778s, episode steps: 236, steps per second: 133, episode reward: 14.581, mean reward: 0.062 [-11.091, 0.538], mean action: -0.506 [-1.144, 1.107], mean observation: 0.152 [-7.956, 6.033], loss: 0.731633, mean_absolute_error: 0.844485, mean_q: 2.702004
 170178/1000000: episode: 730, duration: 2.010s, episode steps: 264, steps per second: 131, episode reward: 34.052, mean reward: 0.129 [-10.807, 0.659], mean action: -0.484 [-1.101, 1.104], mean observation: 0.117 [-7.832, 5.946], loss: 0.712589, mean_absolute_error: 0.825253, mean_q: 2.734403
 170396/1000000: episode: 731, duration: 1.640s, episode steps: 218, steps per second: 133, episode reward: -22.479, mean reward: -0.103 [-10.876, 0.168], mean action: -0.514 [-1.160, 1.101], mean observation: 0.176 [-7.988, 6.081], loss: 0.709807, mean_absolute_error: 0.827694, mean_q: 2.712441
 170644/1000000: episode: 732, duration: 1.884s, episode steps: 248, steps per second: 132, episode reward: -74.382, mean reward: -0.300 [-11.362, 0.239], mean action: -0.499 [-1.159, 1.079], mean observation: 0.171 [-7.959, 6.029], loss: 0.755040, mean_absolute_error: 0.837923, mean_q: 2.865439
 170899/1000000: episode: 733, duration: 1.916s, episode steps: 255, steps per second: 133, episode reward: -18.795, mean reward: -0.074 [-11.137, 0.502], mean action: -0.503 [-1.105, 1.033], mean observation: 0.128 [-7.885, 6.003], loss: 0.702148, mean_absolute_error: 0.835011, mean_q: 2.733902
 171083/1000000: episode: 734, duration: 1.387s, episode steps: 184, steps per second: 133, episode reward: 5.716, mean reward: 0.031 [-10.634, 0.238], mean action: -0.487 [-1.061, 1.042], mean observation: 0.231 [-5.820, 6.001], loss: 0.674261, mean_absolute_error: 0.816088, mean_q: 2.931793
 171326/1000000: episode: 735, duration: 1.843s, episode steps: 243, steps per second: 132, episode reward: -0.610, mean reward: -0.003 [-11.114, 0.519], mean action: -0.490 [-1.135, 1.134], mean observation: 0.143 [-7.997, 6.040], loss: 0.733949, mean_absolute_error: 0.836556, mean_q: 2.747529
 171569/1000000: episode: 736, duration: 1.854s, episode steps: 243, steps per second: 131, episode reward: -23.454, mean reward: -0.097 [-11.049, 0.333], mean action: -0.500 [-1.096, 1.072], mean observation: 0.160 [-7.946, 6.024], loss: 0.744877, mean_absolute_error: 0.833041, mean_q: 2.805344
 171790/1000000: episode: 737, duration: 1.672s, episode steps: 221, steps per second: 132, episode reward: -35.682, mean reward: -0.161 [-11.307, 0.246], mean action: -0.494 [-1.070, 1.186], mean observation: 0.192 [-7.997, 6.068], loss: 0.752351, mean_absolute_error: 0.841106, mean_q: 2.769254
 172021/1000000: episode: 738, duration: 1.828s, episode steps: 231, steps per second: 126, episode reward: -6.895, mean reward: -0.030 [-10.913, 0.308], mean action: -0.497 [-1.114, 1.107], mean observation: 0.164 [-7.956, 6.049], loss: 0.725969, mean_absolute_error: 0.834982, mean_q: 2.815586
 172249/1000000: episode: 739, duration: 1.744s, episode steps: 228, steps per second: 131, episode reward: -23.322, mean reward: -0.102 [-11.173, 0.285], mean action: -0.484 [-1.074, 1.070], mean observation: 0.200 [-7.939, 6.005], loss: 0.754106, mean_absolute_error: 0.841224, mean_q: 2.942454
 172421/1000000: episode: 740, duration: 1.378s, episode steps: 172, steps per second: 125, episode reward: 23.257, mean reward: 0.135 [-10.370, 0.306], mean action: -0.526 [-1.148, 1.034], mean observation: 0.244 [-4.831, 6.033], loss: 0.759244, mean_absolute_error: 0.844044, mean_q: 2.997971
 172621/1000000: episode: 741, duration: 1.542s, episode steps: 200, steps per second: 130, episode reward: -26.548, mean reward: -0.133 [-11.069, 0.157], mean action: -0.517 [-1.103, 1.014], mean observation: 0.238 [-6.922, 5.949], loss: 0.735638, mean_absolute_error: 0.836701, mean_q: 2.923848
 172873/1000000: episode: 742, duration: 1.895s, episode steps: 252, steps per second: 133, episode reward: -48.334, mean reward: -0.192 [-11.231, 0.384], mean action: -0.499 [-1.084, 1.095], mean observation: 0.128 [-7.931, 6.049], loss: 0.770480, mean_absolute_error: 0.843169, mean_q: 2.832393
 173123/1000000: episode: 743, duration: 1.911s, episode steps: 250, steps per second: 131, episode reward: -12.832, mean reward: -0.051 [-11.162, 0.476], mean action: -0.496 [-1.109, 1.050], mean observation: 0.139 [-7.904, 5.980], loss: 0.699837, mean_absolute_error: 0.827922, mean_q: 2.918533
 173387/1000000: episode: 744, duration: 1.994s, episode steps: 264, steps per second: 132, episode reward: -46.230, mean reward: -0.175 [-11.116, 0.433], mean action: -0.519 [-1.126, 1.074], mean observation: 0.131 [-7.932, 5.988], loss: 0.747131, mean_absolute_error: 0.831833, mean_q: 2.867525
 173530/1000000: episode: 745, duration: 1.083s, episode steps: 143, steps per second: 132, episode reward: 1.863, mean reward: 0.013 [-10.114, 0.117], mean action: -0.508 [-1.118, 1.048], mean observation: 0.200 [-2.497, 5.726], loss: 0.694238, mean_absolute_error: 0.825628, mean_q: 2.893611
 173718/1000000: episode: 746, duration: 1.419s, episode steps: 188, steps per second: 132, episode reward: -2.501, mean reward: -0.013 [-10.520, 0.146], mean action: -0.535 [-1.135, 1.006], mean observation: 0.215 [-6.192, 6.004], loss: 0.741769, mean_absolute_error: 0.838084, mean_q: 2.991909
 173926/1000000: episode: 747, duration: 1.561s, episode steps: 208, steps per second: 133, episode reward: 9.066, mean reward: 0.044 [-10.950, 0.355], mean action: -0.486 [-1.125, 1.102], mean observation: 0.223 [-7.702, 6.035], loss: 0.708809, mean_absolute_error: 0.835073, mean_q: 2.815464
 174170/1000000: episode: 748, duration: 1.829s, episode steps: 244, steps per second: 133, episode reward: -21.270, mean reward: -0.087 [-10.857, 0.268], mean action: -0.492 [-1.092, 1.110], mean observation: 0.114 [-7.964, 6.026], loss: 0.719154, mean_absolute_error: 0.832776, mean_q: 3.077331
 174368/1000000: episode: 749, duration: 1.490s, episode steps: 198, steps per second: 133, episode reward: 24.762, mean reward: 0.125 [-10.725, 0.378], mean action: -0.494 [-1.115, 1.131], mean observation: 0.222 [-7.293, 6.078], loss: 0.760415, mean_absolute_error: 0.841038, mean_q: 3.090137
 174593/1000000: episode: 750, duration: 1.714s, episode steps: 225, steps per second: 131, episode reward: -8.848, mean reward: -0.039 [-11.063, 0.312], mean action: -0.502 [-1.113, 1.090], mean observation: 0.172 [-7.874, 5.978], loss: 0.720726, mean_absolute_error: 0.824149, mean_q: 2.984925
 174833/1000000: episode: 751, duration: 1.837s, episode steps: 240, steps per second: 131, episode reward: -29.214, mean reward: -0.122 [-11.277, 0.396], mean action: -0.480 [-1.114, 1.100], mean observation: 0.163 [-7.960, 6.040], loss: 0.712655, mean_absolute_error: 0.825036, mean_q: 3.010026
 175014/1000000: episode: 752, duration: 1.424s, episode steps: 181, steps per second: 127, episode reward: -22.200, mean reward: -0.123 [-10.764, 0.081], mean action: -0.492 [-1.141, 1.136], mean observation: 0.227 [-5.697, 6.008], loss: 0.742617, mean_absolute_error: 0.836302, mean_q: 2.803043
 175278/1000000: episode: 753, duration: 2.003s, episode steps: 264, steps per second: 132, episode reward: -70.859, mean reward: -0.268 [-11.083, 0.304], mean action: -0.493 [-1.131, 1.116], mean observation: 0.101 [-7.962, 6.047], loss: 0.711809, mean_absolute_error: 0.829552, mean_q: 3.095182
 175509/1000000: episode: 754, duration: 1.770s, episode steps: 231, steps per second: 131, episode reward: -0.107, mean reward: -0.000 [-11.154, 0.427], mean action: -0.510 [-1.079, 1.029], mean observation: 0.188 [-7.909, 5.971], loss: 0.722164, mean_absolute_error: 0.831877, mean_q: 3.007291
 175728/1000000: episode: 755, duration: 1.696s, episode steps: 219, steps per second: 129, episode reward: 1.710, mean reward: 0.008 [-11.256, 0.370], mean action: -0.143 [-1.087, 1.089], mean observation: 0.222 [-7.851, 5.722], loss: 0.720581, mean_absolute_error: 0.837284, mean_q: 2.981166
 175982/1000000: episode: 756, duration: 1.967s, episode steps: 254, steps per second: 129, episode reward: -121.052, mean reward: -0.477 [-12.167, 0.371], mean action: -0.017 [-1.076, 1.071], mean observation: 0.173 [-7.063, 5.179], loss: 0.734774, mean_absolute_error: 0.836715, mean_q: 3.038928
 176241/1000000: episode: 757, duration: 1.997s, episode steps: 259, steps per second: 130, episode reward: -160.261, mean reward: -0.619 [-12.076, 0.171], mean action: -0.021 [-1.131, 1.086], mean observation: 0.161 [-7.137, 5.004], loss: 0.730108, mean_absolute_error: 0.839077, mean_q: 3.052795
 176454/1000000: episode: 758, duration: 1.628s, episode steps: 213, steps per second: 131, episode reward: -30.025, mean reward: -0.141 [-11.793, 0.417], mean action: -0.019 [-1.169, 1.132], mean observation: 0.217 [-7.150, 5.166], loss: 0.712127, mean_absolute_error: 0.826102, mean_q: 3.007347
 176649/1000000: episode: 759, duration: 1.498s, episode steps: 195, steps per second: 130, episode reward: -24.503, mean reward: -0.126 [-11.477, 0.271], mean action: -0.030 [-1.078, 1.103], mean observation: 0.270 [-6.981, 5.076], loss: 0.724587, mean_absolute_error: 0.837625, mean_q: 3.076251
 176855/1000000: episode: 760, duration: 1.581s, episode steps: 206, steps per second: 130, episode reward: -10.110, mean reward: -0.049 [-11.576, 0.419], mean action: -0.015 [-1.082, 1.115], mean observation: 0.232 [-7.089, 5.037], loss: 0.706422, mean_absolute_error: 0.822622, mean_q: 3.120989
 177093/1000000: episode: 761, duration: 1.786s, episode steps: 238, steps per second: 133, episode reward: -89.163, mean reward: -0.375 [-12.246, 0.398], mean action: -0.010 [-1.085, 1.101], mean observation: 0.201 [-7.085, 5.174], loss: 0.723621, mean_absolute_error: 0.833284, mean_q: 3.044340
 177319/1000000: episode: 762, duration: 1.690s, episode steps: 226, steps per second: 134, episode reward: -24.465, mean reward: -0.108 [-11.709, 0.442], mean action: -0.019 [-1.178, 1.086], mean observation: 0.177 [-7.089, 5.322], loss: 0.710631, mean_absolute_error: 0.823011, mean_q: 3.087868
 177531/1000000: episode: 763, duration: 1.611s, episode steps: 212, steps per second: 132, episode reward: -53.955, mean reward: -0.255 [-11.737, 0.239], mean action: -0.005 [-1.125, 1.146], mean observation: 0.204 [-7.092, 5.176], loss: 0.733617, mean_absolute_error: 0.828422, mean_q: 3.160977
 177689/1000000: episode: 764, duration: 1.194s, episode steps: 158, steps per second: 132, episode reward: 11.900, mean reward: 0.075 [-10.465, 0.247], mean action: -0.003 [-1.126, 1.116], mean observation: 0.255 [-5.690, 5.294], loss: 0.688649, mean_absolute_error: 0.818344, mean_q: 3.031049
 177923/1000000: episode: 765, duration: 1.756s, episode steps: 234, steps per second: 133, episode reward: -91.888, mean reward: -0.393 [-12.109, 0.282], mean action: 0.002 [-1.111, 1.104], mean observation: 0.192 [-7.045, 5.228], loss: 0.746452, mean_absolute_error: 0.838756, mean_q: 3.103977
 178186/1000000: episode: 766, duration: 1.965s, episode steps: 263, steps per second: 134, episode reward: -104.478, mean reward: -0.397 [-11.858, 0.442], mean action: 0.014 [-1.056, 1.165], mean observation: 0.146 [-7.500, 5.242], loss: 0.731334, mean_absolute_error: 0.823731, mean_q: 3.216344
 178414/1000000: episode: 767, duration: 1.713s, episode steps: 228, steps per second: 133, episode reward: -134.786, mean reward: -0.591 [-12.437, 0.117], mean action: 0.019 [-1.105, 1.129], mean observation: 0.232 [-7.093, 5.200], loss: 0.704427, mean_absolute_error: 0.815497, mean_q: 3.180805
 178601/1000000: episode: 768, duration: 1.526s, episode steps: 187, steps per second: 123, episode reward: -13.041, mean reward: -0.070 [-11.236, 0.266], mean action: -0.001 [-1.068, 1.081], mean observation: 0.264 [-6.985, 5.111], loss: 0.771167, mean_absolute_error: 0.835035, mean_q: 3.210743
 178832/1000000: episode: 769, duration: 1.791s, episode steps: 231, steps per second: 129, episode reward: -8.952, mean reward: -0.039 [-11.788, 0.539], mean action: -0.037 [-1.094, 1.111], mean observation: 0.188 [-7.122, 5.503], loss: 0.709416, mean_absolute_error: 0.826302, mean_q: 3.132604
 179024/1000000: episode: 770, duration: 1.468s, episode steps: 192, steps per second: 131, episode reward: -27.011, mean reward: -0.141 [-11.479, 0.252], mean action: -0.015 [-1.135, 1.109], mean observation: 0.257 [-7.083, 5.230], loss: 0.763942, mean_absolute_error: 0.833333, mean_q: 3.134372
 179259/1000000: episode: 771, duration: 1.880s, episode steps: 235, steps per second: 125, episode reward: -21.134, mean reward: -0.090 [-11.463, 0.391], mean action: -0.388 [-1.095, 1.083], mean observation: 0.218 [-7.303, 4.907], loss: 0.741741, mean_absolute_error: 0.828363, mean_q: 3.262252
 179639/1000000: episode: 772, duration: 2.897s, episode steps: 380, steps per second: 131, episode reward: -22.312, mean reward: -0.059 [-11.974, 0.436], mean action: -0.536 [-1.134, 1.126], mean observation: 0.181 [-7.769, 5.382], loss: 0.743374, mean_absolute_error: 0.829279, mean_q: 3.255454
 179845/1000000: episode: 773, duration: 1.637s, episode steps: 206, steps per second: 126, episode reward: -5.326, mean reward: -0.026 [-10.653, 0.150], mean action: -0.473 [-1.054, 1.039], mean observation: 0.152 [-6.575, 5.958], loss: 0.715573, mean_absolute_error: 0.827667, mean_q: 3.101736
 180499/1000000: episode: 774, duration: 4.994s, episode steps: 654, steps per second: 131, episode reward: 172.895, mean reward: 0.264 [-10.630, 0.407], mean action: -0.879 [-1.154, 1.020], mean observation: 0.183 [-1.730, 2.662], loss: 0.725703, mean_absolute_error: 0.827024, mean_q: 3.314271
 181136/1000000: episode: 775, duration: 4.846s, episode steps: 637, steps per second: 131, episode reward: 29.873, mean reward: 0.047 [-11.985, 0.458], mean action: -0.693 [-1.144, 1.107], mean observation: 0.134 [-7.539, 3.715], loss: 0.731039, mean_absolute_error: 0.820815, mean_q: 3.341798
 181702/1000000: episode: 776, duration: 4.421s, episode steps: 566, steps per second: 128, episode reward: 185.384, mean reward: 0.328 [-10.825, 0.515], mean action: -0.120 [-1.108, 1.078], mean observation: 0.070 [-5.271, 4.127], loss: 0.754947, mean_absolute_error: 0.823090, mean_q: 3.456540
 181976/1000000: episode: 777, duration: 2.084s, episode steps: 274, steps per second: 131, episode reward: 30.297, mean reward: 0.111 [-9.925, 0.164], mean action: -0.099 [-1.135, 1.031], mean observation: 0.064 [-1.933, 1.000], loss: 0.733109, mean_absolute_error: 0.813447, mean_q: 3.425064
 182416/1000000: episode: 778, duration: 3.281s, episode steps: 440, steps per second: 134, episode reward: 116.543, mean reward: 0.265 [-11.220, 0.553], mean action: -0.209 [-1.122, 1.092], mean observation: -0.053 [-6.440, 3.247], loss: 0.737876, mean_absolute_error: 0.820296, mean_q: 3.413016
 182658/1000000: episode: 779, duration: 1.813s, episode steps: 242, steps per second: 133, episode reward: -7.243, mean reward: -0.030 [-11.178, 0.342], mean action: -0.298 [-1.110, 1.197], mean observation: -0.163 [-6.247, 1.000], loss: 0.716918, mean_absolute_error: 0.815848, mean_q: 3.439847
 182818/1000000: episode: 780, duration: 1.203s, episode steps: 160, steps per second: 133, episode reward: 15.482, mean reward: 0.097 [-10.080, 0.202], mean action: -0.374 [-1.194, 1.135], mean observation: -0.009 [-5.696, 1.000], loss: 0.744139, mean_absolute_error: 0.832008, mean_q: 3.432767
 182988/1000000: episode: 781, duration: 1.283s, episode steps: 170, steps per second: 132, episode reward: 19.459, mean reward: 0.114 [-10.178, 0.240], mean action: -0.366 [-1.099, 1.077], mean observation: -0.033 [-5.840, 1.000], loss: 0.761068, mean_absolute_error: 0.824906, mean_q: 3.325157
 183267/1000000: episode: 782, duration: 2.098s, episode steps: 279, steps per second: 133, episode reward: -80.923, mean reward: -0.290 [-11.658, 0.168], mean action: -0.247 [-1.115, 1.071], mean observation: -0.136 [-5.891, 1.476], loss: 0.776841, mean_absolute_error: 0.838915, mean_q: 3.405714
 183486/1000000: episode: 783, duration: 1.651s, episode steps: 219, steps per second: 133, episode reward: -18.277, mean reward: -0.083 [-10.863, 0.193], mean action: -0.249 [-1.092, 1.170], mean observation: -0.135 [-5.745, 1.720], loss: 0.766708, mean_absolute_error: 0.830234, mean_q: 3.487184
 183794/1000000: episode: 784, duration: 2.325s, episode steps: 308, steps per second: 132, episode reward: -48.408, mean reward: -0.157 [-11.732, 0.379], mean action: -0.148 [-1.090, 1.119], mean observation: -0.073 [-5.240, 3.680], loss: 0.759288, mean_absolute_error: 0.826843, mean_q: 3.481756
 183968/1000000: episode: 785, duration: 1.318s, episode steps: 174, steps per second: 132, episode reward: -6.807, mean reward: -0.039 [-10.367, 0.095], mean action: -0.372 [-1.087, 1.090], mean observation: -0.066 [-5.914, 1.000], loss: 0.789352, mean_absolute_error: 0.839792, mean_q: 3.394480
 184218/1000000: episode: 786, duration: 1.909s, episode steps: 250, steps per second: 131, episode reward: -77.267, mean reward: -0.309 [-11.601, 0.168], mean action: -0.268 [-1.129, 1.058], mean observation: -0.132 [-5.744, 2.002], loss: 0.702849, mean_absolute_error: 0.814275, mean_q: 3.522365
 184452/1000000: episode: 787, duration: 1.820s, episode steps: 234, steps per second: 129, episode reward: -55.309, mean reward: -0.236 [-11.590, 0.237], mean action: -0.510 [-1.096, 1.098], mean observation: -0.183 [-7.084, 1.000], loss: 0.781105, mean_absolute_error: 0.839093, mean_q: 3.450625
 184685/1000000: episode: 788, duration: 1.772s, episode steps: 233, steps per second: 131, episode reward: -89.036, mean reward: -0.382 [-11.639, 0.082], mean action: -0.545 [-1.112, 1.072], mean observation: -0.211 [-7.068, 1.000], loss: 0.745703, mean_absolute_error: 0.820468, mean_q: 3.438992
 184957/1000000: episode: 789, duration: 2.040s, episode steps: 272, steps per second: 133, episode reward: -14.062, mean reward: -0.052 [-11.397, 0.546], mean action: -0.533 [-1.119, 1.082], mean observation: -0.196 [-7.228, 3.316], loss: 0.734938, mean_absolute_error: 0.819614, mean_q: 3.484686
 185211/1000000: episode: 790, duration: 1.891s, episode steps: 254, steps per second: 134, episode reward: -50.262, mean reward: -0.198 [-11.279, 0.234], mean action: -0.492 [-1.107, 1.097], mean observation: -0.197 [-7.187, 1.680], loss: 0.750422, mean_absolute_error: 0.828147, mean_q: 3.539654
 185444/1000000: episode: 791, duration: 1.743s, episode steps: 233, steps per second: 134, episode reward: -72.679, mean reward: -0.312 [-11.534, 0.121], mean action: -0.423 [-1.126, 1.083], mean observation: -0.150 [-5.992, 1.000], loss: 0.802178, mean_absolute_error: 0.842247, mean_q: 3.525065
 185725/1000000: episode: 792, duration: 2.132s, episode steps: 281, steps per second: 132, episode reward: -48.881, mean reward: -0.174 [-11.921, 0.564], mean action: -0.331 [-1.140, 1.044], mean observation: -0.212 [-6.091, 1.018], loss: 0.772512, mean_absolute_error: 0.834027, mean_q: 3.556000
 185982/1000000: episode: 793, duration: 1.961s, episode steps: 257, steps per second: 131, episode reward: -38.838, mean reward: -0.151 [-11.500, 0.343], mean action: -0.301 [-1.100, 1.057], mean observation: -0.160 [-6.226, 1.000], loss: 0.799834, mean_absolute_error: 0.841517, mean_q: 3.363284
 186167/1000000: episode: 794, duration: 1.388s, episode steps: 185, steps per second: 133, episode reward: -12.366, mean reward: -0.067 [-10.716, 0.151], mean action: -0.428 [-1.135, 1.102], mean observation: -0.110 [-5.925, 1.000], loss: 0.744335, mean_absolute_error: 0.817266, mean_q: 3.538807
 186393/1000000: episode: 795, duration: 1.704s, episode steps: 226, steps per second: 133, episode reward: -41.381, mean reward: -0.183 [-11.267, 0.202], mean action: -0.328 [-1.096, 1.090], mean observation: -0.143 [-5.876, 1.000], loss: 0.756817, mean_absolute_error: 0.822188, mean_q: 3.428611
 186618/1000000: episode: 796, duration: 1.714s, episode steps: 225, steps per second: 131, episode reward: 25.620, mean reward: 0.114 [-10.930, 0.460], mean action: -0.344 [-1.160, 1.061], mean observation: -0.146 [-5.940, 1.000], loss: 0.750449, mean_absolute_error: 0.826482, mean_q: 3.486597
 186741/1000000: episode: 797, duration: 0.924s, episode steps: 123, steps per second: 133, episode reward: 1.652, mean reward: 0.013 [-9.960, 0.102], mean action: -0.496 [-1.073, 1.129], mean observation: 0.061 [-4.910, 1.000], loss: 0.757058, mean_absolute_error: 0.840707, mean_q: 3.493149
 186995/1000000: episode: 798, duration: 1.927s, episode steps: 254, steps per second: 132, episode reward: -39.810, mean reward: -0.157 [-11.390, 0.316], mean action: -0.341 [-1.139, 1.066], mean observation: -0.166 [-5.971, 1.000], loss: 0.728526, mean_absolute_error: 0.821919, mean_q: 3.554035
 187235/1000000: episode: 799, duration: 1.818s, episode steps: 240, steps per second: 132, episode reward: 8.795, mean reward: 0.037 [-10.900, 0.366], mean action: -0.363 [-1.118, 1.052], mean observation: -0.160 [-5.966, 1.000], loss: 0.794532, mean_absolute_error: 0.843361, mean_q: 3.546670
 187380/1000000: episode: 800, duration: 1.087s, episode steps: 145, steps per second: 133, episode reward: 13.011, mean reward: 0.090 [-9.887, 0.168], mean action: -0.830 [-1.098, 0.980], mean observation: 0.024 [-3.625, 1.000], loss: 0.770014, mean_absolute_error: 0.835462, mean_q: 3.597960
 187822/1000000: episode: 801, duration: 3.293s, episode steps: 442, steps per second: 134, episode reward: -10.738, mean reward: -0.024 [-11.169, 0.221], mean action: -0.690 [-1.153, 1.061], mean observation: -0.049 [-5.218, 1.000], loss: 0.761119, mean_absolute_error: 0.830284, mean_q: 3.530097
 188101/1000000: episode: 802, duration: 2.081s, episode steps: 279, steps per second: 134, episode reward: -13.613, mean reward: -0.049 [-11.434, 0.551], mean action: -0.509 [-1.131, 1.090], mean observation: -0.178 [-5.721, 1.000], loss: 0.745898, mean_absolute_error: 0.829733, mean_q: 3.536233
 189053/1000000: episode: 803, duration: 7.198s, episode steps: 952, steps per second: 132, episode reward: 396.039, mean reward: 0.416 [-10.703, 0.623], mean action: -0.991 [-1.134, -0.811], mean observation: 0.220 [-0.609, 2.398], loss: 0.783904, mean_absolute_error: 0.836193, mean_q: 3.563273
 189745/1000000: episode: 804, duration: 5.261s, episode steps: 692, steps per second: 132, episode reward: 22.174, mean reward: 0.032 [-10.364, 0.139], mean action: -1.006 [-1.125, -0.836], mean observation: 0.043 [-1.581, 1.000], loss: 0.758485, mean_absolute_error: 0.832466, mean_q: 3.652589
 190435/1000000: episode: 805, duration: 5.410s, episode steps: 690, steps per second: 128, episode reward: 133.074, mean reward: 0.193 [-10.298, 0.335], mean action: -0.995 [-1.200, -0.839], mean observation: 0.194 [-0.321, 1.706], loss: 0.774855, mean_absolute_error: 0.835579, mean_q: 3.695506
 191145/1000000: episode: 806, duration: 5.326s, episode steps: 710, steps per second: 133, episode reward: 81.153, mean reward: 0.114 [-10.359, 0.231], mean action: -0.993 [-1.129, -0.818], mean observation: 0.048 [-1.622, 1.000], loss: 0.802342, mean_absolute_error: 0.848173, mean_q: 3.736556
 191759/1000000: episode: 807, duration: 4.655s, episode steps: 614, steps per second: 132, episode reward: 130.876, mean reward: 0.213 [-10.292, 0.382], mean action: -0.988 [-1.139, -0.868], mean observation: 0.200 [-0.296, 1.680], loss: 0.761705, mean_absolute_error: 0.831778, mean_q: 3.708439
 192045/1000000: episode: 808, duration: 2.286s, episode steps: 286, steps per second: 125, episode reward: -19.678, mean reward: -0.069 [-11.230, 0.314], mean action: -0.383 [-1.090, 1.069], mean observation: -0.158 [-6.441, 1.000], loss: 0.784075, mean_absolute_error: 0.839782, mean_q: 3.687900
 192690/1000000: episode: 809, duration: 5.213s, episode steps: 645, steps per second: 124, episode reward: 197.671, mean reward: 0.306 [-10.586, 0.504], mean action: -1.006 [-1.141, -0.869], mean observation: 0.052 [-2.881, 1.000], loss: 0.760995, mean_absolute_error: 0.829314, mean_q: 3.701374
 193322/1000000: episode: 810, duration: 4.947s, episode steps: 632, steps per second: 128, episode reward: 10.384, mean reward: 0.016 [-10.128, 0.083], mean action: -1.006 [-1.149, -0.874], mean observation: 0.067 [-0.823, 1.000], loss: 0.822656, mean_absolute_error: 0.848902, mean_q: 3.723314
 193831/1000000: episode: 811, duration: 3.982s, episode steps: 509, steps per second: 128, episode reward: 153.059, mean reward: 0.301 [-10.987, 0.653], mean action: -0.708 [-1.113, 1.153], mean observation: 0.079 [-7.872, 5.672], loss: 0.808271, mean_absolute_error: 0.848092, mean_q: 3.756289
 194443/1000000: episode: 812, duration: 4.753s, episode steps: 612, steps per second: 129, episode reward: 141.842, mean reward: 0.232 [-11.423, 0.526], mean action: -0.818 [-1.136, 1.090], mean observation: 0.081 [-7.436, 4.914], loss: 0.780993, mean_absolute_error: 0.839954, mean_q: 3.758622
 195208/1000000: episode: 813, duration: 6.061s, episode steps: 765, steps per second: 126, episode reward: 152.142, mean reward: 0.199 [-10.930, 0.358], mean action: -0.924 [-1.155, 1.013], mean observation: 0.237 [-2.862, 3.753], loss: 0.772221, mean_absolute_error: 0.840420, mean_q: 3.811426
 196017/1000000: episode: 814, duration: 6.044s, episode steps: 809, steps per second: 134, episode reward: 297.253, mean reward: 0.367 [-11.453, 0.628], mean action: -0.530 [-1.148, 1.095], mean observation: 0.106 [-7.212, 3.979], loss: 0.766500, mean_absolute_error: 0.838210, mean_q: 3.823413
 196274/1000000: episode: 815, duration: 1.921s, episode steps: 257, steps per second: 134, episode reward: -7.407, mean reward: -0.029 [-10.880, 0.398], mean action: -0.505 [-1.089, 1.060], mean observation: -0.147 [-5.380, 1.000], loss: 0.794809, mean_absolute_error: 0.844049, mean_q: 3.734108
 196551/1000000: episode: 816, duration: 2.093s, episode steps: 277, steps per second: 132, episode reward: -8.249, mean reward: -0.030 [-10.388, 0.239], mean action: -0.614 [-1.110, 1.071], mean observation: -0.072 [-4.425, 1.000], loss: 0.799361, mean_absolute_error: 0.849466, mean_q: 3.781724
 197086/1000000: episode: 817, duration: 4.004s, episode steps: 535, steps per second: 134, episode reward: 149.099, mean reward: 0.279 [-11.185, 0.462], mean action: -0.826 [-1.145, 1.135], mean observation: 0.191 [-7.450, 5.213], loss: 0.802127, mean_absolute_error: 0.849629, mean_q: 3.809338
 197349/1000000: episode: 818, duration: 1.973s, episode steps: 263, steps per second: 133, episode reward: 34.374, mean reward: 0.131 [-10.038, 0.192], mean action: -0.738 [-1.147, 0.988], mean observation: 0.168 [-2.753, 5.428], loss: 0.791074, mean_absolute_error: 0.845365, mean_q: 4.006150
 197582/1000000: episode: 819, duration: 1.788s, episode steps: 233, steps per second: 130, episode reward: -8.132, mean reward: -0.035 [-10.958, 0.267], mean action: -0.520 [-1.129, 1.035], mean observation: 0.141 [-7.978, 6.026], loss: 0.778680, mean_absolute_error: 0.848769, mean_q: 3.809382
 197863/1000000: episode: 820, duration: 2.096s, episode steps: 281, steps per second: 134, episode reward: -60.510, mean reward: -0.215 [-11.029, 0.340], mean action: -0.510 [-1.090, 1.034], mean observation: 0.059 [-7.889, 5.952], loss: 0.770755, mean_absolute_error: 0.840211, mean_q: 3.739112
 198149/1000000: episode: 821, duration: 2.128s, episode steps: 286, steps per second: 134, episode reward: -4.309, mean reward: -0.015 [-10.663, 0.518], mean action: -0.486 [-1.089, 1.071], mean observation: 0.032 [-7.898, 5.989], loss: 0.803317, mean_absolute_error: 0.849124, mean_q: 3.843465
 198418/1000000: episode: 822, duration: 2.040s, episode steps: 269, steps per second: 132, episode reward: -18.180, mean reward: -0.068 [-10.566, 0.269], mean action: -0.509 [-1.136, 1.044], mean observation: 0.052 [-7.982, 6.053], loss: 0.806019, mean_absolute_error: 0.853703, mean_q: 3.866119
 198670/1000000: episode: 823, duration: 1.873s, episode steps: 252, steps per second: 135, episode reward: 29.578, mean reward: 0.117 [-10.938, 0.576], mean action: -0.485 [-1.081, 1.138], mean observation: 0.087 [-8.053, 6.079], loss: 0.804953, mean_absolute_error: 0.851852, mean_q: 3.937577
 198947/1000000: episode: 824, duration: 2.069s, episode steps: 277, steps per second: 134, episode reward: -8.047, mean reward: -0.029 [-10.867, 0.558], mean action: -0.497 [-1.116, 1.049], mean observation: 0.047 [-8.005, 6.036], loss: 0.814781, mean_absolute_error: 0.864741, mean_q: 3.883182
 199163/1000000: episode: 825, duration: 1.606s, episode steps: 216, steps per second: 135, episode reward: -32.399, mean reward: -0.150 [-11.153, 0.151], mean action: -0.501 [-1.141, 1.128], mean observation: 0.188 [-7.720, 6.043], loss: 0.837815, mean_absolute_error: 0.866122, mean_q: 3.887976
 199412/1000000: episode: 826, duration: 1.853s, episode steps: 249, steps per second: 134, episode reward: 7.777, mean reward: 0.031 [-10.817, 0.354], mean action: -0.513 [-1.114, 1.085], mean observation: 0.097 [-7.969, 6.025], loss: 0.789022, mean_absolute_error: 0.855474, mean_q: 3.888013
 199671/1000000: episode: 827, duration: 1.965s, episode steps: 259, steps per second: 132, episode reward: -44.744, mean reward: -0.173 [-11.297, 0.365], mean action: -0.522 [-1.147, 1.032], mean observation: 0.109 [-7.965, 6.010], loss: 0.800035, mean_absolute_error: 0.857623, mean_q: 3.777058
 199929/1000000: episode: 828, duration: 1.921s, episode steps: 258, steps per second: 134, episode reward: -41.944, mean reward: -0.163 [-11.094, 0.269], mean action: -0.494 [-1.088, 1.053], mean observation: 0.113 [-7.947, 6.013], loss: 0.815587, mean_absolute_error: 0.855710, mean_q: 3.825224
 200168/1000000: episode: 829, duration: 1.805s, episode steps: 239, steps per second: 132, episode reward: 10.271, mean reward: 0.043 [-10.901, 0.366], mean action: -0.502 [-1.113, 1.067], mean observation: 0.105 [-7.951, 6.036], loss: 0.827074, mean_absolute_error: 0.855685, mean_q: 3.894076
 200375/1000000: episode: 830, duration: 1.577s, episode steps: 207, steps per second: 131, episode reward: 30.964, mean reward: 0.150 [-10.673, 0.380], mean action: -0.505 [-1.136, 1.067], mean observation: 0.168 [-7.221, 6.058], loss: 0.835413, mean_absolute_error: 0.869312, mean_q: 3.941475
 200609/1000000: episode: 831, duration: 1.785s, episode steps: 234, steps per second: 131, episode reward: 24.844, mean reward: 0.106 [-11.036, 0.493], mean action: -0.490 [-1.114, 1.126], mean observation: 0.138 [-7.997, 6.052], loss: 0.768866, mean_absolute_error: 0.848785, mean_q: 3.758803
 200849/1000000: episode: 832, duration: 1.827s, episode steps: 240, steps per second: 131, episode reward: -53.021, mean reward: -0.221 [-11.381, 0.226], mean action: -0.493 [-1.114, 1.081], mean observation: 0.114 [-7.999, 6.078], loss: 0.783975, mean_absolute_error: 0.863529, mean_q: 3.835070
 201094/1000000: episode: 833, duration: 1.853s, episode steps: 245, steps per second: 132, episode reward: -35.968, mean reward: -0.147 [-11.039, 0.202], mean action: -0.516 [-1.137, 1.087], mean observation: 0.127 [-7.998, 6.030], loss: 0.788221, mean_absolute_error: 0.853261, mean_q: 3.924542
 201329/1000000: episode: 834, duration: 1.749s, episode steps: 235, steps per second: 134, episode reward: -19.783, mean reward: -0.084 [-11.283, 0.338], mean action: -0.484 [-1.146, 1.165], mean observation: 0.145 [-8.004, 6.057], loss: 0.801562, mean_absolute_error: 0.865342, mean_q: 3.774022
 201533/1000000: episode: 835, duration: 1.534s, episode steps: 204, steps per second: 133, episode reward: -34.068, mean reward: -0.167 [-11.055, 0.092], mean action: -0.493 [-1.094, 1.101], mean observation: 0.168 [-7.015, 6.052], loss: 0.832412, mean_absolute_error: 0.858146, mean_q: 3.859028
 201803/1000000: episode: 836, duration: 2.021s, episode steps: 270, steps per second: 134, episode reward: -34.644, mean reward: -0.128 [-10.984, 0.404], mean action: -0.492 [-1.083, 1.089], mean observation: 0.088 [-7.971, 6.043], loss: 0.804266, mean_absolute_error: 0.861838, mean_q: 3.812383
 202042/1000000: episode: 837, duration: 1.784s, episode steps: 239, steps per second: 134, episode reward: 20.750, mean reward: 0.087 [-10.992, 0.484], mean action: -0.478 [-1.108, 1.145], mean observation: 0.109 [-7.998, 6.115], loss: 0.818003, mean_absolute_error: 0.865356, mean_q: 3.785405
 202214/1000000: episode: 838, duration: 1.288s, episode steps: 172, steps per second: 134, episode reward: 13.658, mean reward: 0.079 [-10.244, 0.202], mean action: -0.523 [-1.112, 1.029], mean observation: 0.205 [-3.991, 6.022], loss: 0.768117, mean_absolute_error: 0.849911, mean_q: 3.648373
 202384/1000000: episode: 839, duration: 1.289s, episode steps: 170, steps per second: 132, episode reward: -15.146, mean reward: -0.089 [-10.474, 0.052], mean action: -0.491 [-1.122, 1.083], mean observation: 0.227 [-3.993, 6.050], loss: 0.819972, mean_absolute_error: 0.871646, mean_q: 3.888749
 202580/1000000: episode: 840, duration: 1.477s, episode steps: 196, steps per second: 133, episode reward: -6.812, mean reward: -0.035 [-10.810, 0.192], mean action: -0.477 [-1.074, 1.143], mean observation: 0.190 [-6.266, 6.037], loss: 0.829564, mean_absolute_error: 0.866611, mean_q: 3.724957
 202830/1000000: episode: 841, duration: 1.906s, episode steps: 250, steps per second: 131, episode reward: 20.136, mean reward: 0.081 [-10.976, 0.492], mean action: -0.488 [-1.133, 1.095], mean observation: 0.109 [-7.915, 6.019], loss: 0.772437, mean_absolute_error: 0.855306, mean_q: 3.846827
 203028/1000000: episode: 842, duration: 1.479s, episode steps: 198, steps per second: 134, episode reward: 31.949, mean reward: 0.161 [-10.564, 0.366], mean action: -0.499 [-1.085, 1.076], mean observation: 0.187 [-6.531, 6.062], loss: 0.782122, mean_absolute_error: 0.852973, mean_q: 3.715602
 203186/1000000: episode: 843, duration: 1.180s, episode steps: 158, steps per second: 134, episode reward: 27.905, mean reward: 0.177 [-10.040, 0.288], mean action: -0.497 [-1.074, 1.078], mean observation: 0.193 [-2.844, 5.883], loss: 0.813034, mean_absolute_error: 0.865914, mean_q: 3.895049
 203341/1000000: episode: 844, duration: 1.157s, episode steps: 155, steps per second: 134, episode reward: 29.379, mean reward: 0.190 [-9.997, 0.297], mean action: -0.491 [-1.071, 1.074], mean observation: 0.193 [-2.721, 5.852], loss: 0.762419, mean_absolute_error: 0.852042, mean_q: 3.883106
 203558/1000000: episode: 845, duration: 1.621s, episode steps: 217, steps per second: 134, episode reward: 1.027, mean reward: 0.005 [-10.722, 0.214], mean action: -0.496 [-1.083, 1.106], mean observation: 0.148 [-7.587, 5.979], loss: 0.808363, mean_absolute_error: 0.865132, mean_q: 3.830967
 203827/1000000: episode: 846, duration: 2.009s, episode steps: 269, steps per second: 134, episode reward: 38.046, mean reward: 0.141 [-10.711, 0.612], mean action: -0.517 [-1.133, 1.047], mean observation: 0.054 [-7.956, 6.060], loss: 0.774841, mean_absolute_error: 0.867325, mean_q: 3.939543
 203994/1000000: episode: 847, duration: 1.257s, episode steps: 167, steps per second: 133, episode reward: 1.408, mean reward: 0.008 [-10.220, 0.113], mean action: -0.480 [-1.076, 1.077], mean observation: 0.189 [-3.673, 6.059], loss: 0.745961, mean_absolute_error: 0.856963, mean_q: 3.789951
 204204/1000000: episode: 848, duration: 1.588s, episode steps: 210, steps per second: 132, episode reward: 4.556, mean reward: 0.022 [-10.665, 0.218], mean action: -0.487 [-1.084, 1.071], mean observation: 0.148 [-7.190, 5.991], loss: 0.797221, mean_absolute_error: 0.861295, mean_q: 3.744007
 204465/1000000: episode: 849, duration: 1.980s, episode steps: 261, steps per second: 132, episode reward: 20.218, mean reward: 0.077 [-11.002, 0.593], mean action: -0.472 [-1.089, 1.093], mean observation: 0.092 [-7.961, 6.012], loss: 0.802900, mean_absolute_error: 0.865018, mean_q: 3.878918
 204719/1000000: episode: 850, duration: 1.937s, episode steps: 254, steps per second: 131, episode reward: -21.786, mean reward: -0.086 [-11.131, 0.400], mean action: -0.501 [-1.093, 1.096], mean observation: 0.086 [-7.999, 6.066], loss: 0.819997, mean_absolute_error: 0.869833, mean_q: 3.765351
 204967/1000000: episode: 851, duration: 1.900s, episode steps: 248, steps per second: 131, episode reward: -51.939, mean reward: -0.209 [-11.371, 0.251], mean action: -0.469 [-1.080, 1.091], mean observation: 0.146 [-7.933, 5.996], loss: 0.848347, mean_absolute_error: 0.876660, mean_q: 3.689627
 205234/1000000: episode: 852, duration: 2.002s, episode steps: 267, steps per second: 133, episode reward: -39.270, mean reward: -0.147 [-10.848, 0.319], mean action: -0.518 [-1.164, 1.086], mean observation: 0.045 [-8.030, 6.097], loss: 0.855897, mean_absolute_error: 0.875917, mean_q: 3.813365
 205473/1000000: episode: 853, duration: 1.834s, episode steps: 239, steps per second: 130, episode reward: -47.541, mean reward: -0.199 [-11.400, 0.239], mean action: -0.496 [-1.112, 1.088], mean observation: 0.154 [-7.969, 6.032], loss: 0.833209, mean_absolute_error: 0.879097, mean_q: 3.750081
 205720/1000000: episode: 854, duration: 1.861s, episode steps: 247, steps per second: 133, episode reward: -13.820, mean reward: -0.056 [-10.850, 0.251], mean action: -0.478 [-1.085, 1.115], mean observation: 0.107 [-7.959, 6.009], loss: 0.805857, mean_absolute_error: 0.876199, mean_q: 3.725055
 205909/1000000: episode: 855, duration: 1.429s, episode steps: 189, steps per second: 132, episode reward: -19.839, mean reward: -0.105 [-10.554, 0.036], mean action: -0.479 [-1.079, 1.116], mean observation: 0.150 [-5.550, 6.014], loss: 0.794759, mean_absolute_error: 0.859661, mean_q: 3.631510
 206121/1000000: episode: 856, duration: 1.606s, episode steps: 212, steps per second: 132, episode reward: 17.957, mean reward: 0.085 [-10.831, 0.348], mean action: -0.492 [-1.073, 1.150], mean observation: 0.152 [-7.600, 6.080], loss: 0.810675, mean_absolute_error: 0.859959, mean_q: 3.869495
 206357/1000000: episode: 857, duration: 1.786s, episode steps: 236, steps per second: 132, episode reward: 2.693, mean reward: 0.011 [-11.059, 0.371], mean action: -0.492 [-1.101, 1.088], mean observation: 0.118 [-7.960, 6.028], loss: 0.826353, mean_absolute_error: 0.878348, mean_q: 3.715641
 206622/1000000: episode: 858, duration: 2.009s, episode steps: 265, steps per second: 132, episode reward: -63.540, mean reward: -0.240 [-11.300, 0.301], mean action: -0.458 [-1.052, 1.074], mean observation: 0.086 [-7.897, 5.981], loss: 0.844598, mean_absolute_error: 0.874533, mean_q: 3.725173
 206877/1000000: episode: 859, duration: 1.930s, episode steps: 255, steps per second: 132, episode reward: -37.391, mean reward: -0.147 [-11.105, 0.296], mean action: -0.503 [-1.090, 1.060], mean observation: 0.075 [-7.968, 6.067], loss: 0.815095, mean_absolute_error: 0.875322, mean_q: 3.693107
 207132/1000000: episode: 860, duration: 1.917s, episode steps: 255, steps per second: 133, episode reward: 27.711, mean reward: 0.109 [-10.880, 0.544], mean action: -0.497 [-1.093, 1.108], mean observation: 0.080 [-7.975, 6.066], loss: 0.823856, mean_absolute_error: 0.872567, mean_q: 3.650603
 207376/1000000: episode: 861, duration: 1.841s, episode steps: 244, steps per second: 133, episode reward: 1.115, mean reward: 0.005 [-10.981, 0.381], mean action: -0.530 [-1.155, 1.055], mean observation: 0.121 [-7.983, 6.070], loss: 0.828811, mean_absolute_error: 0.876656, mean_q: 3.740648
 207614/1000000: episode: 862, duration: 1.826s, episode steps: 238, steps per second: 130, episode reward: -26.078, mean reward: -0.110 [-11.026, 0.214], mean action: -0.502 [-1.110, 1.102], mean observation: 0.099 [-8.010, 6.063], loss: 0.842588, mean_absolute_error: 0.883239, mean_q: 3.583322
 207824/1000000: episode: 863, duration: 1.591s, episode steps: 210, steps per second: 132, episode reward: -14.992, mean reward: -0.071 [-10.922, 0.171], mean action: -0.479 [-1.077, 1.143], mean observation: 0.157 [-7.332, 6.041], loss: 0.845022, mean_absolute_error: 0.871513, mean_q: 3.678697
 208088/1000000: episode: 864, duration: 1.994s, episode steps: 264, steps per second: 132, episode reward: 6.933, mean reward: 0.026 [-10.843, 0.469], mean action: -0.508 [-1.139, 1.088], mean observation: 0.062 [-7.917, 6.052], loss: 0.870121, mean_absolute_error: 0.879772, mean_q: 3.763503
 208300/1000000: episode: 865, duration: 1.641s, episode steps: 212, steps per second: 129, episode reward: -22.466, mean reward: -0.106 [-10.825, 0.100], mean action: -0.494 [-1.092, 1.081], mean observation: 0.144 [-7.311, 5.998], loss: 0.800667, mean_absolute_error: 0.873871, mean_q: 3.750895
 208548/1000000: episode: 866, duration: 1.861s, episode steps: 248, steps per second: 133, episode reward: -72.766, mean reward: -0.293 [-11.441, 0.173], mean action: -0.509 [-1.111, 1.123], mean observation: 0.117 [-7.969, 6.006], loss: 0.815509, mean_absolute_error: 0.876305, mean_q: 3.572911
 208811/1000000: episode: 867, duration: 1.980s, episode steps: 263, steps per second: 133, episode reward: -80.807, mean reward: -0.307 [-11.231, 0.210], mean action: -0.503 [-1.108, 1.073], mean observation: 0.109 [-7.983, 6.066], loss: 0.833887, mean_absolute_error: 0.879974, mean_q: 3.774804
 209077/1000000: episode: 868, duration: 2.023s, episode steps: 266, steps per second: 131, episode reward: -24.794, mean reward: -0.093 [-10.624, 0.263], mean action: -0.495 [-1.108, 1.143], mean observation: 0.041 [-8.019, 6.083], loss: 0.835864, mean_absolute_error: 0.875610, mean_q: 3.553808
 209339/1000000: episode: 869, duration: 1.992s, episode steps: 262, steps per second: 132, episode reward: 0.261, mean reward: 0.001 [-10.690, 0.370], mean action: -0.505 [-1.092, 1.086], mean observation: 0.075 [-8.007, 6.061], loss: 0.848927, mean_absolute_error: 0.886740, mean_q: 3.819728
 209611/1000000: episode: 870, duration: 2.049s, episode steps: 272, steps per second: 133, episode reward: -44.115, mean reward: -0.162 [-11.088, 0.428], mean action: -0.524 [-1.174, 1.052], mean observation: 0.075 [-7.961, 6.043], loss: 0.848006, mean_absolute_error: 0.886530, mean_q: 3.768591
 209858/1000000: episode: 871, duration: 1.878s, episode steps: 247, steps per second: 132, episode reward: -30.542, mean reward: -0.124 [-11.287, 0.347], mean action: -0.501 [-1.120, 1.078], mean observation: 0.135 [-7.991, 6.045], loss: 0.809474, mean_absolute_error: 0.874728, mean_q: 3.699306
 210107/1000000: episode: 872, duration: 1.986s, episode steps: 249, steps per second: 125, episode reward: -3.823, mean reward: -0.015 [-10.847, 0.315], mean action: -0.481 [-1.078, 1.094], mean observation: 0.078 [-7.996, 6.032], loss: 0.806476, mean_absolute_error: 0.875036, mean_q: 3.757402
 210372/1000000: episode: 873, duration: 2.019s, episode steps: 265, steps per second: 131, episode reward: -19.526, mean reward: -0.074 [-10.875, 0.344], mean action: -0.516 [-1.111, 1.034], mean observation: 0.091 [-7.889, 6.001], loss: 0.824356, mean_absolute_error: 0.880873, mean_q: 3.616196
 210509/1000000: episode: 874, duration: 1.078s, episode steps: 137, steps per second: 127, episode reward: 6.572, mean reward: 0.048 [-10.023, 0.147], mean action: -0.489 [-1.094, 1.076], mean observation: 0.196 [-1.723, 5.160], loss: 0.848669, mean_absolute_error: 0.877179, mean_q: 3.650605
 210770/1000000: episode: 875, duration: 1.984s, episode steps: 261, steps per second: 132, episode reward: -16.927, mean reward: -0.065 [-10.774, 0.304], mean action: -0.511 [-1.149, 1.083], mean observation: 0.085 [-7.989, 6.054], loss: 0.782555, mean_absolute_error: 0.872956, mean_q: 3.676892
 210984/1000000: episode: 876, duration: 1.604s, episode steps: 214, steps per second: 133, episode reward: -19.177, mean reward: -0.090 [-11.027, 0.187], mean action: -0.487 [-1.137, 1.126], mean observation: 0.155 [-7.642, 6.053], loss: 0.822447, mean_absolute_error: 0.873491, mean_q: 3.688916
 211144/1000000: episode: 877, duration: 1.200s, episode steps: 160, steps per second: 133, episode reward: -11.275, mean reward: -0.070 [-10.330, 0.050], mean action: -0.505 [-1.114, 1.076], mean observation: 0.192 [-3.063, 5.971], loss: 0.810055, mean_absolute_error: 0.874139, mean_q: 3.591029
 211392/1000000: episode: 878, duration: 1.866s, episode steps: 248, steps per second: 133, episode reward: -61.521, mean reward: -0.248 [-11.410, 0.228], mean action: -0.461 [-1.074, 1.131], mean observation: 0.117 [-7.959, 6.041], loss: 0.829826, mean_absolute_error: 0.885394, mean_q: 3.556767
 211606/1000000: episode: 879, duration: 1.639s, episode steps: 214, steps per second: 131, episode reward: 3.753, mean reward: 0.018 [-10.740, 0.237], mean action: -0.520 [-1.109, 1.070], mean observation: 0.154 [-7.615, 6.036], loss: 0.832766, mean_absolute_error: 0.879797, mean_q: 3.719162
 211829/1000000: episode: 880, duration: 1.695s, episode steps: 223, steps per second: 132, episode reward: 21.617, mean reward: 0.097 [-10.955, 0.415], mean action: -0.505 [-1.118, 1.049], mean observation: 0.167 [-7.938, 6.040], loss: 0.824556, mean_absolute_error: 0.884424, mean_q: 3.591419
 212048/1000000: episode: 881, duration: 1.681s, episode steps: 219, steps per second: 130, episode reward: 5.768, mean reward: 0.026 [-10.797, 0.267], mean action: -0.506 [-1.095, 1.128], mean observation: 0.156 [-7.803, 6.024], loss: 0.830593, mean_absolute_error: 0.883293, mean_q: 3.564694
 212308/1000000: episode: 882, duration: 1.953s, episode steps: 260, steps per second: 133, episode reward: 22.847, mean reward: 0.088 [-10.790, 0.482], mean action: -0.508 [-1.087, 1.045], mean observation: 0.079 [-7.921, 6.045], loss: 0.782022, mean_absolute_error: 0.867357, mean_q: 3.672110
 212588/1000000: episode: 883, duration: 2.098s, episode steps: 280, steps per second: 133, episode reward: -30.270, mean reward: -0.108 [-10.587, 0.352], mean action: -0.508 [-1.156, 1.094], mean observation: 0.051 [-7.997, 6.093], loss: 0.827233, mean_absolute_error: 0.879397, mean_q: 3.589972
 212791/1000000: episode: 884, duration: 1.521s, episode steps: 203, steps per second: 134, episode reward: -37.795, mean reward: -0.186 [-11.043, 0.062], mean action: -0.507 [-1.095, 1.036], mean observation: 0.190 [-6.680, 6.002], loss: 0.774932, mean_absolute_error: 0.860036, mean_q: 3.490193
 213074/1000000: episode: 885, duration: 2.162s, episode steps: 283, steps per second: 131, episode reward: 7.169, mean reward: 0.025 [-10.748, 0.644], mean action: -0.490 [-1.120, 1.140], mean observation: 0.047 [-7.993, 6.045], loss: 0.867388, mean_absolute_error: 0.896674, mean_q: 3.664114
 213231/1000000: episode: 886, duration: 1.188s, episode steps: 157, steps per second: 132, episode reward: 0.400, mean reward: 0.003 [-10.223, 0.118], mean action: -0.494 [-1.109, 1.102], mean observation: 0.188 [-2.823, 5.881], loss: 0.845831, mean_absolute_error: 0.898934, mean_q: 3.502377
 213401/1000000: episode: 887, duration: 1.281s, episode steps: 170, steps per second: 133, episode reward: 35.009, mean reward: 0.206 [-10.118, 0.330], mean action: -0.480 [-1.092, 1.137], mean observation: 0.189 [-3.812, 6.036], loss: 0.855515, mean_absolute_error: 0.891824, mean_q: 3.491670
 213681/1000000: episode: 888, duration: 2.166s, episode steps: 280, steps per second: 129, episode reward: 9.566, mean reward: 0.034 [-10.624, 0.494], mean action: -0.500 [-1.104, 1.071], mean observation: 0.033 [-7.908, 6.013], loss: 0.830160, mean_absolute_error: 0.886008, mean_q: 3.658904
 213928/1000000: episode: 889, duration: 1.951s, episode steps: 247, steps per second: 127, episode reward: -46.702, mean reward: -0.189 [-10.950, 0.145], mean action: -0.512 [-1.146, 1.136], mean observation: 0.073 [-8.073, 6.093], loss: 0.860674, mean_absolute_error: 0.891066, mean_q: 3.531626
 214180/1000000: episode: 890, duration: 1.923s, episode steps: 252, steps per second: 131, episode reward: 27.283, mean reward: 0.108 [-10.907, 0.548], mean action: -0.485 [-1.159, 1.115], mean observation: 0.096 [-8.054, 6.088], loss: 0.840274, mean_absolute_error: 0.897458, mean_q: 3.545759
 214449/1000000: episode: 891, duration: 2.064s, episode steps: 269, steps per second: 130, episode reward: -43.674, mean reward: -0.162 [-11.182, 0.410], mean action: -0.496 [-1.114, 1.050], mean observation: 0.087 [-7.931, 6.004], loss: 0.833061, mean_absolute_error: 0.885069, mean_q: 3.595244
 214695/1000000: episode: 892, duration: 1.901s, episode steps: 246, steps per second: 129, episode reward: -38.012, mean reward: -0.155 [-11.321, 0.307], mean action: -0.496 [-1.164, 1.105], mean observation: 0.136 [-7.967, 6.038], loss: 0.881031, mean_absolute_error: 0.894765, mean_q: 3.454612
 214951/1000000: episode: 893, duration: 2.170s, episode steps: 256, steps per second: 118, episode reward: 18.336, mean reward: 0.072 [-11.043, 0.569], mean action: -0.511 [-1.109, 1.041], mean observation: 0.103 [-7.946, 6.025], loss: 0.829516, mean_absolute_error: 0.885335, mean_q: 3.660181
 215179/1000000: episode: 894, duration: 1.763s, episode steps: 228, steps per second: 129, episode reward: 0.347, mean reward: 0.002 [-10.816, 0.254], mean action: -0.487 [-1.066, 1.079], mean observation: 0.122 [-7.960, 6.010], loss: 0.834155, mean_absolute_error: 0.884858, mean_q: 3.562613
 215481/1000000: episode: 895, duration: 2.468s, episode steps: 302, steps per second: 122, episode reward: -12.315, mean reward: -0.041 [-11.542, 0.423], mean action: -0.681 [-1.117, 1.078], mean observation: 0.178 [-7.153, 4.484], loss: 0.834023, mean_absolute_error: 0.886307, mean_q: 3.552252
 215644/1000000: episode: 896, duration: 1.320s, episode steps: 163, steps per second: 123, episode reward: 2.458, mean reward: 0.015 [-10.426, 0.179], mean action: -0.503 [-1.114, 1.126], mean observation: -0.056 [-6.074, 1.000], loss: 0.823743, mean_absolute_error: 0.879610, mean_q: 3.510828
 215863/1000000: episode: 897, duration: 1.784s, episode steps: 219, steps per second: 123, episode reward: -25.856, mean reward: -0.118 [-11.150, 0.231], mean action: -0.497 [-1.072, 1.056], mean observation: -0.178 [-7.920, 1.000], loss: 0.896749, mean_absolute_error: 0.912272, mean_q: 3.600592
 216108/1000000: episode: 898, duration: 1.889s, episode steps: 245, steps per second: 130, episode reward: 16.730, mean reward: 0.068 [-10.837, 0.461], mean action: -0.481 [-1.086, 1.067], mean observation: -0.169 [-7.929, 3.568], loss: 0.828560, mean_absolute_error: 0.886416, mean_q: 3.610033
 216326/1000000: episode: 899, duration: 1.698s, episode steps: 218, steps per second: 128, episode reward: 19.243, mean reward: 0.088 [-10.915, 0.405], mean action: -0.496 [-1.091, 1.080], mean observation: -0.140 [-7.892, 1.000], loss: 0.851455, mean_absolute_error: 0.899712, mean_q: 3.369117
 216559/1000000: episode: 900, duration: 1.766s, episode steps: 233, steps per second: 132, episode reward: -9.813, mean reward: -0.042 [-11.044, 0.336], mean action: -0.500 [-1.114, 1.058], mean observation: -0.183 [-7.892, 1.719], loss: 0.855767, mean_absolute_error: 0.905768, mean_q: 3.592550
 216824/1000000: episode: 901, duration: 1.986s, episode steps: 265, steps per second: 133, episode reward: -10.856, mean reward: -0.041 [-10.496, 0.354], mean action: -0.507 [-1.110, 1.119], mean observation: -0.161 [-8.006, 5.166], loss: 0.874921, mean_absolute_error: 0.899837, mean_q: 3.547102
 217087/1000000: episode: 902, duration: 1.979s, episode steps: 263, steps per second: 133, episode reward: -5.449, mean reward: -0.021 [-11.023, 0.589], mean action: -0.510 [-1.115, 1.075], mean observation: -0.186 [-7.870, 4.948], loss: 0.844922, mean_absolute_error: 0.895864, mean_q: 3.572185
 217328/1000000: episode: 903, duration: 1.855s, episode steps: 241, steps per second: 130, episode reward: -58.881, mean reward: -0.244 [-11.359, 0.258], mean action: -0.506 [-1.122, 1.083], mean observation: -0.207 [-7.974, 3.120], loss: 0.843286, mean_absolute_error: 0.895504, mean_q: 3.503389
 217596/1000000: episode: 904, duration: 2.025s, episode steps: 268, steps per second: 132, episode reward: 7.017, mean reward: 0.026 [-10.686, 0.551], mean action: -0.519 [-1.155, 1.054], mean observation: -0.165 [-7.995, 5.178], loss: 0.852783, mean_absolute_error: 0.895020, mean_q: 3.529123
 217852/1000000: episode: 905, duration: 1.916s, episode steps: 256, steps per second: 134, episode reward: -61.221, mean reward: -0.239 [-11.230, 0.124], mean action: -0.865 [-1.127, 1.071], mean observation: -0.117 [-4.697, 1.000], loss: 0.890390, mean_absolute_error: 0.900001, mean_q: 3.461587
 218808/1000000: episode: 906, duration: 7.136s, episode steps: 956, steps per second: 134, episode reward: 176.789, mean reward: 0.185 [-10.837, 0.342], mean action: -1.004 [-1.119, -0.809], mean observation: 0.132 [-2.515, 1.401], loss: 0.873045, mean_absolute_error: 0.905700, mean_q: 3.505144
 219516/1000000: episode: 907, duration: 5.480s, episode steps: 708, steps per second: 129, episode reward: 184.575, mean reward: 0.261 [-10.603, 0.465], mean action: -1.006 [-1.170, -0.872], mean observation: 0.136 [-1.862, 1.768], loss: 0.909826, mean_absolute_error: 0.916490, mean_q: 3.510751
 220297/1000000: episode: 908, duration: 5.905s, episode steps: 781, steps per second: 132, episode reward: 132.038, mean reward: 0.169 [-11.863, 0.456], mean action: -0.845 [-1.111, 1.116], mean observation: 0.210 [-7.664, 4.722], loss: 0.893814, mean_absolute_error: 0.918362, mean_q: 3.564304
 220541/1000000: episode: 909, duration: 1.806s, episode steps: 244, steps per second: 135, episode reward: 6.296, mean reward: 0.026 [-11.012, 0.404], mean action: -0.501 [-1.169, 1.089], mean observation: 0.098 [-7.981, 6.025], loss: 0.900272, mean_absolute_error: 0.923264, mean_q: 3.512045
 220778/1000000: episode: 910, duration: 1.822s, episode steps: 237, steps per second: 130, episode reward: 21.716, mean reward: 0.092 [-10.951, 0.439], mean action: -0.469 [-1.067, 1.121], mean observation: 0.124 [-7.975, 6.030], loss: 0.880960, mean_absolute_error: 0.915032, mean_q: 3.585211
 221058/1000000: episode: 911, duration: 2.096s, episode steps: 280, steps per second: 134, episode reward: 20.716, mean reward: 0.074 [-10.469, 0.479], mean action: -0.504 [-1.123, 1.089], mean observation: 0.049 [-8.021, 6.044], loss: 0.887610, mean_absolute_error: 0.913949, mean_q: 3.547062
 221258/1000000: episode: 912, duration: 1.494s, episode steps: 200, steps per second: 134, episode reward: 14.500, mean reward: 0.072 [-10.505, 0.249], mean action: -0.658 [-1.087, 1.126], mean observation: 0.186 [-5.617, 5.976], loss: 0.922458, mean_absolute_error: 0.937053, mean_q: 3.535378
 221533/1000000: episode: 913, duration: 2.052s, episode steps: 275, steps per second: 134, episode reward: -2.527, mean reward: -0.009 [-10.984, 0.287], mean action: -0.816 [-1.100, 1.047], mean observation: 0.175 [-5.003, 5.210], loss: 0.903472, mean_absolute_error: 0.920112, mean_q: 3.537587
 221747/1000000: episode: 914, duration: 1.618s, episode steps: 214, steps per second: 132, episode reward: 18.073, mean reward: 0.084 [-10.447, 0.252], mean action: -0.744 [-1.138, 0.992], mean observation: 0.207 [-4.327, 5.418], loss: 0.907511, mean_absolute_error: 0.939147, mean_q: 3.610962
 222112/1000000: episode: 915, duration: 2.750s, episode steps: 365, steps per second: 133, episode reward: 13.927, mean reward: 0.038 [-11.409, 0.510], mean action: -0.943 [-1.160, 0.863], mean observation: 0.172 [-3.129, 3.876], loss: 0.899033, mean_absolute_error: 0.922443, mean_q: 3.518347
 222414/1000000: episode: 916, duration: 2.255s, episode steps: 302, steps per second: 134, episode reward: -25.524, mean reward: -0.085 [-11.392, 0.390], mean action: -0.875 [-1.118, 1.026], mean observation: 0.190 [-3.720, 4.628], loss: 0.902309, mean_absolute_error: 0.921632, mean_q: 3.535073
 223132/1000000: episode: 917, duration: 5.370s, episode steps: 718, steps per second: 134, episode reward: 210.547, mean reward: 0.293 [-10.608, 0.671], mean action: -0.987 [-1.126, -0.670], mean observation: 0.253 [-0.427, 2.303], loss: 0.903609, mean_absolute_error: 0.930058, mean_q: 3.562467
 223851/1000000: episode: 918, duration: 5.460s, episode steps: 719, steps per second: 132, episode reward: 61.136, mean reward: 0.085 [-10.529, 0.272], mean action: -1.005 [-1.149, -0.848], mean observation: 0.154 [-0.698, 1.761], loss: 0.912789, mean_absolute_error: 0.934206, mean_q: 3.549417
 224423/1000000: episode: 919, duration: 4.293s, episode steps: 572, steps per second: 133, episode reward: 81.158, mean reward: 0.142 [-10.259, 0.272], mean action: -1.000 [-1.125, -0.878], mean observation: 0.215 [-0.227, 1.471], loss: 0.910168, mean_absolute_error: 0.932933, mean_q: 3.583012
 225195/1000000: episode: 920, duration: 5.850s, episode steps: 772, steps per second: 132, episode reward: 121.177, mean reward: 0.157 [-11.024, 0.347], mean action: -0.921 [-1.128, 0.892], mean observation: 0.153 [-6.340, 4.640], loss: 0.900633, mean_absolute_error: 0.930758, mean_q: 3.648530
 225380/1000000: episode: 921, duration: 1.393s, episode steps: 185, steps per second: 133, episode reward: 16.971, mean reward: 0.092 [-10.413, 0.261], mean action: -0.663 [-1.119, 1.108], mean observation: 0.194 [-4.310, 5.827], loss: 0.856283, mean_absolute_error: 0.931231, mean_q: 3.436915
 225623/1000000: episode: 922, duration: 1.836s, episode steps: 243, steps per second: 132, episode reward: -48.255, mean reward: -0.199 [-11.473, 0.242], mean action: -0.676 [-1.064, 1.074], mean observation: 0.169 [-6.814, 5.897], loss: 0.913408, mean_absolute_error: 0.937565, mean_q: 3.593887
 225835/1000000: episode: 923, duration: 1.697s, episode steps: 212, steps per second: 125, episode reward: -30.095, mean reward: -0.142 [-11.129, 0.149], mean action: -0.506 [-1.132, 1.062], mean observation: 0.166 [-7.485, 6.053], loss: 0.941619, mean_absolute_error: 0.947984, mean_q: 3.583491
 226085/1000000: episode: 924, duration: 1.882s, episode steps: 250, steps per second: 133, episode reward: 0.309, mean reward: 0.001 [-10.806, 0.331], mean action: -0.505 [-1.175, 1.046], mean observation: 0.082 [-8.038, 6.053], loss: 0.944642, mean_absolute_error: 0.948931, mean_q: 3.600442
 226352/1000000: episode: 925, duration: 2.004s, episode steps: 267, steps per second: 133, episode reward: -87.621, mean reward: -0.328 [-11.260, 0.217], mean action: -0.469 [-1.141, 1.132], mean observation: 0.113 [-7.991, 6.049], loss: 0.930052, mean_absolute_error: 0.947932, mean_q: 3.559640
 226573/1000000: episode: 926, duration: 1.667s, episode steps: 221, steps per second: 133, episode reward: -2.783, mean reward: -0.013 [-10.906, 0.256], mean action: -0.509 [-1.121, 1.041], mean observation: 0.160 [-7.874, 6.048], loss: 0.932369, mean_absolute_error: 0.954511, mean_q: 3.577300
 226855/1000000: episode: 927, duration: 2.097s, episode steps: 282, steps per second: 135, episode reward: 15.327, mean reward: 0.054 [-10.730, 0.682], mean action: -0.495 [-1.128, 1.098], mean observation: 0.052 [-7.973, 6.068], loss: 0.910527, mean_absolute_error: 0.944107, mean_q: 3.684297
 227066/1000000: episode: 928, duration: 1.577s, episode steps: 211, steps per second: 134, episode reward: -6.065, mean reward: -0.029 [-10.832, 0.206], mean action: -0.514 [-1.139, 1.110], mean observation: 0.139 [-7.613, 6.099], loss: 0.963980, mean_absolute_error: 0.972156, mean_q: 3.608582
 227299/1000000: episode: 929, duration: 1.772s, episode steps: 233, steps per second: 131, episode reward: -25.759, mean reward: -0.111 [-11.171, 0.246], mean action: -0.478 [-1.107, 1.122], mean observation: 0.161 [-7.968, 6.041], loss: 0.928612, mean_absolute_error: 0.949918, mean_q: 3.664510
 227550/1000000: episode: 930, duration: 1.957s, episode steps: 251, steps per second: 128, episode reward: -29.806, mean reward: -0.119 [-10.873, 0.221], mean action: -0.480 [-1.088, 1.110], mean observation: 0.098 [-8.016, 6.070], loss: 0.951211, mean_absolute_error: 0.958125, mean_q: 3.483722
 227791/1000000: episode: 931, duration: 1.801s, episode steps: 241, steps per second: 134, episode reward: 8.985, mean reward: 0.037 [-11.097, 0.439], mean action: -0.493 [-1.153, 1.136], mean observation: 0.122 [-7.968, 5.988], loss: 0.903708, mean_absolute_error: 0.937937, mean_q: 3.507881
 227991/1000000: episode: 932, duration: 1.497s, episode steps: 200, steps per second: 134, episode reward: -19.062, mean reward: -0.095 [-10.935, 0.148], mean action: -0.467 [-1.052, 1.117], mean observation: 0.182 [-6.717, 6.067], loss: 0.933293, mean_absolute_error: 0.959977, mean_q: 3.618869
 228196/1000000: episode: 933, duration: 1.606s, episode steps: 205, steps per second: 128, episode reward: 30.229, mean reward: 0.147 [-10.703, 0.387], mean action: -0.502 [-1.073, 1.089], mean observation: 0.181 [-6.947, 6.038], loss: 0.926047, mean_absolute_error: 0.960728, mean_q: 3.555125
 228455/1000000: episode: 934, duration: 2.247s, episode steps: 259, steps per second: 115, episode reward: 5.335, mean reward: 0.021 [-10.612, 0.211], mean action: -0.850 [-1.137, 1.069], mean observation: 0.172 [-4.030, 4.838], loss: 0.932778, mean_absolute_error: 0.966303, mean_q: 3.598001
 229021/1000000: episode: 935, duration: 4.369s, episode steps: 566, steps per second: 130, episode reward: 127.249, mean reward: 0.225 [-10.374, 0.393], mean action: -0.996 [-1.177, -0.845], mean observation: 0.208 [-0.443, 2.087], loss: 0.954956, mean_absolute_error: 0.958739, mean_q: 3.530432
 229615/1000000: episode: 936, duration: 4.490s, episode steps: 594, steps per second: 132, episode reward: 220.000, mean reward: 0.370 [-10.382, 0.595], mean action: -0.985 [-1.150, -0.850], mean observation: 0.022 [-2.133, 1.000], loss: 0.923855, mean_absolute_error: 0.956673, mean_q: 3.655709
 230061/1000000: episode: 937, duration: 3.314s, episode steps: 446, steps per second: 135, episode reward: 31.816, mean reward: 0.071 [-11.402, 0.348], mean action: -0.873 [-1.104, 0.953], mean observation: 0.143 [-6.072, 5.591], loss: 0.947639, mean_absolute_error: 0.961320, mean_q: 3.496494
 230219/1000000: episode: 938, duration: 1.176s, episode steps: 158, steps per second: 134, episode reward: 21.394, mean reward: 0.135 [-9.965, 0.227], mean action: -0.724 [-1.135, 0.987], mean observation: 0.199 [-2.009, 5.188], loss: 0.954332, mean_absolute_error: 0.968258, mean_q: 3.637969
 230479/1000000: episode: 939, duration: 1.935s, episode steps: 260, steps per second: 134, episode reward: -35.211, mean reward: -0.135 [-11.433, 0.320], mean action: -0.849 [-1.189, 1.077], mean observation: 0.195 [-5.209, 5.317], loss: 0.954997, mean_absolute_error: 0.970710, mean_q: 3.685820
 231172/1000000: episode: 940, duration: 5.173s, episode steps: 693, steps per second: 134, episode reward: 195.020, mean reward: 0.281 [-9.827, 0.314], mean action: -0.952 [-1.140, 1.030], mean observation: 0.081 [-0.907, 1.000], loss: 0.954332, mean_absolute_error: 0.973367, mean_q: 3.685793
 231437/1000000: episode: 941, duration: 1.999s, episode steps: 265, steps per second: 133, episode reward: -24.112, mean reward: -0.091 [-11.424, 0.386], mean action: -0.835 [-1.121, 1.077], mean observation: 0.205 [-5.179, 5.297], loss: 0.960680, mean_absolute_error: 0.975026, mean_q: 3.621482
 231762/1000000: episode: 942, duration: 2.482s, episode steps: 325, steps per second: 131, episode reward: -22.533, mean reward: -0.069 [-10.992, 0.224], mean action: -0.866 [-1.145, 0.925], mean observation: 0.247 [-2.861, 3.410], loss: 0.972191, mean_absolute_error: 0.976681, mean_q: 3.560346
 232001/1000000: episode: 943, duration: 1.865s, episode steps: 239, steps per second: 128, episode reward: 31.209, mean reward: 0.131 [-10.930, 0.456], mean action: -0.741 [-1.084, 1.016], mean observation: 0.163 [-6.050, 5.725], loss: 0.980710, mean_absolute_error: 0.975600, mean_q: 3.619153
 232277/1000000: episode: 944, duration: 2.055s, episode steps: 276, steps per second: 134, episode reward: -33.006, mean reward: -0.120 [-11.640, 0.497], mean action: -0.754 [-1.090, 1.081], mean observation: 0.146 [-6.134, 5.749], loss: 0.976805, mean_absolute_error: 0.976273, mean_q: 3.651192
 232569/1000000: episode: 945, duration: 2.227s, episode steps: 292, steps per second: 131, episode reward: -0.832, mean reward: -0.003 [-11.474, 0.549], mean action: -0.799 [-1.097, 1.110], mean observation: 0.123 [-5.703, 5.571], loss: 0.996179, mean_absolute_error: 0.981475, mean_q: 3.701732
 232856/1000000: episode: 946, duration: 2.140s, episode steps: 287, steps per second: 134, episode reward: -29.783, mean reward: -0.104 [-11.485, 0.422], mean action: -0.802 [-1.176, 1.089], mean observation: 0.145 [-5.834, 5.670], loss: 0.955253, mean_absolute_error: 0.985846, mean_q: 3.888138
 233140/1000000: episode: 947, duration: 2.254s, episode steps: 284, steps per second: 126, episode reward: -12.140, mean reward: -0.043 [-11.254, 0.361], mean action: -0.800 [-1.092, 1.016], mean observation: 0.142 [-5.693, 5.540], loss: 0.927758, mean_absolute_error: 0.967808, mean_q: 3.866065
 233441/1000000: episode: 948, duration: 2.362s, episode steps: 301, steps per second: 127, episode reward: 10.666, mean reward: 0.035 [-11.551, 0.644], mean action: -0.823 [-1.091, 1.099], mean observation: 0.153 [-5.333, 5.434], loss: 0.987927, mean_absolute_error: 0.988413, mean_q: 3.746893
 233735/1000000: episode: 949, duration: 2.227s, episode steps: 294, steps per second: 132, episode reward: -42.800, mean reward: -0.146 [-11.493, 0.334], mean action: -0.877 [-1.124, 1.066], mean observation: 0.180 [-4.383, 4.769], loss: 0.969249, mean_absolute_error: 0.974610, mean_q: 3.871498
 234578/1000000: episode: 950, duration: 6.275s, episode steps: 843, steps per second: 134, episode reward: 178.164, mean reward: 0.211 [-10.391, 0.446], mean action: -1.002 [-1.173, -0.869], mean observation: 0.155 [-0.865, 1.500], loss: 0.997443, mean_absolute_error: 0.994251, mean_q: 3.711282
 235341/1000000: episode: 951, duration: 5.806s, episode steps: 763, steps per second: 131, episode reward: 148.613, mean reward: 0.195 [-10.272, 0.293], mean action: -0.992 [-1.109, -0.832], mean observation: 0.209 [-0.492, 1.729], loss: 0.987586, mean_absolute_error: 0.989991, mean_q: 3.671490
 235803/1000000: episode: 952, duration: 3.481s, episode steps: 462, steps per second: 133, episode reward: 51.624, mean reward: 0.112 [-10.021, 0.179], mean action: -0.993 [-1.122, -0.880], mean observation: 0.193 [-0.138, 1.000], loss: 1.028494, mean_absolute_error: 1.000311, mean_q: 3.731313
 236563/1000000: episode: 953, duration: 5.805s, episode steps: 760, steps per second: 131, episode reward: 48.173, mean reward: 0.063 [-10.304, 0.175], mean action: -0.999 [-1.137, -0.845], mean observation: 0.187 [-0.389, 1.041], loss: 1.019753, mean_absolute_error: 1.001737, mean_q: 3.778677
 237360/1000000: episode: 954, duration: 6.007s, episode steps: 797, steps per second: 133, episode reward: 204.588, mean reward: 0.257 [-10.374, 0.388], mean action: -0.987 [-1.127, -0.839], mean observation: 0.071 [-1.985, 1.000], loss: 0.994744, mean_absolute_error: 0.999197, mean_q: 3.731310
 237776/1000000: episode: 955, duration: 3.121s, episode steps: 416, steps per second: 133, episode reward: 28.471, mean reward: 0.068 [-9.868, 0.132], mean action: -0.994 [-1.144, -0.851], mean observation: 0.119 [-1.075, 1.000], loss: 0.971915, mean_absolute_error: 0.995449, mean_q: 3.701711
 238584/1000000: episode: 956, duration: 6.096s, episode steps: 808, steps per second: 133, episode reward: 238.573, mean reward: 0.295 [-10.719, 0.536], mean action: -1.011 [-1.158, -0.811], mean observation: 0.102 [-2.468, 1.569], loss: 0.998030, mean_absolute_error: 0.999849, mean_q: 3.775430
 239228/1000000: episode: 957, duration: 4.817s, episode steps: 644, steps per second: 134, episode reward: 53.122, mean reward: 0.082 [-10.301, 0.206], mean action: -0.995 [-1.126, -0.810], mean observation: 0.063 [-1.395, 1.000], loss: 0.999664, mean_absolute_error: 1.000351, mean_q: 3.754595
 240225/1000000: episode: 958, duration: 7.476s, episode steps: 997, steps per second: 133, episode reward: 294.933, mean reward: 0.296 [-10.588, 0.497], mean action: -1.001 [-1.151, -0.808], mean observation: 0.045 [-1.806, 1.000], loss: 0.982111, mean_absolute_error: 1.002037, mean_q: 3.795047
 241161/1000000: episode: 959, duration: 7.018s, episode steps: 936, steps per second: 133, episode reward: 248.602, mean reward: 0.266 [-10.264, 0.419], mean action: -1.003 [-1.157, -0.849], mean observation: 0.167 [-0.872, 1.034], loss: 1.009414, mean_absolute_error: 1.009500, mean_q: 3.870773
 241502/1000000: episode: 960, duration: 2.536s, episode steps: 341, steps per second: 134, episode reward: 59.826, mean reward: 0.175 [-9.802, 0.206], mean action: -0.927 [-1.112, 0.925], mean observation: 0.095 [-1.116, 1.547], loss: 0.993244, mean_absolute_error: 1.007106, mean_q: 3.905145
 241752/1000000: episode: 961, duration: 2.008s, episode steps: 250, steps per second: 125, episode reward: 2.366, mean reward: 0.009 [-10.933, 0.304], mean action: -0.790 [-1.121, 1.015], mean observation: 0.184 [-5.345, 5.482], loss: 1.036512, mean_absolute_error: 1.017546, mean_q: 3.897121
 242045/1000000: episode: 962, duration: 2.233s, episode steps: 293, steps per second: 131, episode reward: -72.096, mean reward: -0.246 [-11.322, 0.187], mean action: -0.800 [-1.104, 1.056], mean observation: 0.108 [-6.208, 5.789], loss: 1.026650, mean_absolute_error: 1.016570, mean_q: 3.878587
 242312/1000000: episode: 963, duration: 2.049s, episode steps: 267, steps per second: 130, episode reward: -40.427, mean reward: -0.151 [-11.269, 0.231], mean action: -0.791 [-1.112, 1.044], mean observation: 0.167 [-5.815, 5.625], loss: 0.995482, mean_absolute_error: 1.007180, mean_q: 3.689903
 243145/1000000: episode: 964, duration: 6.418s, episode steps: 833, steps per second: 130, episode reward: 86.996, mean reward: 0.104 [-10.594, 0.320], mean action: -0.990 [-1.104, -0.833], mean observation: 0.105 [-1.804, 1.000], loss: 1.027832, mean_absolute_error: 1.019751, mean_q: 3.930783
 243491/1000000: episode: 965, duration: 2.640s, episode steps: 346, steps per second: 131, episode reward: 13.817, mean reward: 0.040 [-10.951, 0.253], mean action: -0.879 [-1.101, 0.920], mean observation: 0.199 [-4.360, 4.877], loss: 1.037979, mean_absolute_error: 1.020585, mean_q: 3.894037
 244267/1000000: episode: 966, duration: 5.764s, episode steps: 776, steps per second: 135, episode reward: 198.930, mean reward: 0.256 [-10.659, 0.633], mean action: -0.989 [-1.159, -0.492], mean observation: 0.153 [-1.472, 2.600], loss: 1.012146, mean_absolute_error: 1.016113, mean_q: 3.930174
 244883/1000000: episode: 967, duration: 4.789s, episode steps: 616, steps per second: 129, episode reward: 69.459, mean reward: 0.113 [-9.952, 0.146], mean action: -0.975 [-1.157, 0.903], mean observation: 0.077 [-0.924, 1.000], loss: 1.020692, mean_absolute_error: 1.020227, mean_q: 3.955016
 245139/1000000: episode: 968, duration: 1.972s, episode steps: 256, steps per second: 130, episode reward: -29.979, mean reward: -0.117 [-11.279, 0.276], mean action: -0.827 [-1.122, 1.027], mean observation: 0.189 [-4.932, 5.201], loss: 1.056943, mean_absolute_error: 1.032113, mean_q: 4.052775
 245628/1000000: episode: 969, duration: 4.008s, episode steps: 489, steps per second: 122, episode reward: 14.798, mean reward: 0.030 [-11.015, 0.291], mean action: -0.997 [-1.155, 0.983], mean observation: 0.173 [-1.436, 3.204], loss: 1.009641, mean_absolute_error: 1.022216, mean_q: 3.883924
 246416/1000000: episode: 970, duration: 6.111s, episode steps: 788, steps per second: 129, episode reward: 193.602, mean reward: 0.246 [-10.503, 0.462], mean action: -0.939 [-1.178, 1.038], mean observation: 0.174 [-0.860, 3.856], loss: 1.043976, mean_absolute_error: 1.026063, mean_q: 3.989848
 246665/1000000: episode: 971, duration: 1.992s, episode steps: 249, steps per second: 125, episode reward: 19.527, mean reward: 0.078 [-10.668, 0.337], mean action: -0.895 [-1.169, 1.052], mean observation: 0.238 [-2.940, 4.505], loss: 1.046339, mean_absolute_error: 1.034448, mean_q: 3.795496
 247085/1000000: episode: 972, duration: 3.336s, episode steps: 420, steps per second: 126, episode reward: 69.741, mean reward: 0.166 [-11.067, 0.435], mean action: -0.945 [-1.156, 0.196], mean observation: 0.194 [-3.151, 3.592], loss: 1.082402, mean_absolute_error: 1.044496, mean_q: 4.005429
 247928/1000000: episode: 973, duration: 6.503s, episode steps: 843, steps per second: 130, episode reward: 219.914, mean reward: 0.261 [-10.309, 0.470], mean action: -0.991 [-1.127, -0.582], mean observation: 0.077 [-1.932, 1.000], loss: 1.031203, mean_absolute_error: 1.027595, mean_q: 3.971083
 248700/1000000: episode: 974, duration: 5.977s, episode steps: 772, steps per second: 129, episode reward: 154.289, mean reward: 0.200 [-10.648, 0.540], mean action: -1.021 [-1.167, -0.862], mean observation: 0.143 [-2.006, 2.328], loss: 1.029429, mean_absolute_error: 1.027014, mean_q: 4.049257
 249229/1000000: episode: 975, duration: 4.074s, episode steps: 529, steps per second: 130, episode reward: 3.855, mean reward: 0.007 [-10.327, 0.109], mean action: -1.003 [-1.138, -0.828], mean observation: 0.197 [-0.697, 1.625], loss: 1.035987, mean_absolute_error: 1.033980, mean_q: 3.986980
 249911/1000000: episode: 976, duration: 5.083s, episode steps: 682, steps per second: 134, episode reward: -16.392, mean reward: -0.024 [-11.153, 0.242], mean action: -0.934 [-1.112, 0.982], mean observation: 0.071 [-4.038, 5.784], loss: 1.039328, mean_absolute_error: 1.032476, mean_q: 3.959151
 250111/1000000: episode: 977, duration: 1.522s, episode steps: 200, steps per second: 131, episode reward: -3.636, mean reward: -0.018 [-10.570, 0.145], mean action: -0.479 [-1.060, 1.064], mean observation: 0.155 [-6.448, 5.999], loss: 1.041171, mean_absolute_error: 1.037938, mean_q: 3.990609
 250345/1000000: episode: 978, duration: 1.756s, episode steps: 234, steps per second: 133, episode reward: 1.366, mean reward: 0.006 [-11.040, 0.351], mean action: -0.493 [-1.092, 1.079], mean observation: 0.142 [-7.947, 6.011], loss: 1.029552, mean_absolute_error: 1.033035, mean_q: 4.028557
 250587/1000000: episode: 979, duration: 1.883s, episode steps: 242, steps per second: 128, episode reward: -24.835, mean reward: -0.103 [-11.259, 0.367], mean action: -0.519 [-1.123, 1.127], mean observation: 0.118 [-8.043, 6.110], loss: 1.047854, mean_absolute_error: 1.035068, mean_q: 4.202874
 250833/1000000: episode: 980, duration: 1.836s, episode steps: 246, steps per second: 134, episode reward: -17.701, mean reward: -0.072 [-10.851, 0.238], mean action: -0.526 [-1.096, 1.049], mean observation: 0.105 [-7.970, 6.052], loss: 0.985587, mean_absolute_error: 1.018899, mean_q: 4.095965
 251039/1000000: episode: 981, duration: 1.527s, episode steps: 206, steps per second: 135, episode reward: -8.654, mean reward: -0.042 [-10.820, 0.179], mean action: -0.516 [-1.137, 1.052], mean observation: 0.196 [-6.995, 6.018], loss: 1.044737, mean_absolute_error: 1.043843, mean_q: 4.073623
 251293/1000000: episode: 982, duration: 1.908s, episode steps: 254, steps per second: 133, episode reward: -42.672, mean reward: -0.168 [-11.069, 0.263], mean action: -0.501 [-1.115, 1.089], mean observation: 0.069 [-7.995, 6.079], loss: 1.074992, mean_absolute_error: 1.052368, mean_q: 4.031420
 251557/1000000: episode: 983, duration: 2.145s, episode steps: 264, steps per second: 123, episode reward: 1.065, mean reward: 0.004 [-10.706, 0.370], mean action: -0.501 [-1.153, 1.064], mean observation: 0.058 [-7.964, 6.014], loss: 1.042309, mean_absolute_error: 1.042050, mean_q: 3.945450
 251801/1000000: episode: 984, duration: 1.946s, episode steps: 244, steps per second: 125, episode reward: 5.979, mean reward: 0.025 [-11.145, 0.469], mean action: -0.515 [-1.141, 1.081], mean observation: 0.118 [-7.973, 6.026], loss: 1.033774, mean_absolute_error: 1.040805, mean_q: 3.974853
 252056/1000000: episode: 985, duration: 1.986s, episode steps: 255, steps per second: 128, episode reward: -14.249, mean reward: -0.056 [-11.178, 0.413], mean action: -0.610 [-1.158, 1.131], mean observation: 0.119 [-7.847, 6.057], loss: 1.004225, mean_absolute_error: 1.026781, mean_q: 4.017416
 252331/1000000: episode: 986, duration: 2.271s, episode steps: 275, steps per second: 121, episode reward: 2.292, mean reward: 0.008 [-11.509, 0.587], mean action: -0.777 [-1.144, 1.017], mean observation: 0.139 [-6.114, 5.712], loss: 1.023276, mean_absolute_error: 1.031519, mean_q: 4.041197
 252579/1000000: episode: 987, duration: 1.951s, episode steps: 248, steps per second: 127, episode reward: -42.114, mean reward: -0.170 [-11.078, 0.138], mean action: -0.831 [-1.125, 0.995], mean observation: 0.209 [-3.768, 4.827], loss: 1.040883, mean_absolute_error: 1.045985, mean_q: 3.922217
 252885/1000000: episode: 988, duration: 2.434s, episode steps: 306, steps per second: 126, episode reward: -13.049, mean reward: -0.043 [-10.906, 0.254], mean action: -0.924 [-1.135, 0.756], mean observation: 0.234 [-2.471, 3.695], loss: 1.066901, mean_absolute_error: 1.039851, mean_q: 4.040598
 253362/1000000: episode: 989, duration: 3.588s, episode steps: 477, steps per second: 133, episode reward: 39.258, mean reward: 0.082 [-10.869, 0.268], mean action: -0.947 [-1.154, 0.460], mean observation: 0.188 [-3.532, 3.194], loss: 1.064100, mean_absolute_error: 1.049362, mean_q: 3.953711
 253770/1000000: episode: 990, duration: 3.024s, episode steps: 408, steps per second: 135, episode reward: 13.892, mean reward: 0.034 [-10.684, 0.200], mean action: -0.910 [-1.133, 0.601], mean observation: 0.224 [-2.607, 3.610], loss: 1.052097, mean_absolute_error: 1.037637, mean_q: 4.091779
 254362/1000000: episode: 991, duration: 4.371s, episode steps: 592, steps per second: 135, episode reward: 79.227, mean reward: 0.134 [-11.504, 0.357], mean action: -0.905 [-1.154, 0.911], mean observation: 0.209 [-5.961, 5.479], loss: 1.042939, mean_absolute_error: 1.043941, mean_q: 4.001410
 254910/1000000: episode: 992, duration: 4.165s, episode steps: 548, steps per second: 132, episode reward: 100.810, mean reward: 0.184 [-11.319, 0.434], mean action: -0.933 [-1.120, 0.506], mean observation: 0.201 [-4.350, 3.667], loss: 1.071575, mean_absolute_error: 1.051129, mean_q: 4.076877
 255532/1000000: episode: 993, duration: 4.712s, episode steps: 622, steps per second: 132, episode reward: 165.226, mean reward: 0.266 [-11.480, 0.530], mean action: -0.910 [-1.105, 0.931], mean observation: 0.172 [-5.507, 4.359], loss: 1.035215, mean_absolute_error: 1.047548, mean_q: 4.012574
 256022/1000000: episode: 994, duration: 3.785s, episode steps: 490, steps per second: 129, episode reward: 83.096, mean reward: 0.170 [-10.832, 0.313], mean action: -0.893 [-1.106, 0.947], mean observation: 0.210 [-4.777, 4.885], loss: 1.051275, mean_absolute_error: 1.049832, mean_q: 4.072641
 256851/1000000: episode: 995, duration: 6.263s, episode steps: 829, steps per second: 132, episode reward: -61.542, mean reward: -0.074 [-11.584, 0.208], mean action: -0.940 [-1.149, 0.983], mean observation: 0.125 [-6.286, 6.773], loss: 1.064603, mean_absolute_error: 1.050311, mean_q: 4.070647
 257150/1000000: episode: 996, duration: 2.278s, episode steps: 299, steps per second: 131, episode reward: -1.313, mean reward: -0.004 [-11.311, 0.463], mean action: -0.805 [-1.153, 1.046], mean observation: 0.105 [-6.122, 5.763], loss: 1.072367, mean_absolute_error: 1.056041, mean_q: 4.174397
 257428/1000000: episode: 997, duration: 2.188s, episode steps: 278, steps per second: 127, episode reward: -31.282, mean reward: -0.113 [-11.637, 0.488], mean action: -0.780 [-1.103, 1.056], mean observation: 0.148 [-5.965, 5.686], loss: 1.032375, mean_absolute_error: 1.044881, mean_q: 3.980598
 257868/1000000: episode: 998, duration: 3.303s, episode steps: 440, steps per second: 133, episode reward: -8.092, mean reward: -0.018 [-11.533, 0.321], mean action: -0.932 [-1.120, 0.986], mean observation: 0.173 [-4.266, 3.345], loss: 1.069267, mean_absolute_error: 1.057469, mean_q: 4.086793
 258450/1000000: episode: 999, duration: 4.537s, episode steps: 582, steps per second: 128, episode reward: -15.067, mean reward: -0.026 [-11.384, 0.151], mean action: -0.869 [-1.174, 1.096], mean observation: 0.219 [-6.273, 6.431], loss: 1.076465, mean_absolute_error: 1.069901, mean_q: 4.142326
 258751/1000000: episode: 1000, duration: 2.348s, episode steps: 301, steps per second: 128, episode reward: -1.085, mean reward: -0.004 [-11.515, 0.543], mean action: -0.844 [-1.092, 1.040], mean observation: 0.158 [-4.940, 5.153], loss: 1.113724, mean_absolute_error: 1.066129, mean_q: 4.009223
 259430/1000000: episode: 1001, duration: 5.248s, episode steps: 679, steps per second: 129, episode reward: 168.176, mean reward: 0.248 [-10.018, 0.305], mean action: -0.933 [-1.162, 0.963], mean observation: 0.068 [-1.168, 3.499], loss: 1.050692, mean_absolute_error: 1.053780, mean_q: 4.104799
 260282/1000000: episode: 1002, duration: 6.321s, episode steps: 852, steps per second: 135, episode reward: 263.216, mean reward: 0.309 [-11.827, 0.649], mean action: -0.914 [-1.109, 1.033], mean observation: 0.170 [-6.784, 5.864], loss: 1.069205, mean_absolute_error: 1.059190, mean_q: 4.139850
 260481/1000000: episode: 1003, duration: 1.491s, episode steps: 199, steps per second: 133, episode reward: 35.460, mean reward: 0.178 [-10.068, 0.272], mean action: -0.680 [-1.074, 0.987], mean observation: 0.185 [-3.129, 5.478], loss: 1.084645, mean_absolute_error: 1.064808, mean_q: 4.235986
 260786/1000000: episode: 1004, duration: 2.292s, episode steps: 305, steps per second: 133, episode reward: 23.540, mean reward: 0.077 [-10.394, 0.177], mean action: -0.755 [-1.155, 1.046], mean observation: 0.166 [-4.325, 4.544], loss: 1.071024, mean_absolute_error: 1.067186, mean_q: 4.308979
 260983/1000000: episode: 1005, duration: 1.595s, episode steps: 197, steps per second: 124, episode reward: -20.410, mean reward: -0.104 [-10.829, 0.112], mean action: -0.602 [-1.130, 1.012], mean observation: 0.184 [-5.803, 5.985], loss: 1.060368, mean_absolute_error: 1.057871, mean_q: 4.136527
 261615/1000000: episode: 1006, duration: 4.751s, episode steps: 632, steps per second: 133, episode reward: 144.290, mean reward: 0.228 [-11.437, 0.469], mean action: -0.889 [-1.178, 1.060], mean observation: 0.085 [-6.964, 5.571], loss: 1.087064, mean_absolute_error: 1.070760, mean_q: 4.080569
 262422/1000000: episode: 1007, duration: 6.056s, episode steps: 807, steps per second: 133, episode reward: 180.040, mean reward: 0.223 [-11.650, 0.484], mean action: -0.950 [-1.112, 0.812], mean observation: 0.115 [-4.862, 3.572], loss: 1.040405, mean_absolute_error: 1.056076, mean_q: 4.170249
 263210/1000000: episode: 1008, duration: 5.926s, episode steps: 788, steps per second: 133, episode reward: 46.941, mean reward: 0.060 [-10.302, 0.138], mean action: -0.918 [-1.132, 1.120], mean observation: 0.058 [-1.647, 3.922], loss: 1.065565, mean_absolute_error: 1.062729, mean_q: 4.147313
 263998/1000000: episode: 1009, duration: 5.943s, episode steps: 788, steps per second: 133, episode reward: 201.550, mean reward: 0.256 [-10.279, 0.386], mean action: -0.942 [-1.156, 0.980], mean observation: 0.068 [-1.609, 5.481], loss: 1.047026, mean_absolute_error: 1.059462, mean_q: 4.201590
 264810/1000000: episode: 1010, duration: 6.069s, episode steps: 812, steps per second: 134, episode reward: 150.324, mean reward: 0.185 [-10.445, 0.355], mean action: -0.927 [-1.163, 1.045], mean observation: 0.097 [-1.918, 4.129], loss: 1.027471, mean_absolute_error: 1.049854, mean_q: 4.329809
 265313/1000000: episode: 1011, duration: 3.724s, episode steps: 503, steps per second: 135, episode reward: 39.709, mean reward: 0.079 [-9.984, 0.121], mean action: -1.001 [-1.132, -0.882], mean observation: 0.103 [-0.594, 1.000], loss: 1.002660, mean_absolute_error: 1.044154, mean_q: 4.243049
 265920/1000000: episode: 1012, duration: 4.571s, episode steps: 607, steps per second: 133, episode reward: 32.214, mean reward: 0.053 [-10.646, 0.212], mean action: -0.916 [-1.124, 0.900], mean observation: 0.231 [-1.697, 4.621], loss: 1.054326, mean_absolute_error: 1.053994, mean_q: 4.388021
 266650/1000000: episode: 1013, duration: 5.545s, episode steps: 730, steps per second: 132, episode reward: 245.249, mean reward: 0.336 [-11.112, 0.604], mean action: -0.933 [-1.134, 0.726], mean observation: 0.122 [-2.943, 3.301], loss: 1.053810, mean_absolute_error: 1.055602, mean_q: 4.270170
 267467/1000000: episode: 1014, duration: 6.150s, episode steps: 817, steps per second: 133, episode reward: 101.306, mean reward: 0.124 [-10.534, 0.308], mean action: -1.010 [-1.147, -0.725], mean observation: 0.025 [-1.855, 1.000], loss: 1.022064, mean_absolute_error: 1.047739, mean_q: 4.359712
 268044/1000000: episode: 1015, duration: 4.300s, episode steps: 577, steps per second: 134, episode reward: 95.574, mean reward: 0.166 [-10.126, 0.269], mean action: -1.010 [-1.143, -0.879], mean observation: 0.201 [-0.464, 1.317], loss: 1.020732, mean_absolute_error: 1.041096, mean_q: 4.386557
 268894/1000000: episode: 1016, duration: 6.412s, episode steps: 850, steps per second: 133, episode reward: 179.301, mean reward: 0.211 [-10.494, 0.364], mean action: -0.988 [-1.149, -0.843], mean observation: 0.209 [-0.398, 2.095], loss: 1.018570, mean_absolute_error: 1.045371, mean_q: 4.373376
 269746/1000000: episode: 1017, duration: 6.357s, episode steps: 852, steps per second: 134, episode reward: 217.231, mean reward: 0.255 [-10.249, 0.360], mean action: -0.996 [-1.136, -0.856], mean observation: 0.198 [-0.503, 1.623], loss: 1.046153, mean_absolute_error: 1.049187, mean_q: 4.372561
 270535/1000000: episode: 1018, duration: 5.930s, episode steps: 789, steps per second: 133, episode reward: 259.409, mean reward: 0.329 [-10.564, 0.548], mean action: -0.942 [-1.136, 0.951], mean observation: 0.058 [-2.348, 6.406], loss: 1.021561, mean_absolute_error: 1.042596, mean_q: 4.443286
 271194/1000000: episode: 1019, duration: 4.877s, episode steps: 659, steps per second: 135, episode reward: 223.386, mean reward: 0.339 [-11.230, 0.587], mean action: -0.901 [-1.193, 1.001], mean observation: 0.088 [-6.008, 6.447], loss: 1.016651, mean_absolute_error: 1.040184, mean_q: 4.485708
 271755/1000000: episode: 1020, duration: 4.170s, episode steps: 561, steps per second: 135, episode reward: 69.497, mean reward: 0.124 [-10.386, 0.191], mean action: -0.934 [-1.115, 0.872], mean observation: 0.205 [-2.368, 4.157], loss: 1.043042, mean_absolute_error: 1.046028, mean_q: 4.503356
 272562/1000000: episode: 1021, duration: 5.972s, episode steps: 807, steps per second: 135, episode reward: 163.007, mean reward: 0.202 [-10.891, 0.465], mean action: -0.942 [-1.128, 0.998], mean observation: 0.148 [-2.376, 6.572], loss: 1.022639, mean_absolute_error: 1.044102, mean_q: 4.553718
 273302/1000000: episode: 1022, duration: 5.693s, episode steps: 740, steps per second: 130, episode reward: 109.051, mean reward: 0.147 [-11.651, 0.418], mean action: -0.943 [-1.119, 0.851], mean observation: 0.122 [-4.425, 4.433], loss: 1.017038, mean_absolute_error: 1.040782, mean_q: 4.588899
 273699/1000000: episode: 1023, duration: 2.985s, episode steps: 397, steps per second: 133, episode reward: 16.900, mean reward: 0.043 [-10.018, 0.092], mean action: -0.999 [-1.133, -0.891], mean observation: 0.155 [-0.983, 1.000], loss: 1.017257, mean_absolute_error: 1.045684, mean_q: 4.651018
 274413/1000000: episode: 1024, duration: 5.425s, episode steps: 714, steps per second: 132, episode reward: 92.375, mean reward: 0.129 [-10.689, 0.262], mean action: -1.003 [-1.154, -0.064], mean observation: 0.151 [-1.931, 2.470], loss: 1.052743, mean_absolute_error: 1.056778, mean_q: 4.603606
 275147/1000000: episode: 1025, duration: 5.481s, episode steps: 734, steps per second: 134, episode reward: 169.245, mean reward: 0.231 [-10.613, 0.379], mean action: -0.900 [-1.136, 1.134], mean observation: 0.054 [-2.009, 5.635], loss: 1.031222, mean_absolute_error: 1.052092, mean_q: 4.683090
 276194/1000000: episode: 1026, duration: 7.851s, episode steps: 1047, steps per second: 133, episode reward: 179.205, mean reward: 0.171 [-11.761, 0.502], mean action: -0.953 [-1.133, 1.033], mean observation: 0.173 [-5.340, 3.930], loss: 1.023697, mean_absolute_error: 1.048395, mean_q: 4.702918
 277374/1000000: episode: 1027, duration: 8.860s, episode steps: 1180, steps per second: 133, episode reward: 96.307, mean reward: 0.082 [-10.312, 0.334], mean action: -0.982 [-1.140, 0.478], mean observation: 0.128 [-1.908, 2.169], loss: 1.017490, mean_absolute_error: 1.046489, mean_q: 4.734121
 278186/1000000: episode: 1028, duration: 6.256s, episode steps: 812, steps per second: 130, episode reward: 85.931, mean reward: 0.106 [-10.396, 0.188], mean action: -0.966 [-1.210, 0.776], mean observation: 0.161 [-1.490, 2.237], loss: 1.031147, mean_absolute_error: 1.053382, mean_q: 4.800363
 278803/1000000: episode: 1029, duration: 4.585s, episode steps: 617, steps per second: 135, episode reward: 40.468, mean reward: 0.066 [-11.111, 0.276], mean action: -0.921 [-1.141, 0.974], mean observation: 0.241 [-3.813, 4.708], loss: 1.014914, mean_absolute_error: 1.046307, mean_q: 4.873809
 279474/1000000: episode: 1030, duration: 4.975s, episode steps: 671, steps per second: 135, episode reward: 121.371, mean reward: 0.181 [-10.801, 0.341], mean action: -0.895 [-1.093, 1.045], mean observation: 0.210 [-3.430, 5.025], loss: 1.022235, mean_absolute_error: 1.050126, mean_q: 4.906705
 280164/1000000: episode: 1031, duration: 5.307s, episode steps: 690, steps per second: 130, episode reward: 31.427, mean reward: 0.046 [-10.745, 0.165], mean action: -0.925 [-1.108, 1.028], mean observation: 0.114 [-3.125, 4.992], loss: 1.010733, mean_absolute_error: 1.046866, mean_q: 4.762003
 280817/1000000: episode: 1032, duration: 4.896s, episode steps: 653, steps per second: 133, episode reward: 89.281, mean reward: 0.137 [-10.200, 0.203], mean action: -0.913 [-1.099, 1.014], mean observation: 0.202 [-0.519, 2.835], loss: 1.027814, mean_absolute_error: 1.046235, mean_q: 4.960858
 281542/1000000: episode: 1033, duration: 5.359s, episode steps: 725, steps per second: 135, episode reward: 130.012, mean reward: 0.179 [-10.563, 0.369], mean action: -0.950 [-1.132, 0.861], mean observation: 0.244 [-0.604, 2.176], loss: 1.013520, mean_absolute_error: 1.045911, mean_q: 4.924492
 282418/1000000: episode: 1034, duration: 6.655s, episode steps: 876, steps per second: 132, episode reward: 249.228, mean reward: 0.285 [-10.272, 0.470], mean action: -1.009 [-1.155, -0.873], mean observation: 0.076 [-1.325, 1.000], loss: 1.016720, mean_absolute_error: 1.045930, mean_q: 4.987239
 283351/1000000: episode: 1035, duration: 7.226s, episode steps: 933, steps per second: 129, episode reward: 278.081, mean reward: 0.298 [-11.696, 0.541], mean action: -0.932 [-1.142, 1.034], mean observation: 0.201 [-5.604, 4.506], loss: 1.016459, mean_absolute_error: 1.050826, mean_q: 5.114701
 284154/1000000: episode: 1036, duration: 6.026s, episode steps: 803, steps per second: 133, episode reward: 349.560, mean reward: 0.435 [-11.231, 0.696], mean action: -0.900 [-1.158, 1.073], mean observation: 0.091 [-7.338, 5.912], loss: 1.017832, mean_absolute_error: 1.052724, mean_q: 5.117949
 284710/1000000: episode: 1037, duration: 4.111s, episode steps: 556, steps per second: 135, episode reward: -0.122, mean reward: -0.000 [-10.423, 0.143], mean action: -0.957 [-1.148, 1.020], mean observation: 0.107 [-1.303, 1.195], loss: 1.001285, mean_absolute_error: 1.050561, mean_q: 5.183140
 285334/1000000: episode: 1038, duration: 4.737s, episode steps: 624, steps per second: 132, episode reward: 184.529, mean reward: 0.296 [-10.299, 0.474], mean action: -0.943 [-1.130, 1.004], mean observation: 0.029 [-1.714, 1.000], loss: 1.008837, mean_absolute_error: 1.047439, mean_q: 5.065277
 286161/1000000: episode: 1039, duration: 6.113s, episode steps: 827, steps per second: 135, episode reward: 213.984, mean reward: 0.259 [-9.916, 0.312], mean action: -1.003 [-1.156, -0.830], mean observation: 0.107 [-0.688, 1.000], loss: 1.029638, mean_absolute_error: 1.051623, mean_q: 5.205043
 287054/1000000: episode: 1040, duration: 6.575s, episode steps: 893, steps per second: 136, episode reward: 270.139, mean reward: 0.303 [-10.550, 0.521], mean action: -0.997 [-1.168, -0.878], mean observation: 0.221 [-0.356, 2.026], loss: 0.983460, mean_absolute_error: 1.038664, mean_q: 5.241652
 287659/1000000: episode: 1041, duration: 4.641s, episode steps: 605, steps per second: 130, episode reward: 83.449, mean reward: 0.138 [-10.277, 0.224], mean action: -1.005 [-1.145, -0.878], mean observation: 0.048 [-2.269, 1.028], loss: 1.024372, mean_absolute_error: 1.048429, mean_q: 5.229678
 288433/1000000: episode: 1042, duration: 5.802s, episode steps: 774, steps per second: 133, episode reward: 88.375, mean reward: 0.114 [-10.634, 0.271], mean action: -1.008 [-1.134, -0.861], mean observation: 0.026 [-2.609, 1.000], loss: 0.979863, mean_absolute_error: 1.033545, mean_q: 5.278635
 288819/1000000: episode: 1043, duration: 2.865s, episode steps: 386, steps per second: 135, episode reward: 76.930, mean reward: 0.199 [-9.771, 0.229], mean action: -0.993 [-1.115, -0.883], mean observation: 0.111 [-0.460, 1.000], loss: 0.978870, mean_absolute_error: 1.035007, mean_q: 5.315385
 289420/1000000: episode: 1044, duration: 4.594s, episode steps: 601, steps per second: 131, episode reward: 137.566, mean reward: 0.229 [-9.983, 0.293], mean action: -0.990 [-1.106, -0.797], mean observation: 0.180 [-0.198, 1.569], loss: 1.007169, mean_absolute_error: 1.044630, mean_q: 5.305607
 290336/1000000: episode: 1045, duration: 7.009s, episode steps: 916, steps per second: 131, episode reward: 299.183, mean reward: 0.327 [-10.971, 0.587], mean action: -0.997 [-1.143, -0.848], mean observation: 0.233 [-0.835, 2.227], loss: 0.994966, mean_absolute_error: 1.041126, mean_q: 5.364219
 291313/1000000: episode: 1046, duration: 7.478s, episode steps: 977, steps per second: 131, episode reward: 227.634, mean reward: 0.233 [-10.562, 0.417], mean action: -0.985 [-1.145, -0.834], mean observation: 0.044 [-1.982, 1.000], loss: 0.998486, mean_absolute_error: 1.040729, mean_q: 5.375800
 292160/1000000: episode: 1047, duration: 6.311s, episode steps: 847, steps per second: 134, episode reward: 170.420, mean reward: 0.201 [-10.394, 0.340], mean action: -0.994 [-1.151, -0.872], mean observation: 0.156 [-1.140, 1.651], loss: 0.981207, mean_absolute_error: 1.035257, mean_q: 5.433252
 292754/1000000: episode: 1048, duration: 4.448s, episode steps: 594, steps per second: 134, episode reward: 82.167, mean reward: 0.138 [-10.250, 0.262], mean action: -0.998 [-1.135, -0.883], mean observation: 0.082 [-1.427, 1.000], loss: 1.005459, mean_absolute_error: 1.047298, mean_q: 5.484812
 293391/1000000: episode: 1049, duration: 4.791s, episode steps: 637, steps per second: 133, episode reward: 127.233, mean reward: 0.200 [-10.391, 0.360], mean action: -0.996 [-1.158, -0.844], mean observation: 0.212 [-0.307, 1.947], loss: 1.000181, mean_absolute_error: 1.038462, mean_q: 5.531163
 294013/1000000: episode: 1050, duration: 4.695s, episode steps: 622, steps per second: 132, episode reward: 61.242, mean reward: 0.098 [-10.120, 0.159], mean action: -1.006 [-1.120, -0.875], mean observation: 0.090 [-1.089, 1.000], loss: 0.990806, mean_absolute_error: 1.038438, mean_q: 5.480115
 294787/1000000: episode: 1051, duration: 5.996s, episode steps: 774, steps per second: 129, episode reward: 76.699, mean reward: 0.099 [-10.567, 0.224], mean action: -1.006 [-1.159, -0.866], mean observation: 0.043 [-2.523, 1.135], loss: 0.980931, mean_absolute_error: 1.031016, mean_q: 5.538092
 295589/1000000: episode: 1052, duration: 5.984s, episode steps: 802, steps per second: 134, episode reward: 326.556, mean reward: 0.407 [-10.275, 0.592], mean action: -0.992 [-1.123, -0.877], mean observation: 0.076 [-1.814, 1.000], loss: 0.969217, mean_absolute_error: 1.029814, mean_q: 5.626498
 296248/1000000: episode: 1053, duration: 4.906s, episode steps: 659, steps per second: 134, episode reward: 58.151, mean reward: 0.088 [-10.433, 0.234], mean action: -1.003 [-1.171, -0.847], mean observation: 0.024 [-1.443, 1.000], loss: 0.966222, mean_absolute_error: 1.027364, mean_q: 5.588461
 296847/1000000: episode: 1054, duration: 4.462s, episode steps: 599, steps per second: 134, episode reward: 93.262, mean reward: 0.156 [-10.532, 0.300], mean action: -1.001 [-1.125, -0.833], mean observation: 0.022 [-2.574, 1.000], loss: 0.993833, mean_absolute_error: 1.036163, mean_q: 5.521806
 297510/1000000: episode: 1055, duration: 4.953s, episode steps: 663, steps per second: 134, episode reward: 44.093, mean reward: 0.067 [-10.271, 0.161], mean action: -1.004 [-1.119, -0.891], mean observation: 0.081 [-1.363, 1.000], loss: 0.977199, mean_absolute_error: 1.028143, mean_q: 5.662594
 298268/1000000: episode: 1056, duration: 6.191s, episode steps: 758, steps per second: 122, episode reward: 280.998, mean reward: 0.371 [-10.508, 0.631], mean action: -0.992 [-1.144, -0.855], mean observation: 0.145 [-1.655, 1.478], loss: 0.985039, mean_absolute_error: 1.029811, mean_q: 5.685231
 299291/1000000: episode: 1057, duration: 7.788s, episode steps: 1023, steps per second: 131, episode reward: 354.978, mean reward: 0.347 [-10.247, 0.514], mean action: -0.995 [-1.160, -0.845], mean observation: 0.187 [-0.388, 1.627], loss: 0.965200, mean_absolute_error: 1.026027, mean_q: 5.681087
 300088/1000000: episode: 1058, duration: 6.245s, episode steps: 797, steps per second: 128, episode reward: 165.717, mean reward: 0.208 [-10.444, 0.350], mean action: -1.002 [-1.174, -0.871], mean observation: 0.191 [-0.498, 2.596], loss: 0.979655, mean_absolute_error: 1.027329, mean_q: 5.897954
 300837/1000000: episode: 1059, duration: 5.648s, episode steps: 749, steps per second: 133, episode reward: 184.350, mean reward: 0.246 [-10.527, 0.472], mean action: -1.013 [-1.147, -0.870], mean observation: 0.077 [-2.731, 1.260], loss: 0.980968, mean_absolute_error: 1.023800, mean_q: 5.781431
 301408/1000000: episode: 1060, duration: 4.516s, episode steps: 571, steps per second: 126, episode reward: 78.892, mean reward: 0.138 [-9.737, 0.263], mean action: -1.002 [-1.125, -0.859], mean observation: 0.153 [-0.469, 1.184], loss: 0.961673, mean_absolute_error: 1.016070, mean_q: 5.792723
 302154/1000000: episode: 1061, duration: 5.787s, episode steps: 746, steps per second: 129, episode reward: 188.923, mean reward: 0.253 [-10.199, 0.371], mean action: -0.990 [-1.118, -0.828], mean observation: 0.094 [-1.562, 1.000], loss: 0.962494, mean_absolute_error: 1.015260, mean_q: 5.854296
 302943/1000000: episode: 1062, duration: 5.846s, episode steps: 789, steps per second: 135, episode reward: 102.729, mean reward: 0.130 [-11.137, 0.492], mean action: -0.816 [-1.120, 1.102], mean observation: 0.095 [-8.033, 6.271], loss: 0.957650, mean_absolute_error: 1.013843, mean_q: 5.906206
 303185/1000000: episode: 1063, duration: 1.798s, episode steps: 242, steps per second: 135, episode reward: -0.658, mean reward: -0.003 [-11.178, 0.438], mean action: -0.504 [-1.094, 1.073], mean observation: 0.117 [-7.967, 6.046], loss: 0.941042, mean_absolute_error: 1.012475, mean_q: 5.915674
 303450/1000000: episode: 1064, duration: 1.957s, episode steps: 265, steps per second: 135, episode reward: -18.896, mean reward: -0.071 [-10.705, 0.281], mean action: -0.523 [-1.160, 1.045], mean observation: 0.069 [-7.959, 6.022], loss: 0.972402, mean_absolute_error: 1.022212, mean_q: 5.835216
 303704/1000000: episode: 1065, duration: 1.883s, episode steps: 254, steps per second: 135, episode reward: 3.313, mean reward: 0.013 [-10.821, 0.360], mean action: -0.482 [-1.089, 1.070], mean observation: 0.078 [-7.954, 6.018], loss: 0.993066, mean_absolute_error: 1.017268, mean_q: 5.991854
 303983/1000000: episode: 1066, duration: 2.173s, episode steps: 279, steps per second: 128, episode reward: -72.722, mean reward: -0.261 [-10.956, 0.288], mean action: -0.481 [-1.077, 1.085], mean observation: 0.045 [-7.951, 6.036], loss: 0.955778, mean_absolute_error: 1.015212, mean_q: 5.904486
 304213/1000000: episode: 1067, duration: 1.839s, episode steps: 230, steps per second: 125, episode reward: -42.728, mean reward: -0.186 [-11.013, 0.086], mean action: -0.508 [-1.146, 1.039], mean observation: 0.103 [-7.934, 6.026], loss: 0.993866, mean_absolute_error: 1.023153, mean_q: 6.024391
 304489/1000000: episode: 1068, duration: 2.051s, episode steps: 276, steps per second: 135, episode reward: -41.811, mean reward: -0.151 [-10.797, 0.333], mean action: -0.508 [-1.144, 1.082], mean observation: 0.037 [-8.005, 6.053], loss: 0.954177, mean_absolute_error: 1.022435, mean_q: 5.951673
 304732/1000000: episode: 1069, duration: 1.891s, episode steps: 243, steps per second: 129, episode reward: 4.685, mean reward: 0.019 [-11.185, 0.465], mean action: -0.508 [-1.104, 1.022], mean observation: 0.129 [-7.931, 6.017], loss: 0.980065, mean_absolute_error: 1.018472, mean_q: 5.875489
 304980/1000000: episode: 1070, duration: 2.021s, episode steps: 248, steps per second: 123, episode reward: -61.699, mean reward: -0.249 [-11.473, 0.225], mean action: -0.507 [-1.131, 1.052], mean observation: 0.148 [-7.823, 5.946], loss: 0.940194, mean_absolute_error: 1.009426, mean_q: 5.876886
 305258/1000000: episode: 1071, duration: 2.070s, episode steps: 278, steps per second: 134, episode reward: -53.205, mean reward: -0.191 [-11.061, 0.411], mean action: -0.529 [-1.124, 1.027], mean observation: 0.085 [-7.919, 6.010], loss: 0.988835, mean_absolute_error: 1.020138, mean_q: 5.987535
 305516/1000000: episode: 1072, duration: 1.939s, episode steps: 258, steps per second: 133, episode reward: 27.767, mean reward: 0.108 [-10.908, 0.545], mean action: -0.503 [-1.132, 1.080], mean observation: 0.085 [-7.917, 6.013], loss: 0.909289, mean_absolute_error: 1.009127, mean_q: 5.927608
 305801/1000000: episode: 1073, duration: 2.198s, episode steps: 285, steps per second: 130, episode reward: -3.568, mean reward: -0.013 [-10.429, 0.366], mean action: -0.486 [-1.093, 1.098], mean observation: 0.049 [-7.923, 5.978], loss: 0.942067, mean_absolute_error: 1.016077, mean_q: 5.890659
 306050/1000000: episode: 1074, duration: 1.881s, episode steps: 249, steps per second: 132, episode reward: -16.450, mean reward: -0.066 [-11.033, 0.325], mean action: -0.491 [-1.081, 1.087], mean observation: 0.113 [-7.956, 6.035], loss: 0.992182, mean_absolute_error: 1.016161, mean_q: 6.048270
 306297/1000000: episode: 1075, duration: 1.970s, episode steps: 247, steps per second: 125, episode reward: 25.049, mean reward: 0.101 [-10.907, 0.505], mean action: -0.506 [-1.169, 1.149], mean observation: 0.093 [-8.024, 6.084], loss: 0.933052, mean_absolute_error: 0.996047, mean_q: 6.054490
 306579/1000000: episode: 1076, duration: 2.191s, episode steps: 282, steps per second: 129, episode reward: 31.016, mean reward: 0.110 [-10.776, 0.702], mean action: -0.497 [-1.086, 1.034], mean observation: 0.051 [-7.903, 5.952], loss: 0.967434, mean_absolute_error: 1.008266, mean_q: 5.955991
 306843/1000000: episode: 1077, duration: 1.971s, episode steps: 264, steps per second: 134, episode reward: 2.591, mean reward: 0.010 [-10.628, 0.360], mean action: -0.500 [-1.110, 1.104], mean observation: 0.057 [-8.011, 6.041], loss: 0.995684, mean_absolute_error: 1.025058, mean_q: 5.998438
 307115/1000000: episode: 1078, duration: 2.031s, episode steps: 272, steps per second: 134, episode reward: -2.925, mean reward: -0.011 [-10.683, 0.416], mean action: -0.511 [-1.122, 1.075], mean observation: 0.043 [-7.983, 6.024], loss: 0.941208, mean_absolute_error: 1.008966, mean_q: 5.983117
 307364/1000000: episode: 1079, duration: 1.842s, episode steps: 249, steps per second: 135, episode reward: -51.211, mean reward: -0.206 [-11.306, 0.255], mean action: -0.507 [-1.139, 1.119], mean observation: 0.103 [-7.981, 6.034], loss: 1.005254, mean_absolute_error: 1.017273, mean_q: 5.990544
 307625/1000000: episode: 1080, duration: 2.004s, episode steps: 261, steps per second: 130, episode reward: -39.936, mean reward: -0.153 [-10.797, 0.192], mean action: -0.486 [-1.112, 1.087], mean observation: 0.044 [-7.992, 6.034], loss: 0.958803, mean_absolute_error: 1.012459, mean_q: 6.019855
 307891/1000000: episode: 1081, duration: 2.014s, episode steps: 266, steps per second: 132, episode reward: -47.956, mean reward: -0.180 [-11.028, 0.319], mean action: -0.500 [-1.116, 1.037], mean observation: 0.063 [-8.015, 6.067], loss: 0.993632, mean_absolute_error: 1.017212, mean_q: 5.816266
 308133/1000000: episode: 1082, duration: 1.941s, episode steps: 242, steps per second: 125, episode reward: 8.623, mean reward: 0.036 [-10.812, 0.356], mean action: -0.509 [-1.126, 1.099], mean observation: 0.102 [-8.044, 6.096], loss: 0.950073, mean_absolute_error: 1.016918, mean_q: 6.146026
 308390/1000000: episode: 1083, duration: 1.905s, episode steps: 257, steps per second: 135, episode reward: -49.575, mean reward: -0.193 [-11.281, 0.350], mean action: -0.508 [-1.147, 1.104], mean observation: 0.111 [-8.011, 6.085], loss: 0.972312, mean_absolute_error: 1.017331, mean_q: 6.073054
 308587/1000000: episode: 1084, duration: 1.463s, episode steps: 197, steps per second: 135, episode reward: -13.429, mean reward: -0.068 [-10.659, 0.103], mean action: -0.491 [-1.124, 1.119], mean observation: 0.186 [-6.578, 6.089], loss: 0.958298, mean_absolute_error: 0.992817, mean_q: 6.073980
 308834/1000000: episode: 1085, duration: 1.826s, episode steps: 247, steps per second: 135, episode reward: 16.206, mean reward: 0.066 [-10.975, 0.460], mean action: -0.478 [-1.065, 1.100], mean observation: 0.101 [-7.979, 6.027], loss: 0.908974, mean_absolute_error: 0.997888, mean_q: 6.019376
 309111/1000000: episode: 1086, duration: 2.077s, episode steps: 277, steps per second: 133, episode reward: 15.940, mean reward: 0.058 [-10.812, 0.649], mean action: -0.513 [-1.116, 1.056], mean observation: 0.057 [-7.956, 6.029], loss: 0.948158, mean_absolute_error: 1.002082, mean_q: 5.851208
 309363/1000000: episode: 1087, duration: 1.870s, episode steps: 252, steps per second: 135, episode reward: 7.610, mean reward: 0.030 [-10.811, 0.362], mean action: -0.494 [-1.133, 1.075], mean observation: 0.094 [-7.958, 6.000], loss: 0.906211, mean_absolute_error: 0.995345, mean_q: 5.958625
 309588/1000000: episode: 1088, duration: 1.696s, episode steps: 225, steps per second: 133, episode reward: 14.177, mean reward: 0.063 [-11.063, 0.419], mean action: -0.479 [-1.056, 1.088], mean observation: 0.150 [-7.927, 6.044], loss: 0.968255, mean_absolute_error: 1.011075, mean_q: 6.115372
 309786/1000000: episode: 1089, duration: 1.470s, episode steps: 198, steps per second: 135, episode reward: -34.042, mean reward: -0.172 [-10.953, 0.055], mean action: -0.508 [-1.149, 1.048], mean observation: 0.180 [-6.483, 6.039], loss: 0.947866, mean_absolute_error: 0.998695, mean_q: 6.150210
 310053/1000000: episode: 1090, duration: 1.984s, episode steps: 267, steps per second: 135, episode reward: 24.822, mean reward: 0.093 [-10.798, 0.567], mean action: -0.470 [-1.085, 1.104], mean observation: 0.077 [-7.969, 6.037], loss: 0.945416, mean_absolute_error: 0.998885, mean_q: 6.060783
 310250/1000000: episode: 1091, duration: 1.462s, episode steps: 197, steps per second: 135, episode reward: 0.113, mean reward: 0.001 [-10.569, 0.166], mean action: -0.529 [-1.139, 1.029], mean observation: 0.162 [-6.031, 5.973], loss: 0.923486, mean_absolute_error: 0.999821, mean_q: 5.995898
 310376/1000000: episode: 1092, duration: 0.938s, episode steps: 126, steps per second: 134, episode reward: 5.663, mean reward: 0.045 [-9.964, 0.140], mean action: -0.502 [-1.182, 1.075], mean observation: 0.185 [-1.219, 4.533], loss: 0.981025, mean_absolute_error: 1.006099, mean_q: 6.098475
 310629/1000000: episode: 1093, duration: 1.881s, episode steps: 253, steps per second: 135, episode reward: 25.532, mean reward: 0.101 [-10.850, 0.503], mean action: -0.505 [-1.102, 1.087], mean observation: 0.079 [-8.021, 6.067], loss: 0.946894, mean_absolute_error: 0.998919, mean_q: 6.114851
 310789/1000000: episode: 1094, duration: 1.199s, episode steps: 160, steps per second: 133, episode reward: 13.110, mean reward: 0.082 [-10.119, 0.188], mean action: -0.513 [-1.137, 1.018], mean observation: 0.174 [-2.936, 5.908], loss: 0.963154, mean_absolute_error: 0.991633, mean_q: 6.029626
 310957/1000000: episode: 1095, duration: 1.266s, episode steps: 168, steps per second: 133, episode reward: 6.683, mean reward: 0.040 [-10.169, 0.139], mean action: -0.502 [-1.115, 1.031], mean observation: 0.181 [-3.624, 6.026], loss: 0.945514, mean_absolute_error: 1.001117, mean_q: 5.971116
 311158/1000000: episode: 1096, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: 21.924, mean reward: 0.109 [-10.716, 0.345], mean action: -0.484 [-1.105, 1.091], mean observation: 0.193 [-6.778, 6.051], loss: 0.962825, mean_absolute_error: 1.017602, mean_q: 6.100949
 311410/1000000: episode: 1097, duration: 1.861s, episode steps: 252, steps per second: 135, episode reward: 6.340, mean reward: 0.025 [-10.876, 0.365], mean action: -0.514 [-1.097, 1.063], mean observation: 0.108 [-7.912, 5.950], loss: 0.931664, mean_absolute_error: 1.001118, mean_q: 6.006848
 311618/1000000: episode: 1098, duration: 1.558s, episode steps: 208, steps per second: 134, episode reward: 17.254, mean reward: 0.083 [-10.629, 0.282], mean action: -0.509 [-1.127, 1.049], mean observation: 0.158 [-7.134, 6.017], loss: 0.939527, mean_absolute_error: 0.991450, mean_q: 6.045204
 311866/1000000: episode: 1099, duration: 1.848s, episode steps: 248, steps per second: 134, episode reward: -79.919, mean reward: -0.322 [-11.433, 0.155], mean action: -0.504 [-1.144, 1.088], mean observation: 0.108 [-8.018, 6.078], loss: 0.962236, mean_absolute_error: 1.000101, mean_q: 6.144867
 312086/1000000: episode: 1100, duration: 1.632s, episode steps: 220, steps per second: 135, episode reward: 4.790, mean reward: 0.022 [-10.988, 0.322], mean action: -0.514 [-1.155, 1.072], mean observation: 0.180 [-7.813, 6.032], loss: 0.950093, mean_absolute_error: 1.006340, mean_q: 6.046699
 312328/1000000: episode: 1101, duration: 1.807s, episode steps: 242, steps per second: 134, episode reward: 13.523, mean reward: 0.056 [-11.094, 0.486], mean action: -0.506 [-1.147, 1.097], mean observation: 0.135 [-8.013, 6.055], loss: 0.926157, mean_absolute_error: 0.999427, mean_q: 6.106302
 312491/1000000: episode: 1102, duration: 1.217s, episode steps: 163, steps per second: 134, episode reward: -9.426, mean reward: -0.058 [-10.372, 0.073], mean action: -0.517 [-1.153, 1.100], mean observation: 0.220 [-3.407, 6.045], loss: 0.901808, mean_absolute_error: 0.982288, mean_q: 6.068046
 312681/1000000: episode: 1103, duration: 1.405s, episode steps: 190, steps per second: 135, episode reward: -11.197, mean reward: -0.059 [-10.773, 0.153], mean action: -0.483 [-1.102, 1.099], mean observation: 0.194 [-5.957, 6.087], loss: 0.919353, mean_absolute_error: 1.001127, mean_q: 5.863442
 312866/1000000: episode: 1104, duration: 1.368s, episode steps: 185, steps per second: 135, episode reward: 0.942, mean reward: 0.005 [-10.372, 0.130], mean action: -0.487 [-1.093, 1.079], mean observation: 0.161 [-5.050, 6.003], loss: 0.999881, mean_absolute_error: 1.003946, mean_q: 6.043724
 313064/1000000: episode: 1105, duration: 1.465s, episode steps: 198, steps per second: 135, episode reward: 2.913, mean reward: 0.015 [-10.635, 0.202], mean action: -0.523 [-1.110, 1.071], mean observation: 0.186 [-6.651, 6.075], loss: 0.891865, mean_absolute_error: 0.984147, mean_q: 6.139873
 313250/1000000: episode: 1106, duration: 1.384s, episode steps: 186, steps per second: 134, episode reward: 29.775, mean reward: 0.160 [-10.441, 0.340], mean action: -0.499 [-1.112, 1.101], mean observation: 0.194 [-5.464, 6.056], loss: 0.895451, mean_absolute_error: 0.994157, mean_q: 6.007471
 313469/1000000: episode: 1107, duration: 1.633s, episode steps: 219, steps per second: 134, episode reward: -11.794, mean reward: -0.054 [-10.952, 0.213], mean action: -0.494 [-1.141, 1.078], mean observation: 0.140 [-7.786, 6.043], loss: 0.949564, mean_absolute_error: 1.006181, mean_q: 6.155702
 313726/1000000: episode: 1108, duration: 1.904s, episode steps: 257, steps per second: 135, episode reward: -42.175, mean reward: -0.164 [-11.010, 0.251], mean action: -0.493 [-1.094, 1.120], mean observation: 0.059 [-7.990, 6.049], loss: 0.980676, mean_absolute_error: 1.009676, mean_q: 6.110584
 314011/1000000: episode: 1109, duration: 2.110s, episode steps: 285, steps per second: 135, episode reward: -45.132, mean reward: -0.158 [-10.841, 0.452], mean action: -0.500 [-1.082, 1.130], mean observation: 0.067 [-7.974, 6.036], loss: 0.938374, mean_absolute_error: 0.991885, mean_q: 6.026546
 314289/1000000: episode: 1110, duration: 2.054s, episode steps: 278, steps per second: 135, episode reward: -6.334, mean reward: -0.023 [-10.684, 0.456], mean action: -0.524 [-1.140, 1.054], mean observation: 0.064 [-7.950, 6.011], loss: 0.874073, mean_absolute_error: 0.979268, mean_q: 5.965101
 314529/1000000: episode: 1111, duration: 1.779s, episode steps: 240, steps per second: 135, episode reward: 9.731, mean reward: 0.041 [-10.986, 0.408], mean action: -0.493 [-1.118, 1.124], mean observation: 0.111 [-8.000, 6.062], loss: 0.916309, mean_absolute_error: 0.992897, mean_q: 6.100326
 314775/1000000: episode: 1112, duration: 1.819s, episode steps: 246, steps per second: 135, episode reward: -71.209, mean reward: -0.289 [-11.378, 0.165], mean action: -0.479 [-1.156, 1.128], mean observation: 0.105 [-7.997, 6.069], loss: 0.912055, mean_absolute_error: 0.987342, mean_q: 6.050473
 315058/1000000: episode: 1113, duration: 2.101s, episode steps: 283, steps per second: 135, episode reward: -50.281, mean reward: -0.178 [-10.913, 0.417], mean action: -0.518 [-1.106, 1.048], mean observation: 0.053 [-7.953, 6.023], loss: 0.907910, mean_absolute_error: 0.982757, mean_q: 6.098716
 315215/1000000: episode: 1114, duration: 1.164s, episode steps: 157, steps per second: 135, episode reward: 21.057, mean reward: 0.134 [-10.064, 0.240], mean action: -0.482 [-1.094, 1.113], mean observation: 0.195 [-3.066, 6.000], loss: 0.897941, mean_absolute_error: 0.976966, mean_q: 6.093024
 315411/1000000: episode: 1115, duration: 1.455s, episode steps: 196, steps per second: 135, episode reward: 12.401, mean reward: 0.063 [-10.470, 0.219], mean action: -0.492 [-1.063, 1.010], mean observation: 0.164 [-5.988, 5.986], loss: 0.935241, mean_absolute_error: 0.986579, mean_q: 5.977061
 315620/1000000: episode: 1116, duration: 1.552s, episode steps: 209, steps per second: 135, episode reward: -32.624, mean reward: -0.156 [-10.825, 0.039], mean action: -0.519 [-1.143, 1.040], mean observation: 0.140 [-7.321, 6.053], loss: 0.873460, mean_absolute_error: 0.977751, mean_q: 6.090010
 315877/1000000: episode: 1117, duration: 1.909s, episode steps: 257, steps per second: 135, episode reward: -38.176, mean reward: -0.149 [-11.270, 0.384], mean action: -0.486 [-1.053, 1.108], mean observation: 0.108 [-7.979, 6.035], loss: 0.906027, mean_absolute_error: 0.988580, mean_q: 6.115336
 316106/1000000: episode: 1118, duration: 1.708s, episode steps: 229, steps per second: 134, episode reward: -10.256, mean reward: -0.045 [-10.914, 0.231], mean action: -0.493 [-1.113, 1.058], mean observation: 0.120 [-8.015, 6.037], loss: 1.013018, mean_absolute_error: 1.004249, mean_q: 6.082967
 316369/1000000: episode: 1119, duration: 1.948s, episode steps: 263, steps per second: 135, episode reward: 3.354, mean reward: 0.013 [-10.977, 0.565], mean action: -0.510 [-1.135, 1.082], mean observation: 0.077 [-8.008, 6.087], loss: 0.959528, mean_absolute_error: 0.996801, mean_q: 6.115416
 316605/1000000: episode: 1120, duration: 1.747s, episode steps: 236, steps per second: 135, episode reward: -2.078, mean reward: -0.009 [-11.036, 0.351], mean action: -0.489 [-1.109, 1.132], mean observation: 0.106 [-8.048, 6.069], loss: 0.908506, mean_absolute_error: 0.987472, mean_q: 6.059364
 316886/1000000: episode: 1121, duration: 2.078s, episode steps: 281, steps per second: 135, episode reward: 33.781, mean reward: 0.120 [-10.566, 0.575], mean action: -0.485 [-1.097, 1.094], mean observation: 0.048 [-7.948, 5.978], loss: 0.930794, mean_absolute_error: 0.992310, mean_q: 6.111365
 317133/1000000: episode: 1122, duration: 1.969s, episode steps: 247, steps per second: 125, episode reward: -27.752, mean reward: -0.112 [-10.900, 0.214], mean action: -0.521 [-1.122, 1.060], mean observation: 0.107 [-7.966, 6.052], loss: 0.931239, mean_absolute_error: 0.990424, mean_q: 6.092970
 317333/1000000: episode: 1123, duration: 1.499s, episode steps: 200, steps per second: 133, episode reward: -7.402, mean reward: -0.037 [-10.769, 0.172], mean action: -0.504 [-1.171, 1.025], mean observation: 0.213 [-6.394, 6.002], loss: 1.012552, mean_absolute_error: 0.999400, mean_q: 6.230919
 317615/1000000: episode: 1124, duration: 2.101s, episode steps: 282, steps per second: 134, episode reward: -22.666, mean reward: -0.080 [-10.576, 0.361], mean action: -0.507 [-1.104, 1.083], mean observation: 0.018 [-8.015, 6.039], loss: 0.915063, mean_absolute_error: 0.983432, mean_q: 6.043633
 317857/1000000: episode: 1125, duration: 1.868s, episode steps: 242, steps per second: 130, episode reward: -44.356, mean reward: -0.183 [-11.264, 0.229], mean action: -0.504 [-1.120, 1.135], mean observation: 0.145 [-7.997, 6.034], loss: 0.967774, mean_absolute_error: 1.006064, mean_q: 6.044493
 318119/1000000: episode: 1126, duration: 2.042s, episode steps: 262, steps per second: 128, episode reward: 32.972, mean reward: 0.126 [-10.775, 0.563], mean action: -0.494 [-1.093, 1.089], mean observation: 0.064 [-8.003, 6.059], loss: 0.952950, mean_absolute_error: 0.987130, mean_q: 6.240112
 318361/1000000: episode: 1127, duration: 1.868s, episode steps: 242, steps per second: 130, episode reward: 16.228, mean reward: 0.067 [-11.047, 0.495], mean action: -0.506 [-1.097, 1.105], mean observation: 0.112 [-8.051, 6.083], loss: 0.912356, mean_absolute_error: 0.984211, mean_q: 6.260843
 318512/1000000: episode: 1128, duration: 1.212s, episode steps: 151, steps per second: 125, episode reward: 15.412, mean reward: 0.102 [-9.993, 0.192], mean action: -0.502 [-1.075, 1.045], mean observation: 0.183 [-2.465, 5.727], loss: 0.945524, mean_absolute_error: 0.997395, mean_q: 6.280241
 318762/1000000: episode: 1129, duration: 1.971s, episode steps: 250, steps per second: 127, episode reward: 21.655, mean reward: 0.087 [-11.032, 0.527], mean action: -0.477 [-1.087, 1.072], mean observation: 0.101 [-7.927, 6.019], loss: 0.919229, mean_absolute_error: 0.981371, mean_q: 6.118465
 319020/1000000: episode: 1130, duration: 1.995s, episode steps: 258, steps per second: 129, episode reward: 18.312, mean reward: 0.071 [-10.952, 0.528], mean action: -0.506 [-1.109, 1.077], mean observation: 0.099 [-7.973, 6.016], loss: 0.920862, mean_absolute_error: 0.984917, mean_q: 6.142053
 319278/1000000: episode: 1131, duration: 1.938s, episode steps: 258, steps per second: 133, episode reward: -4.640, mean reward: -0.018 [-10.852, 0.374], mean action: -0.496 [-1.109, 1.104], mean observation: 0.091 [-7.968, 6.041], loss: 0.891822, mean_absolute_error: 0.961813, mean_q: 6.248602
 319475/1000000: episode: 1132, duration: 1.469s, episode steps: 197, steps per second: 134, episode reward: -31.606, mean reward: -0.160 [-10.944, 0.068], mean action: -0.506 [-1.112, 1.082], mean observation: 0.187 [-6.300, 6.025], loss: 0.902723, mean_absolute_error: 0.973234, mean_q: 6.214990
 319720/1000000: episode: 1133, duration: 1.888s, episode steps: 245, steps per second: 130, episode reward: -35.618, mean reward: -0.145 [-10.961, 0.166], mean action: -0.509 [-1.128, 1.021], mean observation: 0.112 [-7.939, 6.004], loss: 0.900081, mean_absolute_error: 0.984917, mean_q: 6.166592
 319962/1000000: episode: 1134, duration: 1.936s, episode steps: 242, steps per second: 125, episode reward: 6.411, mean reward: 0.026 [-11.161, 0.456], mean action: -0.499 [-1.069, 1.063], mean observation: 0.137 [-7.940, 5.986], loss: 0.892817, mean_absolute_error: 0.971690, mean_q: 6.187304
 320200/1000000: episode: 1135, duration: 1.886s, episode steps: 238, steps per second: 126, episode reward: 8.656, mean reward: 0.036 [-11.021, 0.404], mean action: -0.503 [-1.168, 1.034], mean observation: 0.132 [-7.979, 6.045], loss: 0.887781, mean_absolute_error: 0.983917, mean_q: 6.196770
 320434/1000000: episode: 1136, duration: 1.756s, episode steps: 234, steps per second: 133, episode reward: -55.637, mean reward: -0.238 [-11.350, 0.155], mean action: -0.514 [-1.125, 1.069], mean observation: 0.165 [-7.999, 6.068], loss: 0.919512, mean_absolute_error: 0.980558, mean_q: 6.186942
 320628/1000000: episode: 1137, duration: 1.450s, episode steps: 194, steps per second: 134, episode reward: -6.935, mean reward: -0.036 [-10.603, 0.131], mean action: -0.483 [-1.091, 1.091], mean observation: 0.186 [-6.260, 6.071], loss: 0.857385, mean_absolute_error: 0.966570, mean_q: 6.181786
 320797/1000000: episode: 1138, duration: 1.258s, episode steps: 169, steps per second: 134, episode reward: 25.626, mean reward: 0.152 [-10.166, 0.273], mean action: -0.502 [-1.125, 1.094], mean observation: 0.193 [-3.980, 6.092], loss: 0.898998, mean_absolute_error: 0.974670, mean_q: 6.187961
 320980/1000000: episode: 1139, duration: 1.365s, episode steps: 183, steps per second: 134, episode reward: 27.206, mean reward: 0.149 [-10.377, 0.311], mean action: -0.494 [-1.101, 1.092], mean observation: 0.186 [-5.085, 6.033], loss: 0.907894, mean_absolute_error: 0.980379, mean_q: 6.180117
 321149/1000000: episode: 1140, duration: 1.256s, episode steps: 169, steps per second: 135, episode reward: 34.913, mean reward: 0.207 [-10.087, 0.325], mean action: -0.510 [-1.105, 1.022], mean observation: 0.188 [-3.637, 5.994], loss: 0.873591, mean_absolute_error: 0.968125, mean_q: 6.263443
 321435/1000000: episode: 1141, duration: 2.251s, episode steps: 286, steps per second: 127, episode reward: -32.388, mean reward: -0.113 [-10.693, 0.375], mean action: -0.505 [-1.124, 1.076], mean observation: 0.055 [-7.876, 6.006], loss: 0.912244, mean_absolute_error: 0.969085, mean_q: 6.288889
 321718/1000000: episode: 1142, duration: 2.172s, episode steps: 283, steps per second: 130, episode reward: -68.092, mean reward: -0.241 [-10.839, 0.297], mean action: -0.496 [-1.110, 1.066], mean observation: 0.019 [-7.984, 6.052], loss: 0.881903, mean_absolute_error: 0.969001, mean_q: 6.257844
 321976/1000000: episode: 1143, duration: 1.927s, episode steps: 258, steps per second: 134, episode reward: -46.099, mean reward: -0.179 [-11.252, 0.348], mean action: -0.493 [-1.072, 1.065], mean observation: 0.112 [-7.983, 6.024], loss: 0.881049, mean_absolute_error: 0.971026, mean_q: 6.300546
 322240/1000000: episode: 1144, duration: 1.971s, episode steps: 264, steps per second: 134, episode reward: -86.671, mean reward: -0.328 [-11.271, 0.205], mean action: -0.512 [-1.105, 1.067], mean observation: 0.075 [-7.964, 6.052], loss: 0.889415, mean_absolute_error: 0.968310, mean_q: 6.162653
 322500/1000000: episode: 1145, duration: 1.925s, episode steps: 260, steps per second: 135, episode reward: -6.248, mean reward: -0.024 [-11.119, 0.517], mean action: -0.509 [-1.100, 1.037], mean observation: 0.098 [-7.977, 6.034], loss: 0.906234, mean_absolute_error: 0.970541, mean_q: 6.252860
 322760/1000000: episode: 1146, duration: 1.924s, episode steps: 260, steps per second: 135, episode reward: -10.498, mean reward: -0.040 [-10.820, 0.365], mean action: -0.494 [-1.128, 1.103], mean observation: 0.083 [-8.023, 6.057], loss: 0.840054, mean_absolute_error: 0.953982, mean_q: 6.380349
 323041/1000000: episode: 1147, duration: 2.076s, episode steps: 281, steps per second: 135, episode reward: 0.695, mean reward: 0.002 [-10.364, 0.378], mean action: -0.488 [-1.143, 1.071], mean observation: 0.016 [-8.030, 6.077], loss: 0.897098, mean_absolute_error: 0.975514, mean_q: 6.328243
 323288/1000000: episode: 1148, duration: 1.833s, episode steps: 247, steps per second: 135, episode reward: 9.788, mean reward: 0.040 [-10.990, 0.450], mean action: -0.489 [-1.103, 1.104], mean observation: 0.114 [-7.977, 6.074], loss: 0.908008, mean_absolute_error: 0.964709, mean_q: 6.285057
 323570/1000000: episode: 1149, duration: 2.085s, episode steps: 282, steps per second: 135, episode reward: -22.062, mean reward: -0.078 [-10.683, 0.416], mean action: -0.501 [-1.143, 1.057], mean observation: 0.061 [-7.952, 6.025], loss: 0.872313, mean_absolute_error: 0.967102, mean_q: 6.340944
 323845/1000000: episode: 1150, duration: 2.074s, episode steps: 275, steps per second: 133, episode reward: -35.612, mean reward: -0.129 [-10.536, 0.253], mean action: -0.499 [-1.143, 1.083], mean observation: 0.020 [-8.036, 6.100], loss: 0.866391, mean_absolute_error: 0.963617, mean_q: 6.252485
 324060/1000000: episode: 1151, duration: 1.601s, episode steps: 215, steps per second: 134, episode reward: 14.333, mean reward: 0.067 [-10.952, 0.368], mean action: -0.509 [-1.150, 1.081], mean observation: 0.177 [-7.582, 6.027], loss: 0.951791, mean_absolute_error: 0.971073, mean_q: 6.403514
 324264/1000000: episode: 1152, duration: 1.510s, episode steps: 204, steps per second: 135, episode reward: -10.870, mean reward: -0.053 [-10.758, 0.149], mean action: -0.485 [-1.097, 1.113], mean observation: 0.185 [-7.037, 6.057], loss: 0.918198, mean_absolute_error: 0.966847, mean_q: 6.382908
 324488/1000000: episode: 1153, duration: 1.661s, episode steps: 224, steps per second: 135, episode reward: -20.273, mean reward: -0.091 [-10.981, 0.187], mean action: -0.492 [-1.120, 1.161], mean observation: 0.124 [-8.060, 6.090], loss: 0.880701, mean_absolute_error: 0.958651, mean_q: 6.207021
 324774/1000000: episode: 1154, duration: 2.119s, episode steps: 286, steps per second: 135, episode reward: -7.146, mean reward: -0.025 [-10.329, 0.312], mean action: -0.508 [-1.119, 1.061], mean observation: 0.023 [-7.921, 6.001], loss: 0.901131, mean_absolute_error: 0.969536, mean_q: 6.363038
 324934/1000000: episode: 1155, duration: 1.192s, episode steps: 160, steps per second: 134, episode reward: 15.884, mean reward: 0.099 [-10.145, 0.216], mean action: -0.486 [-1.059, 1.046], mean observation: 0.209 [-3.053, 5.958], loss: 0.905093, mean_absolute_error: 0.980143, mean_q: 6.295043
 325193/1000000: episode: 1156, duration: 1.912s, episode steps: 259, steps per second: 135, episode reward: 20.729, mean reward: 0.080 [-10.779, 0.472], mean action: -0.501 [-1.088, 1.057], mean observation: 0.069 [-7.969, 6.034], loss: 0.870706, mean_absolute_error: 0.952088, mean_q: 6.315701
 325463/1000000: episode: 1157, duration: 2.000s, episode steps: 270, steps per second: 135, episode reward: -59.857, mean reward: -0.222 [-11.080, 0.320], mean action: -0.513 [-1.121, 1.084], mean observation: 0.093 [-8.007, 6.047], loss: 0.880075, mean_absolute_error: 0.966836, mean_q: 6.289369
 325727/1000000: episode: 1158, duration: 1.950s, episode steps: 264, steps per second: 135, episode reward: 5.925, mean reward: 0.022 [-10.839, 0.432], mean action: -0.490 [-1.073, 1.021], mean observation: 0.068 [-7.921, 5.988], loss: 0.915833, mean_absolute_error: 0.978534, mean_q: 6.250495
 326000/1000000: episode: 1159, duration: 2.028s, episode steps: 273, steps per second: 135, episode reward: -1.961, mean reward: -0.007 [-10.877, 0.492], mean action: -0.501 [-1.130, 1.067], mean observation: 0.049 [-7.931, 5.988], loss: 0.873903, mean_absolute_error: 0.959311, mean_q: 6.428633
 326221/1000000: episode: 1160, duration: 1.663s, episode steps: 221, steps per second: 133, episode reward: -67.028, mean reward: -0.303 [-11.398, 0.043], mean action: -0.478 [-1.094, 1.126], mean observation: 0.151 [-7.960, 6.066], loss: 0.842805, mean_absolute_error: 0.953498, mean_q: 6.397314
 326413/1000000: episode: 1161, duration: 1.436s, episode steps: 192, steps per second: 134, episode reward: 10.440, mean reward: 0.054 [-10.448, 0.205], mean action: -0.486 [-1.107, 1.082], mean observation: 0.166 [-5.757, 6.000], loss: 0.881790, mean_absolute_error: 0.965918, mean_q: 6.541987
 326678/1000000: episode: 1162, duration: 1.957s, episode steps: 265, steps per second: 135, episode reward: 29.303, mean reward: 0.111 [-10.919, 0.644], mean action: -0.514 [-1.150, 1.119], mean observation: 0.084 [-8.019, 6.036], loss: 0.885059, mean_absolute_error: 0.960622, mean_q: 6.250346
 326884/1000000: episode: 1163, duration: 1.545s, episode steps: 206, steps per second: 133, episode reward: -14.354, mean reward: -0.070 [-11.034, 0.216], mean action: -0.499 [-1.123, 1.133], mean observation: 0.191 [-7.371, 6.116], loss: 0.959690, mean_absolute_error: 0.973203, mean_q: 6.412129
 327057/1000000: episode: 1164, duration: 1.302s, episode steps: 173, steps per second: 133, episode reward: 1.234, mean reward: 0.007 [-10.428, 0.159], mean action: -0.488 [-1.057, 1.063], mean observation: 0.212 [-4.112, 6.026], loss: 0.923242, mean_absolute_error: 0.965539, mean_q: 6.426641
 327249/1000000: episode: 1165, duration: 1.425s, episode steps: 192, steps per second: 135, episode reward: 14.276, mean reward: 0.074 [-10.581, 0.267], mean action: -0.513 [-1.154, 1.029], mean observation: 0.192 [-5.544, 5.971], loss: 0.865696, mean_absolute_error: 0.953107, mean_q: 6.479912
 327497/1000000: episode: 1166, duration: 1.905s, episode steps: 248, steps per second: 130, episode reward: -1.886, mean reward: -0.008 [-10.970, 0.373], mean action: -0.516 [-1.123, 1.101], mean observation: 0.117 [-8.020, 6.048], loss: 0.849243, mean_absolute_error: 0.953199, mean_q: 6.355067
 327747/1000000: episode: 1167, duration: 2.013s, episode steps: 250, steps per second: 124, episode reward: -63.126, mean reward: -0.253 [-11.351, 0.219], mean action: -0.502 [-1.156, 1.060], mean observation: 0.131 [-8.007, 6.052], loss: 0.892436, mean_absolute_error: 0.967225, mean_q: 6.403161
 327996/1000000: episode: 1168, duration: 1.840s, episode steps: 249, steps per second: 135, episode reward: -34.983, mean reward: -0.140 [-10.955, 0.187], mean action: -0.493 [-1.083, 1.030], mean observation: 0.078 [-7.892, 5.992], loss: 0.878498, mean_absolute_error: 0.958054, mean_q: 6.371945
 328248/1000000: episode: 1169, duration: 1.994s, episode steps: 252, steps per second: 126, episode reward: -19.979, mean reward: -0.079 [-10.938, 0.282], mean action: -0.506 [-1.067, 1.045], mean observation: 0.108 [-7.969, 6.038], loss: 0.919795, mean_absolute_error: 0.968418, mean_q: 6.397481
 328533/1000000: episode: 1170, duration: 2.224s, episode steps: 285, steps per second: 128, episode reward: -75.244, mean reward: -0.264 [-10.842, 0.292], mean action: -0.501 [-1.120, 1.078], mean observation: 0.025 [-7.975, 6.072], loss: 0.892604, mean_absolute_error: 0.952539, mean_q: 6.477841
 328682/1000000: episode: 1171, duration: 1.109s, episode steps: 149, steps per second: 134, episode reward: 9.500, mean reward: 0.064 [-10.042, 0.157], mean action: -0.525 [-1.150, 1.066], mean observation: 0.158 [-2.466, 5.753], loss: 0.908567, mean_absolute_error: 0.958483, mean_q: 6.364855
 328930/1000000: episode: 1172, duration: 1.853s, episode steps: 248, steps per second: 134, episode reward: -49.421, mean reward: -0.199 [-11.363, 0.279], mean action: -0.490 [-1.081, 1.043], mean observation: 0.131 [-7.943, 6.006], loss: 0.839757, mean_absolute_error: 0.945275, mean_q: 6.430600
 329217/1000000: episode: 1173, duration: 2.121s, episode steps: 287, steps per second: 135, episode reward: 0.453, mean reward: 0.002 [-10.709, 0.630], mean action: -0.517 [-1.130, 1.075], mean observation: 0.044 [-7.955, 6.011], loss: 0.875169, mean_absolute_error: 0.953785, mean_q: 6.495817
 329500/1000000: episode: 1174, duration: 2.118s, episode steps: 283, steps per second: 134, episode reward: -27.161, mean reward: -0.096 [-10.361, 0.256], mean action: -0.492 [-1.104, 1.080], mean observation: 0.011 [-7.940, 6.035], loss: 0.876785, mean_absolute_error: 0.959745, mean_q: 6.412097
 329703/1000000: episode: 1175, duration: 1.682s, episode steps: 203, steps per second: 121, episode reward: 33.104, mean reward: 0.163 [-10.606, 0.377], mean action: -0.499 [-1.100, 1.111], mean observation: 0.182 [-6.718, 6.014], loss: 0.875802, mean_absolute_error: 0.944071, mean_q: 6.428732
 329918/1000000: episode: 1176, duration: 1.811s, episode steps: 215, steps per second: 119, episode reward: 21.820, mean reward: 0.101 [-10.818, 0.366], mean action: -0.523 [-1.116, 1.034], mean observation: 0.154 [-7.685, 6.054], loss: 0.879229, mean_absolute_error: 0.959937, mean_q: 6.470821
 330185/1000000: episode: 1177, duration: 2.157s, episode steps: 267, steps per second: 124, episode reward: -52.619, mean reward: -0.197 [-11.154, 0.316], mean action: -0.496 [-1.091, 1.088], mean observation: 0.064 [-7.925, 5.998], loss: 0.925733, mean_absolute_error: 0.960482, mean_q: 6.462008
 330462/1000000: episode: 1178, duration: 2.282s, episode steps: 277, steps per second: 121, episode reward: 41.736, mean reward: 0.151 [-10.505, 0.632], mean action: -0.504 [-1.121, 1.128], mean observation: 0.038 [-8.091, 6.104], loss: 0.919409, mean_absolute_error: 0.959009, mean_q: 6.439546
 330734/1000000: episode: 1179, duration: 2.145s, episode steps: 272, steps per second: 127, episode reward: -39.492, mean reward: -0.145 [-10.743, 0.267], mean action: -0.496 [-1.073, 1.051], mean observation: 0.033 [-7.958, 6.035], loss: 0.925182, mean_absolute_error: 0.960404, mean_q: 6.561885
 330905/1000000: episode: 1180, duration: 1.412s, episode steps: 171, steps per second: 121, episode reward: 14.579, mean reward: 0.085 [-10.204, 0.199], mean action: -0.495 [-1.086, 1.118], mean observation: 0.192 [-4.014, 6.057], loss: 0.874555, mean_absolute_error: 0.962885, mean_q: 6.488063
 331097/1000000: episode: 1181, duration: 1.512s, episode steps: 192, steps per second: 127, episode reward: 7.980, mean reward: 0.042 [-10.539, 0.211], mean action: -0.487 [-1.140, 1.124], mean observation: 0.162 [-5.866, 6.035], loss: 0.868548, mean_absolute_error: 0.947245, mean_q: 6.391768
 331355/1000000: episode: 1182, duration: 2.044s, episode steps: 258, steps per second: 126, episode reward: -18.681, mean reward: -0.072 [-10.867, 0.323], mean action: -0.522 [-1.128, 1.053], mean observation: 0.066 [-8.026, 6.056], loss: 0.882222, mean_absolute_error: 0.963382, mean_q: 6.567394
 331616/1000000: episode: 1183, duration: 1.983s, episode steps: 261, steps per second: 132, episode reward: 19.308, mean reward: 0.074 [-10.649, 0.452], mean action: -0.493 [-1.128, 1.108], mean observation: 0.057 [-8.062, 6.113], loss: 0.874950, mean_absolute_error: 0.955431, mean_q: 6.445051
 331849/1000000: episode: 1184, duration: 1.793s, episode steps: 233, steps per second: 130, episode reward: -18.651, mean reward: -0.080 [-10.908, 0.201], mean action: -0.494 [-1.097, 1.122], mean observation: 0.103 [-7.991, 6.068], loss: 0.901767, mean_absolute_error: 0.960879, mean_q: 6.512695
 332108/1000000: episode: 1185, duration: 1.994s, episode steps: 259, steps per second: 130, episode reward: 16.068, mean reward: 0.062 [-10.976, 0.572], mean action: -0.510 [-1.119, 1.069], mean observation: 0.096 [-7.996, 6.058], loss: 0.871030, mean_absolute_error: 0.947084, mean_q: 6.613069
 332351/1000000: episode: 1186, duration: 1.831s, episode steps: 243, steps per second: 133, episode reward: -27.491, mean reward: -0.113 [-11.026, 0.242], mean action: -0.494 [-1.106, 1.096], mean observation: 0.089 [-8.007, 6.068], loss: 0.886402, mean_absolute_error: 0.960583, mean_q: 6.446669
 332627/1000000: episode: 1187, duration: 2.081s, episode steps: 276, steps per second: 133, episode reward: -0.063, mean reward: -0.000 [-10.634, 0.483], mean action: -0.494 [-1.146, 1.123], mean observation: 0.030 [-8.013, 6.077], loss: 0.883206, mean_absolute_error: 0.949563, mean_q: 6.582016
 332806/1000000: episode: 1188, duration: 1.355s, episode steps: 179, steps per second: 132, episode reward: 14.902, mean reward: 0.083 [-10.370, 0.232], mean action: -0.487 [-1.079, 1.072], mean observation: 0.210 [-4.605, 6.023], loss: 0.871053, mean_absolute_error: 0.944506, mean_q: 6.561831
 333037/1000000: episode: 1189, duration: 1.800s, episode steps: 231, steps per second: 128, episode reward: 25.932, mean reward: 0.112 [-11.009, 0.467], mean action: -0.490 [-1.122, 1.088], mean observation: 0.153 [-7.955, 5.999], loss: 0.895544, mean_absolute_error: 0.954912, mean_q: 6.641889
 333293/1000000: episode: 1190, duration: 1.948s, episode steps: 256, steps per second: 131, episode reward: 17.834, mean reward: 0.070 [-10.790, 0.428], mean action: -0.467 [-1.095, 1.099], mean observation: 0.084 [-7.969, 6.015], loss: 0.893530, mean_absolute_error: 0.949381, mean_q: 6.528531
 333546/1000000: episode: 1191, duration: 1.957s, episode steps: 253, steps per second: 129, episode reward: -37.262, mean reward: -0.147 [-11.097, 0.252], mean action: -0.508 [-1.120, 1.054], mean observation: 0.080 [-7.938, 5.994], loss: 0.914776, mean_absolute_error: 0.950978, mean_q: 6.628220
 333828/1000000: episode: 1192, duration: 2.149s, episode steps: 282, steps per second: 131, episode reward: -39.471, mean reward: -0.140 [-10.733, 0.378], mean action: -0.492 [-1.093, 1.132], mean observation: 0.033 [-7.958, 6.056], loss: 0.871894, mean_absolute_error: 0.954555, mean_q: 6.584847
 334081/1000000: episode: 1193, duration: 1.964s, episode steps: 253, steps per second: 129, episode reward: -40.331, mean reward: -0.159 [-11.258, 0.363], mean action: -0.512 [-1.167, 1.101], mean observation: 0.094 [-8.041, 6.089], loss: 0.855697, mean_absolute_error: 0.941318, mean_q: 6.535882
 334341/1000000: episode: 1194, duration: 1.982s, episode steps: 260, steps per second: 131, episode reward: 17.572, mean reward: 0.068 [-11.000, 0.559], mean action: -0.513 [-1.136, 1.046], mean observation: 0.099 [-7.912, 5.988], loss: 0.838730, mean_absolute_error: 0.935683, mean_q: 6.537036
 334601/1000000: episode: 1195, duration: 2.009s, episode steps: 260, steps per second: 129, episode reward: -77.208, mean reward: -0.297 [-11.292, 0.222], mean action: -0.504 [-1.158, 1.138], mean observation: 0.079 [-7.988, 6.058], loss: 0.884560, mean_absolute_error: 0.952485, mean_q: 6.537321
 334869/1000000: episode: 1196, duration: 2.090s, episode steps: 268, steps per second: 128, episode reward: -75.696, mean reward: -0.282 [-11.205, 0.270], mean action: -0.482 [-1.091, 1.072], mean observation: 0.108 [-7.962, 6.030], loss: 0.882101, mean_absolute_error: 0.940457, mean_q: 6.607797
 335107/1000000: episode: 1197, duration: 1.821s, episode steps: 238, steps per second: 131, episode reward: -72.974, mean reward: -0.307 [-11.468, 0.114], mean action: -0.529 [-1.203, 1.031], mean observation: 0.130 [-7.934, 6.029], loss: 0.870352, mean_absolute_error: 0.953675, mean_q: 6.610490
 335327/1000000: episode: 1198, duration: 1.690s, episode steps: 220, steps per second: 130, episode reward: 13.357, mean reward: 0.061 [-11.018, 0.387], mean action: -0.492 [-1.086, 1.053], mean observation: 0.158 [-7.768, 6.011], loss: 0.921934, mean_absolute_error: 0.957285, mean_q: 6.555274
 335562/1000000: episode: 1199, duration: 1.762s, episode steps: 235, steps per second: 133, episode reward: -14.849, mean reward: -0.063 [-11.065, 0.276], mean action: -0.492 [-1.101, 1.135], mean observation: 0.146 [-8.017, 6.043], loss: 0.882621, mean_absolute_error: 0.947953, mean_q: 6.700219
 335789/1000000: episode: 1200, duration: 1.708s, episode steps: 227, steps per second: 133, episode reward: -20.289, mean reward: -0.089 [-11.031, 0.200], mean action: -0.514 [-1.126, 1.027], mean observation: 0.164 [-7.877, 5.989], loss: 0.906565, mean_absolute_error: 0.946414, mean_q: 6.625720
 336036/1000000: episode: 1201, duration: 1.887s, episode steps: 247, steps per second: 131, episode reward: 27.517, mean reward: 0.111 [-10.859, 0.497], mean action: -0.508 [-1.194, 1.112], mean observation: 0.091 [-8.024, 6.094], loss: 0.854712, mean_absolute_error: 0.939108, mean_q: 6.582718
 336299/1000000: episode: 1202, duration: 2.042s, episode steps: 263, steps per second: 129, episode reward: -62.703, mean reward: -0.238 [-11.179, 0.309], mean action: -0.481 [-1.101, 1.150], mean observation: 0.103 [-8.030, 6.094], loss: 0.823514, mean_absolute_error: 0.927952, mean_q: 6.581271
 336557/1000000: episode: 1203, duration: 2.027s, episode steps: 258, steps per second: 127, episode reward: -46.716, mean reward: -0.181 [-11.307, 0.354], mean action: -0.510 [-1.116, 1.057], mean observation: 0.112 [-7.947, 6.002], loss: 0.850767, mean_absolute_error: 0.947711, mean_q: 6.575016
 336783/1000000: episode: 1204, duration: 1.719s, episode steps: 226, steps per second: 131, episode reward: -30.820, mean reward: -0.136 [-11.224, 0.204], mean action: -0.501 [-1.134, 1.043], mean observation: 0.178 [-7.850, 5.991], loss: 0.826179, mean_absolute_error: 0.928728, mean_q: 6.680322
 337007/1000000: episode: 1205, duration: 1.827s, episode steps: 224, steps per second: 123, episode reward: 18.866, mean reward: 0.084 [-10.883, 0.372], mean action: -0.509 [-1.134, 1.044], mean observation: 0.156 [-7.918, 6.034], loss: 0.877738, mean_absolute_error: 0.935671, mean_q: 6.529096
 337264/1000000: episode: 1206, duration: 1.970s, episode steps: 257, steps per second: 130, episode reward: -52.512, mean reward: -0.204 [-11.268, 0.329], mean action: -0.501 [-1.095, 1.104], mean observation: 0.103 [-8.027, 6.067], loss: 0.938348, mean_absolute_error: 0.948074, mean_q: 6.628718
 337494/1000000: episode: 1207, duration: 1.716s, episode steps: 230, steps per second: 134, episode reward: 16.587, mean reward: 0.072 [-10.944, 0.383], mean action: -0.498 [-1.122, 1.050], mean observation: 0.156 [-7.935, 5.977], loss: 0.849595, mean_absolute_error: 0.930247, mean_q: 6.703040
 337714/1000000: episode: 1208, duration: 1.648s, episode steps: 220, steps per second: 134, episode reward: -20.368, mean reward: -0.093 [-10.892, 0.152], mean action: -0.504 [-1.084, 1.135], mean observation: 0.155 [-7.813, 6.050], loss: 0.886583, mean_absolute_error: 0.945017, mean_q: 6.515924
 337958/1000000: episode: 1209, duration: 1.860s, episode steps: 244, steps per second: 131, episode reward: -69.774, mean reward: -0.286 [-11.425, 0.179], mean action: -0.518 [-1.138, 1.095], mean observation: 0.123 [-8.023, 6.074], loss: 0.869362, mean_absolute_error: 0.938091, mean_q: 6.639667
 338240/1000000: episode: 1210, duration: 2.173s, episode steps: 282, steps per second: 130, episode reward: -39.332, mean reward: -0.139 [-10.904, 0.453], mean action: -0.501 [-1.113, 1.053], mean observation: 0.051 [-7.924, 6.001], loss: 0.906005, mean_absolute_error: 0.947494, mean_q: 6.612188
 338493/1000000: episode: 1211, duration: 1.932s, episode steps: 253, steps per second: 131, episode reward: -20.623, mean reward: -0.082 [-11.215, 0.428], mean action: -0.477 [-1.092, 1.106], mean observation: 0.118 [-7.974, 6.049], loss: 0.891767, mean_absolute_error: 0.940024, mean_q: 6.697620
 338663/1000000: episode: 1212, duration: 1.277s, episode steps: 170, steps per second: 133, episode reward: -30.524, mean reward: -0.180 [-10.547, -0.041], mean action: -0.491 [-1.051, 1.045], mean observation: 0.191 [-3.753, 5.998], loss: 0.931194, mean_absolute_error: 0.936986, mean_q: 6.584164
 338915/1000000: episode: 1213, duration: 1.941s, episode steps: 252, steps per second: 130, episode reward: -9.592, mean reward: -0.038 [-10.971, 0.365], mean action: -0.510 [-1.166, 1.088], mean observation: 0.080 [-7.990, 6.048], loss: 0.847636, mean_absolute_error: 0.937081, mean_q: 6.755958
 339160/1000000: episode: 1214, duration: 1.847s, episode steps: 245, steps per second: 133, episode reward: -5.188, mean reward: -0.021 [-11.177, 0.417], mean action: -0.498 [-1.109, 1.106], mean observation: 0.113 [-7.970, 6.020], loss: 0.883481, mean_absolute_error: 0.936284, mean_q: 6.657384
 339394/1000000: episode: 1215, duration: 1.823s, episode steps: 234, steps per second: 128, episode reward: 11.835, mean reward: 0.051 [-10.871, 0.360], mean action: -0.512 [-1.177, 1.076], mean observation: 0.116 [-8.032, 6.087], loss: 0.842936, mean_absolute_error: 0.936920, mean_q: 6.788351
 339650/1000000: episode: 1216, duration: 1.944s, episode steps: 256, steps per second: 132, episode reward: -45.972, mean reward: -0.180 [-11.134, 0.263], mean action: -0.488 [-1.071, 1.109], mean observation: 0.075 [-7.976, 6.038], loss: 0.826170, mean_absolute_error: 0.923571, mean_q: 6.626837
 339894/1000000: episode: 1217, duration: 1.820s, episode steps: 244, steps per second: 134, episode reward: 16.973, mean reward: 0.070 [-10.881, 0.400], mean action: -0.512 [-1.115, 1.051], mean observation: 0.112 [-7.968, 6.009], loss: 0.837646, mean_absolute_error: 0.934478, mean_q: 6.764174
 340098/1000000: episode: 1218, duration: 1.565s, episode steps: 204, steps per second: 130, episode reward: 28.310, mean reward: 0.139 [-10.576, 0.335], mean action: -0.481 [-1.079, 1.048], mean observation: 0.176 [-6.627, 5.984], loss: 0.885974, mean_absolute_error: 0.940501, mean_q: 6.824788
 340349/1000000: episode: 1219, duration: 1.942s, episode steps: 251, steps per second: 129, episode reward: -34.581, mean reward: -0.138 [-11.284, 0.363], mean action: -0.505 [-1.127, 1.106], mean observation: 0.120 [-7.988, 6.056], loss: 0.871633, mean_absolute_error: 0.934760, mean_q: 6.740176
 340608/1000000: episode: 1220, duration: 2.007s, episode steps: 259, steps per second: 129, episode reward: -60.469, mean reward: -0.233 [-11.288, 0.290], mean action: -0.503 [-1.100, 1.051], mean observation: 0.112 [-7.984, 6.055], loss: 0.893721, mean_absolute_error: 0.930237, mean_q: 6.752036
 340862/1000000: episode: 1221, duration: 1.946s, episode steps: 254, steps per second: 130, episode reward: -37.842, mean reward: -0.149 [-11.325, 0.353], mean action: -0.517 [-1.145, 1.098], mean observation: 0.123 [-7.896, 5.971], loss: 0.846692, mean_absolute_error: 0.931790, mean_q: 6.714729
 341058/1000000: episode: 1222, duration: 1.471s, episode steps: 196, steps per second: 133, episode reward: 1.844, mean reward: 0.009 [-10.532, 0.170], mean action: -0.492 [-1.112, 1.072], mean observation: 0.162 [-6.475, 6.077], loss: 0.827967, mean_absolute_error: 0.927154, mean_q: 6.787394
 341233/1000000: episode: 1223, duration: 1.347s, episode steps: 175, steps per second: 130, episode reward: 21.501, mean reward: 0.123 [-10.267, 0.259], mean action: -0.485 [-1.099, 1.086], mean observation: 0.208 [-4.235, 6.022], loss: 0.913533, mean_absolute_error: 0.937954, mean_q: 6.773635
 341459/1000000: episode: 1224, duration: 1.774s, episode steps: 226, steps per second: 127, episode reward: -24.844, mean reward: -0.110 [-11.107, 0.203], mean action: -0.485 [-1.084, 1.103], mean observation: 0.131 [-7.904, 6.027], loss: 0.911829, mean_absolute_error: 0.932594, mean_q: 6.631229
 341732/1000000: episode: 1225, duration: 2.046s, episode steps: 273, steps per second: 133, episode reward: -27.361, mean reward: -0.100 [-10.673, 0.308], mean action: -0.500 [-1.121, 1.108], mean observation: 0.028 [-8.006, 6.058], loss: 0.878159, mean_absolute_error: 0.926653, mean_q: 6.864432
 342018/1000000: episode: 1226, duration: 2.171s, episode steps: 286, steps per second: 132, episode reward: 1.233, mean reward: 0.004 [-10.797, 0.634], mean action: -0.518 [-1.111, 1.061], mean observation: 0.054 [-7.897, 5.983], loss: 0.849805, mean_absolute_error: 0.925184, mean_q: 6.738234
 342203/1000000: episode: 1227, duration: 1.423s, episode steps: 185, steps per second: 130, episode reward: -1.095, mean reward: -0.006 [-10.624, 0.182], mean action: -0.491 [-1.093, 1.094], mean observation: 0.202 [-5.329, 6.057], loss: 0.872419, mean_absolute_error: 0.921984, mean_q: 6.875258
 342419/1000000: episode: 1228, duration: 1.657s, episode steps: 216, steps per second: 130, episode reward: -16.169, mean reward: -0.075 [-10.960, 0.190], mean action: -0.502 [-1.146, 1.141], mean observation: 0.175 [-7.824, 6.080], loss: 0.883303, mean_absolute_error: 0.933387, mean_q: 6.863800
 342658/1000000: episode: 1229, duration: 1.841s, episode steps: 239, steps per second: 130, episode reward: -22.100, mean reward: -0.092 [-11.292, 0.334], mean action: -0.511 [-1.132, 1.166], mean observation: 0.150 [-8.008, 6.003], loss: 0.872819, mean_absolute_error: 0.925794, mean_q: 6.843950
 342904/1000000: episode: 1230, duration: 1.908s, episode steps: 246, steps per second: 129, episode reward: -78.374, mean reward: -0.319 [-11.420, 0.150], mean action: -0.497 [-1.133, 1.105], mean observation: 0.147 [-8.025, 6.090], loss: 0.839314, mean_absolute_error: 0.916757, mean_q: 6.797027
 343079/1000000: episode: 1231, duration: 1.333s, episode steps: 175, steps per second: 131, episode reward: 15.809, mean reward: 0.090 [-10.245, 0.212], mean action: -0.515 [-1.088, 1.100], mean observation: 0.188 [-4.453, 6.052], loss: 0.872971, mean_absolute_error: 0.930095, mean_q: 6.874780
 343353/1000000: episode: 1232, duration: 2.156s, episode steps: 274, steps per second: 127, episode reward: -3.156, mean reward: -0.012 [-10.765, 0.479], mean action: -0.529 [-1.165, 1.059], mean observation: 0.047 [-7.954, 6.023], loss: 0.842358, mean_absolute_error: 0.916844, mean_q: 6.732725
 343570/1000000: episode: 1233, duration: 1.688s, episode steps: 217, steps per second: 129, episode reward: -0.873, mean reward: -0.004 [-10.898, 0.259], mean action: -0.506 [-1.100, 1.138], mean observation: 0.165 [-7.805, 6.076], loss: 0.870020, mean_absolute_error: 0.925574, mean_q: 6.908883
 343850/1000000: episode: 1234, duration: 2.162s, episode steps: 280, steps per second: 130, episode reward: -30.571, mean reward: -0.109 [-10.784, 0.461], mean action: -0.503 [-1.110, 1.095], mean observation: 0.036 [-8.017, 6.088], loss: 0.862406, mean_absolute_error: 0.921079, mean_q: 6.898824
 344098/1000000: episode: 1235, duration: 1.862s, episode steps: 248, steps per second: 133, episode reward: -28.712, mean reward: -0.116 [-11.021, 0.241], mean action: -0.480 [-1.100, 1.123], mean observation: 0.120 [-7.947, 6.030], loss: 0.886905, mean_absolute_error: 0.924856, mean_q: 6.830259
 344365/1000000: episode: 1236, duration: 2.018s, episode steps: 267, steps per second: 132, episode reward: -40.764, mean reward: -0.153 [-11.146, 0.424], mean action: -0.500 [-1.122, 1.053], mean observation: 0.081 [-7.964, 6.043], loss: 0.878370, mean_absolute_error: 0.934448, mean_q: 7.004525
 344607/1000000: episode: 1237, duration: 1.806s, episode steps: 242, steps per second: 134, episode reward: -61.694, mean reward: -0.255 [-11.435, 0.186], mean action: -0.522 [-1.091, 1.015], mean observation: 0.153 [-7.945, 6.023], loss: 0.856041, mean_absolute_error: 0.929086, mean_q: 6.955235
 344841/1000000: episode: 1238, duration: 1.743s, episode steps: 234, steps per second: 134, episode reward: -20.790, mean reward: -0.089 [-11.166, 0.259], mean action: -0.501 [-1.100, 1.024], mean observation: 0.160 [-7.910, 5.959], loss: 0.928919, mean_absolute_error: 0.938685, mean_q: 6.984053
 345123/1000000: episode: 1239, duration: 2.100s, episode steps: 282, steps per second: 134, episode reward: 13.219, mean reward: 0.047 [-10.726, 0.670], mean action: -0.476 [-1.158, 1.150], mean observation: 0.056 [-8.012, 6.046], loss: 0.872042, mean_absolute_error: 0.924471, mean_q: 6.871576
 345341/1000000: episode: 1240, duration: 1.639s, episode steps: 218, steps per second: 133, episode reward: -20.116, mean reward: -0.092 [-10.910, 0.148], mean action: -0.508 [-1.133, 1.050], mean observation: 0.168 [-7.724, 6.031], loss: 0.864255, mean_absolute_error: 0.926306, mean_q: 7.086409
 345609/1000000: episode: 1241, duration: 2.037s, episode steps: 268, steps per second: 132, episode reward: 27.637, mean reward: 0.103 [-10.845, 0.627], mean action: -0.485 [-1.059, 1.085], mean observation: 0.065 [-7.960, 6.042], loss: 0.838076, mean_absolute_error: 0.929400, mean_q: 6.969235
 345891/1000000: episode: 1242, duration: 2.130s, episode steps: 282, steps per second: 132, episode reward: -7.260, mean reward: -0.026 [-10.359, 0.327], mean action: -0.508 [-1.130, 1.136], mean observation: 0.025 [-8.006, 6.029], loss: 0.819053, mean_absolute_error: 0.927643, mean_q: 7.015796
 346170/1000000: episode: 1243, duration: 2.132s, episode steps: 279, steps per second: 131, episode reward: 9.419, mean reward: 0.034 [-10.520, 0.467], mean action: -0.523 [-1.133, 1.043], mean observation: 0.048 [-7.986, 6.057], loss: 0.826260, mean_absolute_error: 0.928213, mean_q: 6.955762
 346356/1000000: episode: 1244, duration: 1.413s, episode steps: 186, steps per second: 132, episode reward: 2.223, mean reward: 0.012 [-10.615, 0.201], mean action: -0.497 [-1.077, 1.078], mean observation: 0.196 [-5.403, 6.055], loss: 0.888555, mean_absolute_error: 0.940989, mean_q: 6.977658
 346557/1000000: episode: 1245, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: 34.565, mean reward: 0.172 [-10.558, 0.376], mean action: -0.493 [-1.104, 1.046], mean observation: 0.181 [-6.402, 5.980], loss: 0.848300, mean_absolute_error: 0.935420, mean_q: 6.967432
 346822/1000000: episode: 1246, duration: 2.053s, episode steps: 265, steps per second: 129, episode reward: 5.106, mean reward: 0.019 [-10.827, 0.527], mean action: -0.504 [-1.135, 1.098], mean observation: 0.083 [-8.060, 6.109], loss: 0.876263, mean_absolute_error: 0.937140, mean_q: 7.166300
 347016/1000000: episode: 1247, duration: 1.492s, episode steps: 194, steps per second: 130, episode reward: 14.893, mean reward: 0.077 [-10.641, 0.283], mean action: -0.504 [-1.098, 1.108], mean observation: 0.174 [-6.427, 6.102], loss: 0.842752, mean_absolute_error: 0.922611, mean_q: 6.994050
 347284/1000000: episode: 1248, duration: 2.024s, episode steps: 268, steps per second: 132, episode reward: -9.114, mean reward: -0.034 [-10.656, 0.381], mean action: -0.493 [-1.135, 1.179], mean observation: 0.069 [-8.065, 6.090], loss: 0.860934, mean_absolute_error: 0.935066, mean_q: 6.986402
 347543/1000000: episode: 1249, duration: 1.974s, episode steps: 259, steps per second: 131, episode reward: 17.843, mean reward: 0.069 [-10.765, 0.434], mean action: -0.506 [-1.092, 1.041], mean observation: 0.074 [-7.970, 6.021], loss: 0.892312, mean_absolute_error: 0.939346, mean_q: 7.098462
 347805/1000000: episode: 1250, duration: 1.994s, episode steps: 262, steps per second: 131, episode reward: -52.802, mean reward: -0.202 [-11.257, 0.345], mean action: -0.511 [-1.105, 1.065], mean observation: 0.113 [-7.928, 6.040], loss: 0.930400, mean_absolute_error: 0.965561, mean_q: 7.031142
 348052/1000000: episode: 1251, duration: 1.850s, episode steps: 247, steps per second: 134, episode reward: 16.443, mean reward: 0.067 [-10.866, 0.440], mean action: -0.501 [-1.143, 1.052], mean observation: 0.098 [-8.030, 6.100], loss: 0.871098, mean_absolute_error: 0.937302, mean_q: 7.051186
 348279/1000000: episode: 1252, duration: 1.734s, episode steps: 227, steps per second: 131, episode reward: -9.859, mean reward: -0.043 [-11.095, 0.294], mean action: -0.478 [-1.138, 1.095], mean observation: 0.131 [-7.979, 6.050], loss: 0.876963, mean_absolute_error: 0.939090, mean_q: 7.110803
 348555/1000000: episode: 1253, duration: 2.141s, episode steps: 276, steps per second: 129, episode reward: 41.119, mean reward: 0.149 [-10.638, 0.617], mean action: -0.496 [-1.091, 1.060], mean observation: 0.050 [-7.976, 6.011], loss: 0.865963, mean_absolute_error: 0.939872, mean_q: 7.096825
 348837/1000000: episode: 1254, duration: 2.126s, episode steps: 282, steps per second: 133, episode reward: -75.058, mean reward: -0.266 [-10.924, 0.301], mean action: -0.515 [-1.144, 1.051], mean observation: 0.079 [-7.975, 6.046], loss: 0.879598, mean_absolute_error: 0.950134, mean_q: 7.111657
 349081/1000000: episode: 1255, duration: 1.832s, episode steps: 244, steps per second: 133, episode reward: 10.883, mean reward: 0.045 [-11.106, 0.510], mean action: -0.505 [-1.124, 1.058], mean observation: 0.109 [-8.035, 6.089], loss: 0.857059, mean_absolute_error: 0.937265, mean_q: 7.150187
 349341/1000000: episode: 1256, duration: 2.053s, episode steps: 260, steps per second: 127, episode reward: 23.995, mean reward: 0.092 [-10.785, 0.521], mean action: -0.503 [-1.108, 1.164], mean observation: 0.069 [-7.966, 6.067], loss: 0.846264, mean_absolute_error: 0.938794, mean_q: 7.187125
 349569/1000000: episode: 1257, duration: 1.783s, episode steps: 228, steps per second: 128, episode reward: 11.653, mean reward: 0.051 [-10.852, 0.340], mean action: -0.489 [-1.087, 1.114], mean observation: 0.135 [-8.022, 6.094], loss: 0.879017, mean_absolute_error: 0.939562, mean_q: 7.139375
 349827/1000000: episode: 1258, duration: 1.950s, episode steps: 258, steps per second: 132, episode reward: 25.308, mean reward: 0.098 [-10.990, 0.592], mean action: -0.487 [-1.096, 1.038], mean observation: 0.095 [-7.921, 6.000], loss: 0.870434, mean_absolute_error: 0.943304, mean_q: 7.153355
 350028/1000000: episode: 1259, duration: 1.545s, episode steps: 201, steps per second: 130, episode reward: -4.677, mean reward: -0.023 [-10.898, 0.231], mean action: -0.500 [-1.120, 1.109], mean observation: 0.197 [-6.797, 6.050], loss: 0.857257, mean_absolute_error: 0.954367, mean_q: 7.192006
 350247/1000000: episode: 1260, duration: 1.660s, episode steps: 219, steps per second: 132, episode reward: -5.616, mean reward: -0.026 [-10.881, 0.230], mean action: -0.485 [-1.090, 1.095], mean observation: 0.129 [-7.863, 6.059], loss: 0.887604, mean_absolute_error: 0.956065, mean_q: 7.167895
 350439/1000000: episode: 1261, duration: 1.488s, episode steps: 192, steps per second: 129, episode reward: 19.973, mean reward: 0.104 [-10.506, 0.280], mean action: -0.503 [-1.115, 1.189], mean observation: 0.188 [-6.149, 6.086], loss: 0.868938, mean_absolute_error: 0.949826, mean_q: 7.291155
 350687/1000000: episode: 1262, duration: 1.889s, episode steps: 248, steps per second: 131, episode reward: -63.995, mean reward: -0.258 [-11.419, 0.236], mean action: -0.492 [-1.121, 1.107], mean observation: 0.111 [-7.983, 6.074], loss: 0.922474, mean_absolute_error: 0.965353, mean_q: 7.184823
 350955/1000000: episode: 1263, duration: 2.034s, episode steps: 268, steps per second: 132, episode reward: 12.935, mean reward: 0.048 [-10.850, 0.543], mean action: -0.500 [-1.097, 1.072], mean observation: 0.081 [-7.939, 6.031], loss: 0.869928, mean_absolute_error: 0.949967, mean_q: 7.298405
 351211/1000000: episode: 1264, duration: 1.958s, episode steps: 256, steps per second: 131, episode reward: 17.526, mean reward: 0.068 [-10.963, 0.503], mean action: -0.535 [-1.115, 1.001], mean observation: 0.089 [-7.926, 6.008], loss: 0.878343, mean_absolute_error: 0.961328, mean_q: 7.234511
 351428/1000000: episode: 1265, duration: 1.714s, episode steps: 217, steps per second: 127, episode reward: -8.916, mean reward: -0.041 [-11.014, 0.251], mean action: -0.516 [-1.135, 1.104], mean observation: 0.143 [-7.872, 6.083], loss: 0.928967, mean_absolute_error: 0.966740, mean_q: 7.306858
 351637/1000000: episode: 1266, duration: 1.617s, episode steps: 209, steps per second: 129, episode reward: 7.967, mean reward: 0.038 [-10.915, 0.316], mean action: -0.517 [-1.135, 1.093], mean observation: 0.173 [-7.262, 6.054], loss: 0.851742, mean_absolute_error: 0.953447, mean_q: 7.322053
 351866/1000000: episode: 1267, duration: 1.727s, episode steps: 229, steps per second: 133, episode reward: -17.783, mean reward: -0.078 [-11.194, 0.284], mean action: -0.495 [-1.090, 1.123], mean observation: 0.140 [-7.994, 6.024], loss: 0.940339, mean_absolute_error: 0.967280, mean_q: 7.295714
 352050/1000000: episode: 1268, duration: 1.384s, episode steps: 184, steps per second: 133, episode reward: -15.278, mean reward: -0.083 [-10.500, 0.052], mean action: -0.504 [-1.110, 1.079], mean observation: 0.159 [-5.114, 6.038], loss: 0.903110, mean_absolute_error: 0.963922, mean_q: 7.247413
 352304/1000000: episode: 1269, duration: 2.058s, episode steps: 254, steps per second: 123, episode reward: -49.109, mean reward: -0.193 [-11.271, 0.299], mean action: -0.504 [-1.139, 1.058], mean observation: 0.082 [-7.976, 6.062], loss: 0.904483, mean_absolute_error: 0.960309, mean_q: 7.322845
 352583/1000000: episode: 1270, duration: 2.120s, episode steps: 279, steps per second: 132, episode reward: -40.038, mean reward: -0.144 [-11.035, 0.461], mean action: -0.504 [-1.148, 1.083], mean observation: 0.071 [-7.918, 5.948], loss: 0.857088, mean_absolute_error: 0.953978, mean_q: 7.346892
 352855/1000000: episode: 1271, duration: 2.054s, episode steps: 272, steps per second: 132, episode reward: -15.825, mean reward: -0.058 [-10.668, 0.377], mean action: -0.505 [-1.097, 1.078], mean observation: 0.033 [-7.992, 6.073], loss: 0.900509, mean_absolute_error: 0.960036, mean_q: 7.252117
 353072/1000000: episode: 1272, duration: 1.657s, episode steps: 217, steps per second: 131, episode reward: -23.143, mean reward: -0.107 [-10.915, 0.133], mean action: -0.500 [-1.123, 1.089], mean observation: 0.168 [-7.781, 6.059], loss: 0.851545, mean_absolute_error: 0.959737, mean_q: 7.299156
 353309/1000000: episode: 1273, duration: 1.809s, episode steps: 237, steps per second: 131, episode reward: -34.105, mean reward: -0.144 [-10.986, 0.147], mean action: -0.497 [-1.090, 1.067], mean observation: 0.135 [-7.942, 6.025], loss: 0.910585, mean_absolute_error: 0.977694, mean_q: 7.391593
 353507/1000000: episode: 1274, duration: 1.544s, episode steps: 198, steps per second: 128, episode reward: -34.500, mean reward: -0.174 [-10.948, 0.050], mean action: -0.507 [-1.091, 1.030], mean observation: 0.224 [-6.195, 5.994], loss: 1.034702, mean_absolute_error: 0.999486, mean_q: 7.273524
 353675/1000000: episode: 1275, duration: 1.289s, episode steps: 168, steps per second: 130, episode reward: 25.817, mean reward: 0.154 [-10.137, 0.271], mean action: -0.503 [-1.085, 1.068], mean observation: 0.198 [-3.779, 6.051], loss: 0.885798, mean_absolute_error: 0.966445, mean_q: 7.178884
 353917/1000000: episode: 1276, duration: 1.906s, episode steps: 242, steps per second: 127, episode reward: -51.762, mean reward: -0.214 [-11.431, 0.248], mean action: -0.505 [-1.107, 1.046], mean observation: 0.149 [-7.934, 6.014], loss: 0.898956, mean_absolute_error: 0.972263, mean_q: 7.484805
 354172/1000000: episode: 1277, duration: 1.919s, episode steps: 255, steps per second: 133, episode reward: -37.969, mean reward: -0.149 [-11.063, 0.269], mean action: -0.475 [-1.078, 1.092], mean observation: 0.110 [-7.968, 6.040], loss: 0.913812, mean_absolute_error: 0.972956, mean_q: 7.387650
 354377/1000000: episode: 1278, duration: 1.565s, episode steps: 205, steps per second: 131, episode reward: -24.726, mean reward: -0.121 [-10.738, 0.058], mean action: -0.491 [-1.070, 1.034], mean observation: 0.142 [-6.780, 6.001], loss: 0.903391, mean_absolute_error: 0.972153, mean_q: 7.426762
 354562/1000000: episode: 1279, duration: 1.391s, episode steps: 185, steps per second: 133, episode reward: 13.211, mean reward: 0.071 [-10.331, 0.202], mean action: -0.490 [-1.110, 1.093], mean observation: 0.171 [-5.136, 6.010], loss: 0.864037, mean_absolute_error: 0.964385, mean_q: 7.431184
 354799/1000000: episode: 1280, duration: 1.868s, episode steps: 237, steps per second: 127, episode reward: 29.051, mean reward: 0.123 [-10.952, 0.473], mean action: -0.502 [-1.071, 1.022], mean observation: 0.126 [-7.922, 6.012], loss: 0.922277, mean_absolute_error: 0.975227, mean_q: 7.542539
 354969/1000000: episode: 1281, duration: 1.302s, episode steps: 170, steps per second: 131, episode reward: 0.984, mean reward: 0.006 [-10.353, 0.139], mean action: -0.476 [-1.111, 1.144], mean observation: 0.215 [-3.953, 6.059], loss: 0.935171, mean_absolute_error: 0.970597, mean_q: 7.365575
 355255/1000000: episode: 1282, duration: 2.207s, episode steps: 286, steps per second: 130, episode reward: -15.503, mean reward: -0.054 [-11.413, 0.543], mean action: -0.779 [-1.161, 1.145], mean observation: 0.109 [-6.771, 5.971], loss: 0.874536, mean_absolute_error: 0.969730, mean_q: 7.376683
 355943/1000000: episode: 1283, duration: 5.262s, episode steps: 688, steps per second: 131, episode reward: 101.937, mean reward: 0.148 [-10.493, 0.370], mean action: -1.009 [-1.171, -0.871], mean observation: 0.158 [-0.783, 1.557], loss: 0.912961, mean_absolute_error: 0.983096, mean_q: 7.600485
 356594/1000000: episode: 1284, duration: 4.976s, episode steps: 651, steps per second: 131, episode reward: 158.798, mean reward: 0.244 [-11.025, 0.446], mean action: -0.785 [-1.137, 1.077], mean observation: 0.136 [-8.238, 7.337], loss: 0.891903, mean_absolute_error: 0.977438, mean_q: 7.584274
 357491/1000000: episode: 1285, duration: 6.882s, episode steps: 897, steps per second: 130, episode reward: 254.442, mean reward: 0.284 [-11.073, 0.600], mean action: -0.854 [-1.145, 1.101], mean observation: 0.086 [-8.268, 7.507], loss: 0.935331, mean_absolute_error: 0.986106, mean_q: 7.713872
 358355/1000000: episode: 1286, duration: 6.532s, episode steps: 864, steps per second: 132, episode reward: 257.051, mean reward: 0.298 [-11.410, 0.575], mean action: -0.852 [-1.124, 1.103], mean observation: 0.133 [-8.127, 7.821], loss: 0.952915, mean_absolute_error: 1.002361, mean_q: 7.730756
 359020/1000000: episode: 1287, duration: 5.151s, episode steps: 665, steps per second: 129, episode reward: 140.661, mean reward: 0.212 [-11.238, 0.404], mean action: -0.834 [-1.131, 1.088], mean observation: 0.183 [-7.539, 6.637], loss: 0.935122, mean_absolute_error: 1.000153, mean_q: 7.821141
 359351/1000000: episode: 1288, duration: 2.542s, episode steps: 331, steps per second: 130, episode reward: 33.259, mean reward: 0.100 [-11.015, 0.558], mean action: -0.614 [-1.155, 1.115], mean observation: 0.083 [-8.005, 6.057], loss: 0.969300, mean_absolute_error: 1.008189, mean_q: 7.951937
 360057/1000000: episode: 1289, duration: 5.410s, episode steps: 706, steps per second: 131, episode reward: 46.060, mean reward: 0.065 [-11.442, 0.232], mean action: -0.859 [-1.132, 1.086], mean observation: 0.210 [-6.742, 6.424], loss: 0.934715, mean_absolute_error: 1.002868, mean_q: 7.904698
 360730/1000000: episode: 1290, duration: 5.195s, episode steps: 673, steps per second: 130, episode reward: 158.091, mean reward: 0.235 [-11.904, 0.650], mean action: -0.826 [-1.137, 1.073], mean observation: 0.097 [-7.510, 4.480], loss: 0.934904, mean_absolute_error: 1.009571, mean_q: 7.964139
 360954/1000000: episode: 1291, duration: 1.859s, episode steps: 224, steps per second: 121, episode reward: -0.308, mean reward: -0.001 [-10.802, 0.269], mean action: -0.414 [-1.133, 1.055], mean observation: -0.119 [-5.768, 1.000], loss: 0.958975, mean_absolute_error: 1.021699, mean_q: 7.996208
 361222/1000000: episode: 1292, duration: 2.072s, episode steps: 268, steps per second: 129, episode reward: -9.543, mean reward: -0.036 [-11.107, 0.364], mean action: -0.240 [-1.117, 1.102], mean observation: -0.179 [-6.556, 2.221], loss: 0.936171, mean_absolute_error: 0.999805, mean_q: 8.043982
 361484/1000000: episode: 1293, duration: 1.994s, episode steps: 262, steps per second: 131, episode reward: -11.515, mean reward: -0.044 [-11.352, 0.503], mean action: -0.247 [-1.184, 1.098], mean observation: -0.200 [-6.757, 2.268], loss: 0.925223, mean_absolute_error: 0.998653, mean_q: 8.051668
 361730/1000000: episode: 1294, duration: 1.847s, episode steps: 246, steps per second: 133, episode reward: -56.255, mean reward: -0.229 [-11.518, 0.246], mean action: -0.225 [-1.051, 1.082], mean observation: -0.172 [-5.891, 1.000], loss: 0.993359, mean_absolute_error: 1.024226, mean_q: 8.003228
 361953/1000000: episode: 1295, duration: 1.695s, episode steps: 223, steps per second: 132, episode reward: -6.274, mean reward: -0.028 [-10.966, 0.280], mean action: -0.256 [-1.116, 1.056], mean observation: -0.124 [-5.955, 1.000], loss: 1.001310, mean_absolute_error: 1.013120, mean_q: 7.862300
 362168/1000000: episode: 1296, duration: 1.619s, episode steps: 215, steps per second: 133, episode reward: -16.672, mean reward: -0.078 [-10.863, 0.176], mean action: -0.266 [-1.118, 1.151], mean observation: -0.108 [-5.691, 1.000], loss: 0.935765, mean_absolute_error: 1.018810, mean_q: 8.094522
 362447/1000000: episode: 1297, duration: 2.095s, episode steps: 279, steps per second: 133, episode reward: -41.290, mean reward: -0.148 [-11.412, 0.362], mean action: -0.225 [-1.193, 1.113], mean observation: -0.163 [-6.240, 2.080], loss: 0.958870, mean_absolute_error: 1.010291, mean_q: 8.097436
 362647/1000000: episode: 1298, duration: 1.496s, episode steps: 200, steps per second: 134, episode reward: -4.314, mean reward: -0.022 [-10.556, 0.161], mean action: -0.320 [-1.156, 1.062], mean observation: -0.108 [-5.779, 1.000], loss: 0.968172, mean_absolute_error: 1.009076, mean_q: 8.112844
 362865/1000000: episode: 1299, duration: 1.631s, episode steps: 218, steps per second: 134, episode reward: -0.766, mean reward: -0.004 [-10.780, 0.247], mean action: -0.259 [-1.095, 1.108], mean observation: -0.138 [-5.732, 1.000], loss: 0.987231, mean_absolute_error: 1.019030, mean_q: 8.015735
 363094/1000000: episode: 1300, duration: 1.719s, episode steps: 229, steps per second: 133, episode reward: -45.095, mean reward: -0.197 [-11.315, 0.192], mean action: -0.211 [-1.087, 1.148], mean observation: -0.156 [-5.573, 1.000], loss: 0.983446, mean_absolute_error: 1.023075, mean_q: 8.120073
 363388/1000000: episode: 1301, duration: 2.206s, episode steps: 294, steps per second: 133, episode reward: -71.923, mean reward: -0.245 [-11.642, 0.336], mean action: -0.175 [-1.110, 1.095], mean observation: -0.164 [-5.626, 1.791], loss: 0.928979, mean_absolute_error: 1.003582, mean_q: 8.236791
 363624/1000000: episode: 1302, duration: 1.817s, episode steps: 236, steps per second: 130, episode reward: 19.526, mean reward: 0.083 [-10.878, 0.392], mean action: -0.229 [-1.172, 1.107], mean observation: -0.127 [-5.417, 1.000], loss: 0.923093, mean_absolute_error: 1.004431, mean_q: 8.148832
 363768/1000000: episode: 1303, duration: 1.185s, episode steps: 144, steps per second: 121, episode reward: 10.923, mean reward: 0.076 [-9.988, 0.167], mean action: -0.388 [-1.112, 1.078], mean observation: 0.021 [-5.498, 1.000], loss: 0.967787, mean_absolute_error: 1.005915, mean_q: 8.222754
 363936/1000000: episode: 1304, duration: 1.287s, episode steps: 168, steps per second: 131, episode reward: 0.500, mean reward: 0.003 [-10.353, 0.155], mean action: -0.291 [-1.117, 1.054], mean observation: -0.048 [-5.556, 1.000], loss: 0.964250, mean_absolute_error: 1.021077, mean_q: 8.136195
 364112/1000000: episode: 1305, duration: 1.421s, episode steps: 176, steps per second: 124, episode reward: -20.816, mean reward: -0.118 [-10.365, -0.002], mean action: -0.289 [-1.065, 1.083], mean observation: -0.017 [-5.532, 1.000], loss: 0.948770, mean_absolute_error: 1.007087, mean_q: 8.211332
 364401/1000000: episode: 1306, duration: 2.215s, episode steps: 289, steps per second: 130, episode reward: -45.660, mean reward: -0.158 [-11.468, 0.388], mean action: -0.186 [-1.127, 1.121], mean observation: -0.180 [-6.016, 2.310], loss: 0.926263, mean_absolute_error: 1.000351, mean_q: 8.168638
 364649/1000000: episode: 1307, duration: 1.941s, episode steps: 248, steps per second: 128, episode reward: -24.378, mean reward: -0.098 [-11.391, 0.381], mean action: -0.208 [-1.127, 1.162], mean observation: -0.173 [-5.555, 1.000], loss: 0.881328, mean_absolute_error: 0.994112, mean_q: 8.105668
 364853/1000000: episode: 1308, duration: 1.626s, episode steps: 204, steps per second: 125, episode reward: -10.017, mean reward: -0.049 [-10.558, 0.133], mean action: -0.210 [-1.088, 1.086], mean observation: -0.075 [-5.308, 1.000], loss: 0.930328, mean_absolute_error: 1.009745, mean_q: 8.213124
 365038/1000000: episode: 1309, duration: 1.433s, episode steps: 185, steps per second: 129, episode reward: 28.364, mean reward: 0.153 [-10.171, 0.287], mean action: -0.260 [-1.086, 1.026], mean observation: -0.062 [-5.278, 1.000], loss: 0.919343, mean_absolute_error: 1.006105, mean_q: 8.138899
 365324/1000000: episode: 1310, duration: 2.233s, episode steps: 286, steps per second: 128, episode reward: -23.177, mean reward: -0.081 [-11.437, 0.417], mean action: -0.152 [-1.101, 1.096], mean observation: -0.183 [-5.334, 1.000], loss: 0.929593, mean_absolute_error: 1.007323, mean_q: 8.299526
 365522/1000000: episode: 1311, duration: 1.499s, episode steps: 198, steps per second: 132, episode reward: -23.370, mean reward: -0.118 [-10.722, 0.106], mean action: -0.212 [-1.102, 1.088], mean observation: -0.095 [-5.166, 1.000], loss: 0.954925, mean_absolute_error: 1.002966, mean_q: 8.225153
 365793/1000000: episode: 1312, duration: 2.095s, episode steps: 271, steps per second: 129, episode reward: -31.596, mean reward: -0.117 [-11.606, 0.494], mean action: -0.188 [-1.116, 1.084], mean observation: -0.178 [-5.694, 1.000], loss: 0.960537, mean_absolute_error: 0.999891, mean_q: 8.192186
 366021/1000000: episode: 1313, duration: 1.742s, episode steps: 228, steps per second: 131, episode reward: -21.125, mean reward: -0.093 [-11.074, 0.253], mean action: -0.188 [-1.092, 1.080], mean observation: -0.140 [-5.262, 1.000], loss: 0.983061, mean_absolute_error: 1.014131, mean_q: 8.158629
 366222/1000000: episode: 1314, duration: 1.531s, episode steps: 201, steps per second: 131, episode reward: 31.707, mean reward: 0.158 [-10.375, 0.343], mean action: -0.240 [-1.108, 1.081], mean observation: -0.086 [-5.235, 1.000], loss: 0.879069, mean_absolute_error: 0.993848, mean_q: 8.268758
 366449/1000000: episode: 1315, duration: 1.706s, episode steps: 227, steps per second: 133, episode reward: 21.311, mean reward: 0.094 [-10.588, 0.313], mean action: -0.196 [-1.077, 1.089], mean observation: -0.112 [-5.253, 1.000], loss: 0.958595, mean_absolute_error: 1.014858, mean_q: 8.230382
 366726/1000000: episode: 1316, duration: 2.114s, episode steps: 277, steps per second: 131, episode reward: -51.602, mean reward: -0.186 [-11.618, 0.374], mean action: -0.167 [-1.070, 1.097], mean observation: -0.169 [-5.354, 1.000], loss: 0.924434, mean_absolute_error: 1.006867, mean_q: 8.270723
 367001/1000000: episode: 1317, duration: 2.091s, episode steps: 275, steps per second: 132, episode reward: -9.956, mean reward: -0.036 [-11.307, 0.430], mean action: -0.194 [-1.106, 1.130], mean observation: -0.186 [-5.841, 1.196], loss: 0.954869, mean_absolute_error: 1.002155, mean_q: 8.139936
 367271/1000000: episode: 1318, duration: 2.052s, episode steps: 270, steps per second: 132, episode reward: -45.488, mean reward: -0.168 [-11.465, 0.336], mean action: -0.218 [-1.082, 1.076], mean observation: -0.198 [-5.990, 1.130], loss: 0.934217, mean_absolute_error: 1.009747, mean_q: 8.284896
 367527/1000000: episode: 1319, duration: 1.958s, episode steps: 256, steps per second: 131, episode reward: -1.659, mean reward: -0.006 [-11.157, 0.387], mean action: -0.202 [-1.134, 1.091], mean observation: -0.151 [-5.376, 1.000], loss: 0.919759, mean_absolute_error: 0.999714, mean_q: 8.265215
 367789/1000000: episode: 1320, duration: 2.074s, episode steps: 262, steps per second: 126, episode reward: -43.767, mean reward: -0.167 [-11.592, 0.416], mean action: -0.212 [-1.103, 1.081], mean observation: -0.188 [-6.179, 1.000], loss: 0.949301, mean_absolute_error: 1.008970, mean_q: 8.324556
 368062/1000000: episode: 1321, duration: 2.120s, episode steps: 273, steps per second: 129, episode reward: -1.965, mean reward: -0.007 [-11.369, 0.530], mean action: -0.245 [-1.101, 1.110], mean observation: -0.196 [-6.359, 1.962], loss: 0.901958, mean_absolute_error: 0.993386, mean_q: 8.276664
 368223/1000000: episode: 1322, duration: 1.237s, episode steps: 161, steps per second: 130, episode reward: -18.027, mean reward: -0.112 [-10.427, 0.032], mean action: -0.333 [-1.052, 1.054], mean observation: -0.071 [-5.662, 1.000], loss: 0.921820, mean_absolute_error: 1.001013, mean_q: 8.193866
 368456/1000000: episode: 1323, duration: 1.789s, episode steps: 233, steps per second: 130, episode reward: -41.662, mean reward: -0.179 [-11.410, 0.270], mean action: -0.235 [-1.144, 1.101], mean observation: -0.169 [-6.182, 1.000], loss: 0.964632, mean_absolute_error: 0.994051, mean_q: 8.212597
 368683/1000000: episode: 1324, duration: 1.761s, episode steps: 227, steps per second: 129, episode reward: -4.088, mean reward: -0.018 [-11.156, 0.374], mean action: -0.210 [-1.058, 1.136], mean observation: -0.174 [-6.138, 1.000], loss: 0.950708, mean_absolute_error: 0.999992, mean_q: 8.262658
 368955/1000000: episode: 1325, duration: 2.093s, episode steps: 272, steps per second: 130, episode reward: -36.514, mean reward: -0.134 [-11.625, 0.537], mean action: -0.224 [-1.131, 1.070], mean observation: -0.197 [-6.407, 2.099], loss: 0.892420, mean_absolute_error: 0.978679, mean_q: 8.393628
 369186/1000000: episode: 1326, duration: 1.771s, episode steps: 231, steps per second: 130, episode reward: -12.959, mean reward: -0.056 [-10.899, 0.225], mean action: -0.242 [-1.082, 1.118], mean observation: -0.133 [-6.106, 1.000], loss: 0.921661, mean_absolute_error: 0.989476, mean_q: 8.315035
 369434/1000000: episode: 1327, duration: 1.864s, episode steps: 248, steps per second: 133, episode reward: -20.691, mean reward: -0.083 [-11.207, 0.316], mean action: -0.248 [-1.121, 1.082], mean observation: -0.147 [-6.335, 1.000], loss: 0.879237, mean_absolute_error: 0.974017, mean_q: 8.166698
 369717/1000000: episode: 1328, duration: 2.114s, episode steps: 283, steps per second: 134, episode reward: -46.826, mean reward: -0.165 [-11.196, 0.298], mean action: -0.239 [-1.136, 1.080], mean observation: -0.201 [-6.698, 3.402], loss: 0.879392, mean_absolute_error: 0.978244, mean_q: 8.152920
 369992/1000000: episode: 1329, duration: 2.086s, episode steps: 275, steps per second: 132, episode reward: -55.889, mean reward: -0.203 [-11.255, 0.210], mean action: -0.216 [-1.123, 1.145], mean observation: -0.158 [-6.215, 1.834], loss: 0.884494, mean_absolute_error: 0.979631, mean_q: 8.281281
 370287/1000000: episode: 1330, duration: 2.277s, episode steps: 295, steps per second: 130, episode reward: -50.010, mean reward: -0.170 [-11.715, 0.498], mean action: -0.181 [-1.176, 1.136], mean observation: -0.177 [-5.711, 1.874], loss: 0.889460, mean_absolute_error: 0.971717, mean_q: 8.151320
 370532/1000000: episode: 1331, duration: 1.871s, episode steps: 245, steps per second: 131, episode reward: 22.352, mean reward: 0.091 [-11.067, 0.478], mean action: -0.241 [-1.119, 1.099], mean observation: -0.162 [-6.069, 1.000], loss: 0.913308, mean_absolute_error: 0.980771, mean_q: 8.238224
 370751/1000000: episode: 1332, duration: 1.672s, episode steps: 219, steps per second: 131, episode reward: -16.596, mean reward: -0.076 [-10.859, 0.185], mean action: -0.227 [-1.073, 1.100], mean observation: -0.147 [-5.598, 1.000], loss: 0.907717, mean_absolute_error: 0.967583, mean_q: 8.252555
 370997/1000000: episode: 1333, duration: 1.892s, episode steps: 246, steps per second: 130, episode reward: -6.934, mean reward: -0.028 [-11.291, 0.424], mean action: -0.215 [-1.106, 1.118], mean observation: -0.165 [-6.069, 1.000], loss: 0.933591, mean_absolute_error: 0.981460, mean_q: 8.218266
 371200/1000000: episode: 1334, duration: 1.544s, episode steps: 203, steps per second: 131, episode reward: 22.609, mean reward: 0.111 [-10.591, 0.337], mean action: -0.280 [-1.057, 1.069], mean observation: -0.117 [-5.709, 1.000], loss: 0.897179, mean_absolute_error: 0.970316, mean_q: 8.175297
 371440/1000000: episode: 1335, duration: 1.830s, episode steps: 240, steps per second: 131, episode reward: -18.292, mean reward: -0.076 [-11.249, 0.337], mean action: -0.257 [-1.103, 1.102], mean observation: -0.186 [-6.411, 1.000], loss: 0.859105, mean_absolute_error: 0.962633, mean_q: 8.205548
 371676/1000000: episode: 1336, duration: 1.796s, episode steps: 236, steps per second: 131, episode reward: -30.018, mean reward: -0.127 [-10.872, 0.227], mean action: -0.495 [-1.115, 1.078], mean observation: -0.124 [-5.337, 1.000], loss: 0.925768, mean_absolute_error: 0.976056, mean_q: 8.052728
 371922/1000000: episode: 1337, duration: 1.885s, episode steps: 246, steps per second: 130, episode reward: 1.506, mean reward: 0.006 [-10.336, 0.202], mean action: -0.496 [-1.100, 1.107], mean observation: -0.076 [-5.190, 1.000], loss: 0.882014, mean_absolute_error: 0.966766, mean_q: 8.192878
 372221/1000000: episode: 1338, duration: 2.259s, episode steps: 299, steps per second: 132, episode reward: -39.072, mean reward: -0.131 [-10.798, 0.339], mean action: -0.510 [-1.107, 1.073], mean observation: -0.103 [-5.366, 1.150], loss: 0.870034, mean_absolute_error: 0.960087, mean_q: 8.135036
 372459/1000000: episode: 1339, duration: 1.793s, episode steps: 238, steps per second: 133, episode reward: 45.930, mean reward: 0.193 [-10.001, 0.324], mean action: -0.684 [-1.167, 1.068], mean observation: -0.064 [-4.077, 1.000], loss: 0.924448, mean_absolute_error: 0.970425, mean_q: 8.158906
 373051/1000000: episode: 1340, duration: 4.473s, episode steps: 592, steps per second: 132, episode reward: 152.240, mean reward: 0.257 [-11.438, 0.546], mean action: -0.852 [-1.148, 1.076], mean observation: 0.225 [-7.007, 4.369], loss: 0.885279, mean_absolute_error: 0.968904, mean_q: 8.121488
 373742/1000000: episode: 1341, duration: 5.276s, episode steps: 691, steps per second: 131, episode reward: 158.669, mean reward: 0.230 [-11.395, 0.489], mean action: -0.850 [-1.136, 1.056], mean observation: 0.127 [-7.543, 4.142], loss: 0.912084, mean_absolute_error: 0.970306, mean_q: 8.221528
 374419/1000000: episode: 1342, duration: 5.173s, episode steps: 677, steps per second: 131, episode reward: 211.954, mean reward: 0.313 [-10.263, 0.522], mean action: -0.997 [-1.149, -0.842], mean observation: 0.209 [-0.160, 1.472], loss: 0.847240, mean_absolute_error: 0.955592, mean_q: 8.150840
 374773/1000000: episode: 1343, duration: 2.699s, episode steps: 354, steps per second: 131, episode reward: 37.109, mean reward: 0.105 [-9.831, 0.168], mean action: -0.999 [-1.095, -0.884], mean observation: 0.121 [-0.458, 1.000], loss: 0.892779, mean_absolute_error: 0.958432, mean_q: 8.128325
 375374/1000000: episode: 1344, duration: 4.537s, episode steps: 601, steps per second: 132, episode reward: 125.511, mean reward: 0.209 [-10.270, 0.374], mean action: -0.995 [-1.130, -0.872], mean observation: 0.053 [-1.682, 1.000], loss: 0.867487, mean_absolute_error: 0.957646, mean_q: 8.120877
 376089/1000000: episode: 1345, duration: 5.453s, episode steps: 715, steps per second: 131, episode reward: 56.961, mean reward: 0.080 [-10.537, 0.212], mean action: -0.998 [-1.130, -0.877], mean observation: 0.221 [-0.415, 2.234], loss: 0.899795, mean_absolute_error: 0.965147, mean_q: 8.128736
 376643/1000000: episode: 1346, duration: 4.180s, episode steps: 554, steps per second: 133, episode reward: 4.913, mean reward: 0.009 [-10.368, 0.126], mean action: -1.030 [-1.175, -0.911], mean observation: 0.042 [-1.635, 1.000], loss: 0.835146, mean_absolute_error: 0.947019, mean_q: 8.107273
 376955/1000000: episode: 1347, duration: 2.384s, episode steps: 312, steps per second: 131, episode reward: -41.009, mean reward: -0.131 [-11.436, 0.227], mean action: -0.666 [-1.065, 1.044], mean observation: -0.125 [-7.806, 1.713], loss: 0.914798, mean_absolute_error: 0.963531, mean_q: 8.161985
 377159/1000000: episode: 1348, duration: 1.583s, episode steps: 204, steps per second: 129, episode reward: -22.535, mean reward: -0.110 [-11.128, 0.212], mean action: -0.520 [-1.128, 1.027], mean observation: -0.156 [-7.479, 1.000], loss: 0.914219, mean_absolute_error: 0.976940, mean_q: 8.092889
 377400/1000000: episode: 1349, duration: 1.829s, episode steps: 241, steps per second: 132, episode reward: -38.291, mean reward: -0.159 [-11.068, 0.222], mean action: -0.512 [-1.090, 1.106], mean observation: -0.177 [-7.943, 2.739], loss: 0.916116, mean_absolute_error: 0.963927, mean_q: 8.262070
 377633/1000000: episode: 1350, duration: 1.766s, episode steps: 233, steps per second: 132, episode reward: 22.134, mean reward: 0.095 [-10.915, 0.446], mean action: -0.518 [-1.124, 1.017], mean observation: -0.156 [-7.902, 1.421], loss: 0.889050, mean_absolute_error: 0.961052, mean_q: 8.118174
 377911/1000000: episode: 1351, duration: 2.111s, episode steps: 278, steps per second: 132, episode reward: -22.589, mean reward: -0.081 [-11.232, 0.563], mean action: -0.700 [-1.119, 1.076], mean observation: -0.206 [-7.422, 4.403], loss: 0.849229, mean_absolute_error: 0.944992, mean_q: 8.131477
 378633/1000000: episode: 1352, duration: 5.571s, episode steps: 722, steps per second: 130, episode reward: 133.453, mean reward: 0.185 [-10.765, 0.400], mean action: -0.999 [-1.191, -0.840], mean observation: 0.096 [-2.805, 1.148], loss: 0.856446, mean_absolute_error: 0.947117, mean_q: 8.108711
 379348/1000000: episode: 1353, duration: 5.399s, episode steps: 715, steps per second: 132, episode reward: 179.509, mean reward: 0.251 [-10.413, 0.402], mean action: -0.991 [-1.133, -0.857], mean observation: 0.208 [-0.663, 2.278], loss: 0.910341, mean_absolute_error: 0.962561, mean_q: 8.082133
 380007/1000000: episode: 1354, duration: 4.935s, episode steps: 659, steps per second: 134, episode reward: 175.378, mean reward: 0.266 [-10.759, 0.529], mean action: -1.006 [-1.165, -0.872], mean observation: -0.014 [-2.370, 1.000], loss: 0.876910, mean_absolute_error: 0.957456, mean_q: 8.094978
 380707/1000000: episode: 1355, duration: 5.298s, episode steps: 700, steps per second: 132, episode reward: 177.722, mean reward: 0.254 [-10.538, 0.439], mean action: -1.003 [-1.126, -0.824], mean observation: 0.020 [-2.612, 1.000], loss: 0.891179, mean_absolute_error: 0.958073, mean_q: 8.028818
 381147/1000000: episode: 1356, duration: 3.311s, episode steps: 440, steps per second: 133, episode reward: 83.963, mean reward: 0.191 [-11.156, 0.442], mean action: -0.898 [-1.113, 0.947], mean observation: -0.048 [-5.439, 1.000], loss: 0.829602, mean_absolute_error: 0.951583, mean_q: 8.067442
 381438/1000000: episode: 1357, duration: 2.359s, episode steps: 291, steps per second: 123, episode reward: -76.788, mean reward: -0.264 [-11.768, 0.332], mean action: -0.849 [-1.152, 1.018], mean observation: -0.219 [-5.227, 1.000], loss: 0.896051, mean_absolute_error: 0.955103, mean_q: 8.052307
 382559/1000000: episode: 1358, duration: 8.614s, episode steps: 1121, steps per second: 130, episode reward: 200.512, mean reward: 0.179 [-9.912, 0.231], mean action: -0.984 [-1.151, -0.840], mean observation: 0.135 [-1.800, 1.904], loss: 0.851458, mean_absolute_error: 0.948276, mean_q: 8.096750
 382747/1000000: episode: 1359, duration: 1.535s, episode steps: 188, steps per second: 122, episode reward: -29.120, mean reward: -0.155 [-10.818, 0.073], mean action: -0.690 [-1.114, 1.119], mean observation: -0.102 [-5.751, 1.000], loss: 0.842320, mean_absolute_error: 0.948108, mean_q: 8.112462
 382973/1000000: episode: 1360, duration: 1.726s, episode steps: 226, steps per second: 131, episode reward: -1.212, mean reward: -0.005 [-11.119, 0.375], mean action: -0.729 [-1.097, 1.050], mean observation: -0.162 [-5.916, 1.000], loss: 0.809308, mean_absolute_error: 0.939279, mean_q: 8.034384
 383258/1000000: episode: 1361, duration: 2.156s, episode steps: 285, steps per second: 132, episode reward: -45.656, mean reward: -0.160 [-11.507, 0.463], mean action: -0.778 [-1.138, 1.045], mean observation: -0.213 [-6.473, 3.010], loss: 0.913733, mean_absolute_error: 0.961887, mean_q: 8.008397
 383503/1000000: episode: 1362, duration: 1.940s, episode steps: 245, steps per second: 126, episode reward: -19.464, mean reward: -0.079 [-11.066, 0.262], mean action: -0.827 [-1.119, 1.074], mean observation: -0.124 [-5.121, 1.000], loss: 0.829621, mean_absolute_error: 0.953993, mean_q: 7.956557
 384074/1000000: episode: 1363, duration: 4.436s, episode steps: 571, steps per second: 129, episode reward: -31.323, mean reward: -0.055 [-10.356, 0.053], mean action: -1.002 [-1.136, -0.871], mean observation: 0.187 [-0.397, 1.354], loss: 0.847806, mean_absolute_error: 0.945357, mean_q: 8.042783
 384794/1000000: episode: 1364, duration: 5.527s, episode steps: 720, steps per second: 130, episode reward: 129.646, mean reward: 0.180 [-10.311, 0.300], mean action: -1.009 [-1.169, -0.832], mean observation: 0.201 [-0.768, 1.847], loss: 0.875383, mean_absolute_error: 0.945128, mean_q: 8.029754
 385549/1000000: episode: 1365, duration: 6.032s, episode steps: 755, steps per second: 125, episode reward: 27.327, mean reward: 0.036 [-10.367, 0.150], mean action: -1.005 [-1.127, -0.872], mean observation: 0.053 [-1.204, 1.000], loss: 0.847555, mean_absolute_error: 0.943978, mean_q: 8.035218
 386290/1000000: episode: 1366, duration: 5.721s, episode steps: 741, steps per second: 130, episode reward: 280.234, mean reward: 0.378 [-10.342, 0.552], mean action: -0.989 [-1.115, -0.840], mean observation: 0.214 [-0.310, 1.967], loss: 0.890255, mean_absolute_error: 0.950505, mean_q: 8.013920
 387102/1000000: episode: 1367, duration: 6.151s, episode steps: 812, steps per second: 132, episode reward: 93.003, mean reward: 0.115 [-10.389, 0.315], mean action: -1.000 [-1.133, -0.870], mean observation: 0.056 [-1.109, 1.000], loss: 0.851479, mean_absolute_error: 0.938207, mean_q: 8.053086
 387759/1000000: episode: 1368, duration: 4.906s, episode steps: 657, steps per second: 134, episode reward: 85.674, mean reward: 0.130 [-10.753, 0.393], mean action: -0.991 [-1.147, -0.866], mean observation: -0.017 [-1.913, 1.000], loss: 0.836659, mean_absolute_error: 0.938022, mean_q: 8.051742
 388368/1000000: episode: 1369, duration: 4.585s, episode steps: 609, steps per second: 133, episode reward: 117.768, mean reward: 0.193 [-10.590, 0.348], mean action: -1.003 [-1.127, -0.884], mean observation: 0.235 [-1.022, 2.361], loss: 0.847774, mean_absolute_error: 0.938344, mean_q: 8.097895
 388629/1000000: episode: 1370, duration: 1.959s, episode steps: 261, steps per second: 133, episode reward: -33.377, mean reward: -0.128 [-11.083, 0.195], mean action: -0.836 [-1.117, 1.005], mean observation: -0.175 [-5.318, 1.000], loss: 0.816692, mean_absolute_error: 0.935232, mean_q: 8.057617
 389000/1000000: episode: 1371, duration: 2.799s, episode steps: 371, steps per second: 133, episode reward: 34.674, mean reward: 0.093 [-9.926, 0.134], mean action: -1.021 [-1.161, -0.877], mean observation: 0.109 [-1.244, 1.000], loss: 0.814762, mean_absolute_error: 0.933153, mean_q: 8.006916
 389451/1000000: episode: 1372, duration: 3.414s, episode steps: 451, steps per second: 132, episode reward: 20.156, mean reward: 0.045 [-10.489, 0.217], mean action: -1.001 [-1.120, -0.448], mean observation: 0.010 [-2.287, 1.000], loss: 0.846392, mean_absolute_error: 0.933450, mean_q: 8.082683
 390129/1000000: episode: 1373, duration: 5.111s, episode steps: 678, steps per second: 133, episode reward: 66.199, mean reward: 0.098 [-10.401, 0.267], mean action: -1.019 [-1.155, -0.902], mean observation: 0.041 [-1.642, 1.000], loss: 0.838983, mean_absolute_error: 0.934667, mean_q: 8.104464
 390772/1000000: episode: 1374, duration: 4.868s, episode steps: 643, steps per second: 132, episode reward: 167.269, mean reward: 0.260 [-9.860, 0.297], mean action: -0.996 [-1.136, -0.805], mean observation: 0.111 [-0.893, 1.096], loss: 0.842741, mean_absolute_error: 0.936769, mean_q: 8.157191
 391410/1000000: episode: 1375, duration: 4.822s, episode steps: 638, steps per second: 132, episode reward: 111.773, mean reward: 0.175 [-10.443, 0.354], mean action: -1.004 [-1.130, -0.874], mean observation: 0.148 [-1.488, 1.421], loss: 0.821694, mean_absolute_error: 0.930085, mean_q: 8.141251
 391925/1000000: episode: 1376, duration: 3.870s, episode steps: 515, steps per second: 133, episode reward: 83.491, mean reward: 0.162 [-10.261, 0.288], mean action: -0.993 [-1.148, -0.863], mean observation: 0.219 [-0.308, 1.734], loss: 0.830359, mean_absolute_error: 0.940120, mean_q: 8.149974
 392884/1000000: episode: 1377, duration: 7.254s, episode steps: 959, steps per second: 132, episode reward: 164.494, mean reward: 0.172 [-10.702, 0.343], mean action: -1.012 [-1.170, -0.862], mean observation: 0.236 [-0.308, 1.962], loss: 0.841372, mean_absolute_error: 0.935231, mean_q: 8.137642
 393443/1000000: episode: 1378, duration: 4.225s, episode steps: 559, steps per second: 132, episode reward: 77.035, mean reward: 0.138 [-11.537, 0.408], mean action: -0.920 [-1.142, 0.993], mean observation: -0.003 [-5.061, 1.000], loss: 0.850042, mean_absolute_error: 0.938124, mean_q: 8.166602
 393685/1000000: episode: 1379, duration: 1.877s, episode steps: 242, steps per second: 129, episode reward: -37.189, mean reward: -0.154 [-11.367, 0.284], mean action: -0.788 [-1.108, 1.076], mean observation: -0.164 [-5.470, 1.000], loss: 0.818795, mean_absolute_error: 0.936824, mean_q: 8.193443
 393874/1000000: episode: 1380, duration: 1.430s, episode steps: 189, steps per second: 132, episode reward: 36.084, mean reward: 0.191 [-10.240, 0.355], mean action: -0.771 [-1.182, 1.024], mean observation: -0.058 [-5.197, 1.000], loss: 0.795972, mean_absolute_error: 0.919697, mean_q: 8.286772
 394089/1000000: episode: 1381, duration: 1.680s, episode steps: 215, steps per second: 128, episode reward: -22.465, mean reward: -0.104 [-10.768, 0.141], mean action: -0.814 [-1.159, 1.068], mean observation: -0.096 [-4.825, 1.000], loss: 0.872966, mean_absolute_error: 0.943871, mean_q: 8.182275
 394343/1000000: episode: 1382, duration: 1.981s, episode steps: 254, steps per second: 128, episode reward: -7.100, mean reward: -0.028 [-11.162, 0.377], mean action: -0.872 [-1.169, 1.006], mean observation: -0.155 [-4.936, 1.000], loss: 0.802832, mean_absolute_error: 0.930650, mean_q: 8.181027
 394645/1000000: episode: 1383, duration: 2.298s, episode steps: 302, steps per second: 131, episode reward: -5.198, mean reward: -0.017 [-11.144, 0.313], mean action: -0.884 [-1.169, 1.021], mean observation: -0.139 [-4.938, 1.000], loss: 0.839404, mean_absolute_error: 0.931299, mean_q: 8.096004
 394799/1000000: episode: 1384, duration: 1.165s, episode steps: 154, steps per second: 132, episode reward: 21.275, mean reward: 0.138 [-9.890, 0.221], mean action: -0.813 [-1.131, 1.078], mean observation: 0.020 [-4.484, 1.000], loss: 0.815681, mean_absolute_error: 0.934664, mean_q: 8.229544
 395097/1000000: episode: 1385, duration: 2.236s, episode steps: 298, steps per second: 133, episode reward: 12.757, mean reward: 0.043 [-11.078, 0.384], mean action: -0.858 [-1.066, 1.063], mean observation: -0.151 [-4.618, 1.000], loss: 0.817640, mean_absolute_error: 0.935926, mean_q: 8.138938
 395394/1000000: episode: 1386, duration: 2.280s, episode steps: 297, steps per second: 130, episode reward: -60.377, mean reward: -0.203 [-11.459, 0.249], mean action: -0.867 [-1.164, 1.064], mean observation: -0.189 [-5.190, 1.000], loss: 0.827915, mean_absolute_error: 0.939351, mean_q: 8.222356
 395699/1000000: episode: 1387, duration: 2.315s, episode steps: 305, steps per second: 132, episode reward: -24.921, mean reward: -0.082 [-11.427, 0.393], mean action: -0.866 [-1.155, 1.046], mean observation: -0.179 [-5.307, 1.347], loss: 0.842669, mean_absolute_error: 0.932283, mean_q: 8.195033
 395971/1000000: episode: 1388, duration: 2.034s, episode steps: 272, steps per second: 134, episode reward: 14.843, mean reward: 0.055 [-10.349, 0.222], mean action: -0.924 [-1.129, 0.960], mean observation: -0.031 [-3.012, 1.000], loss: 0.813094, mean_absolute_error: 0.930327, mean_q: 8.229280
 396605/1000000: episode: 1389, duration: 4.800s, episode steps: 634, steps per second: 132, episode reward: 70.814, mean reward: 0.112 [-10.422, 0.271], mean action: -0.977 [-1.131, -0.850], mean observation: 0.036 [-1.963, 1.000], loss: 0.809643, mean_absolute_error: 0.931132, mean_q: 8.107184
 397382/1000000: episode: 1390, duration: 5.822s, episode steps: 777, steps per second: 133, episode reward: 313.389, mean reward: 0.403 [-10.347, 0.597], mean action: -0.994 [-1.151, -0.861], mean observation: 0.067 [-2.107, 1.096], loss: 0.825569, mean_absolute_error: 0.935938, mean_q: 8.145185
 398128/1000000: episode: 1391, duration: 5.658s, episode steps: 746, steps per second: 132, episode reward: 72.485, mean reward: 0.097 [-10.570, 0.244], mean action: -1.008 [-1.164, -0.888], mean observation: 0.012 [-2.020, 1.000], loss: 0.832355, mean_absolute_error: 0.937497, mean_q: 8.200920
 398634/1000000: episode: 1392, duration: 3.901s, episode steps: 506, steps per second: 130, episode reward: 52.106, mean reward: 0.103 [-10.360, 0.234], mean action: -1.002 [-1.125, -0.883], mean observation: 0.230 [-0.453, 1.879], loss: 0.853981, mean_absolute_error: 0.937235, mean_q: 8.137588
 399392/1000000: episode: 1393, duration: 5.835s, episode steps: 758, steps per second: 130, episode reward: 127.110, mean reward: 0.168 [-10.137, 0.299], mean action: -1.009 [-1.152, -0.852], mean observation: 0.073 [-2.497, 1.000], loss: 0.825022, mean_absolute_error: 0.933163, mean_q: 8.146804
 399769/1000000: episode: 1394, duration: 2.962s, episode steps: 377, steps per second: 127, episode reward: 43.947, mean reward: 0.117 [-9.891, 0.153], mean action: -1.012 [-1.145, -0.911], mean observation: 0.085 [-0.668, 1.000], loss: 0.831834, mean_absolute_error: 0.938572, mean_q: 8.171168
 400619/1000000: episode: 1395, duration: 6.576s, episode steps: 850, steps per second: 129, episode reward: 157.322, mean reward: 0.185 [-10.400, 0.334], mean action: -0.995 [-1.129, -0.822], mean observation: 0.095 [-1.359, 1.306], loss: 0.860362, mean_absolute_error: 0.947579, mean_q: 8.195827
 401379/1000000: episode: 1396, duration: 5.696s, episode steps: 760, steps per second: 133, episode reward: 121.272, mean reward: 0.160 [-10.229, 0.251], mean action: -0.997 [-1.142, -0.869], mean observation: 0.046 [-1.597, 1.000], loss: 0.856781, mean_absolute_error: 0.950166, mean_q: 8.207900
 402078/1000000: episode: 1397, duration: 5.270s, episode steps: 699, steps per second: 133, episode reward: 165.386, mean reward: 0.237 [-10.243, 0.374], mean action: -1.002 [-1.130, -0.843], mean observation: 0.077 [-1.490, 1.000], loss: 0.825984, mean_absolute_error: 0.942590, mean_q: 8.256776
 402833/1000000: episode: 1398, duration: 5.718s, episode steps: 755, steps per second: 132, episode reward: 197.618, mean reward: 0.262 [-10.470, 0.540], mean action: -1.003 [-1.188, -0.853], mean observation: 0.111 [-1.428, 1.000], loss: 0.835203, mean_absolute_error: 0.946395, mean_q: 8.284232
 403470/1000000: episode: 1399, duration: 4.821s, episode steps: 637, steps per second: 132, episode reward: 67.897, mean reward: 0.107 [-10.408, 0.223], mean action: -0.991 [-1.137, -0.830], mean observation: 0.017 [-1.738, 1.000], loss: 0.895097, mean_absolute_error: 0.957721, mean_q: 8.290228
 404134/1000000: episode: 1400, duration: 5.021s, episode steps: 664, steps per second: 132, episode reward: 211.790, mean reward: 0.319 [-10.574, 0.546], mean action: -1.007 [-1.166, -0.851], mean observation: 0.152 [-1.656, 2.049], loss: 0.857186, mean_absolute_error: 0.947837, mean_q: 8.262771
 405057/1000000: episode: 1401, duration: 7.114s, episode steps: 923, steps per second: 130, episode reward: 198.720, mean reward: 0.215 [-10.518, 0.379], mean action: -1.007 [-1.153, -0.862], mean observation: 0.216 [-0.281, 1.662], loss: 0.881836, mean_absolute_error: 0.957578, mean_q: 8.298677
 405794/1000000: episode: 1402, duration: 5.772s, episode steps: 737, steps per second: 128, episode reward: 194.942, mean reward: 0.265 [-10.523, 0.459], mean action: -1.004 [-1.144, -0.888], mean observation: 0.110 [-1.495, 1.969], loss: 0.880980, mean_absolute_error: 0.956536, mean_q: 8.381842
 406500/1000000: episode: 1403, duration: 5.430s, episode steps: 706, steps per second: 130, episode reward: 66.939, mean reward: 0.095 [-10.411, 0.254], mean action: -0.995 [-1.134, -0.862], mean observation: 0.156 [-0.783, 1.168], loss: 0.875295, mean_absolute_error: 0.963872, mean_q: 8.369349
 407151/1000000: episode: 1404, duration: 4.850s, episode steps: 651, steps per second: 134, episode reward: 76.231, mean reward: 0.117 [-10.544, 0.358], mean action: -1.001 [-1.124, -0.865], mean observation: 0.014 [-1.417, 1.000], loss: 0.849497, mean_absolute_error: 0.955320, mean_q: 8.349843
 408028/1000000: episode: 1405, duration: 6.536s, episode steps: 877, steps per second: 134, episode reward: 143.433, mean reward: 0.164 [-10.601, 0.301], mean action: -1.003 [-1.171, -0.835], mean observation: 0.136 [-2.264, 1.053], loss: 0.865623, mean_absolute_error: 0.958445, mean_q: 8.442444
 408515/1000000: episode: 1406, duration: 3.634s, episode steps: 487, steps per second: 134, episode reward: -21.552, mean reward: -0.044 [-10.448, 0.099], mean action: -0.999 [-1.146, -0.875], mean observation: 0.245 [-0.239, 1.563], loss: 0.899895, mean_absolute_error: 0.970291, mean_q: 8.466886
 409220/1000000: episode: 1407, duration: 5.365s, episode steps: 705, steps per second: 131, episode reward: 216.660, mean reward: 0.307 [-10.477, 0.522], mean action: -1.010 [-1.148, -0.872], mean observation: 0.234 [-0.312, 1.847], loss: 0.887968, mean_absolute_error: 0.962128, mean_q: 8.397082
 410083/1000000: episode: 1408, duration: 6.716s, episode steps: 863, steps per second: 128, episode reward: 253.387, mean reward: 0.294 [-10.541, 0.533], mean action: -1.002 [-1.202, -0.854], mean observation: 0.120 [-1.465, 1.000], loss: 0.866021, mean_absolute_error: 0.962574, mean_q: 8.459187
 410732/1000000: episode: 1409, duration: 4.824s, episode steps: 649, steps per second: 135, episode reward: -59.477, mean reward: -0.092 [-10.339, 0.013], mean action: -0.988 [-1.090, -0.854], mean observation: 0.179 [-0.573, 1.000], loss: 0.866750, mean_absolute_error: 0.954398, mean_q: 8.361794
 411418/1000000: episode: 1410, duration: 5.187s, episode steps: 686, steps per second: 132, episode reward: 168.062, mean reward: 0.245 [-10.595, 0.431], mean action: -0.997 [-1.139, -0.862], mean observation: 0.149 [-1.524, 2.146], loss: 0.862874, mean_absolute_error: 0.957379, mean_q: 8.489985
 412091/1000000: episode: 1411, duration: 5.073s, episode steps: 673, steps per second: 133, episode reward: 162.298, mean reward: 0.241 [-10.804, 0.526], mean action: -0.999 [-1.177, -0.816], mean observation: 0.126 [-2.012, 1.953], loss: 0.894012, mean_absolute_error: 0.965045, mean_q: 8.454555
 412789/1000000: episode: 1412, duration: 5.242s, episode steps: 698, steps per second: 133, episode reward: 232.482, mean reward: 0.333 [-10.222, 0.510], mean action: -1.004 [-1.170, -0.845], mean observation: 0.198 [-0.477, 1.381], loss: 0.858867, mean_absolute_error: 0.949819, mean_q: 8.466537
 413577/1000000: episode: 1413, duration: 5.966s, episode steps: 788, steps per second: 132, episode reward: 218.497, mean reward: 0.277 [-10.310, 0.433], mean action: -0.991 [-1.169, -0.855], mean observation: 0.065 [-1.976, 1.000], loss: 0.901946, mean_absolute_error: 0.962078, mean_q: 8.532118
 414238/1000000: episode: 1414, duration: 4.977s, episode steps: 661, steps per second: 133, episode reward: 203.253, mean reward: 0.307 [-10.474, 0.548], mean action: -1.002 [-1.148, -0.851], mean observation: 0.248 [-0.296, 1.984], loss: 0.891003, mean_absolute_error: 0.959743, mean_q: 8.555901
 414442/1000000: episode: 1415, duration: 1.542s, episode steps: 204, steps per second: 132, episode reward: 21.328, mean reward: 0.105 [-9.846, 0.154], mean action: -1.001 [-1.126, -0.920], mean observation: 0.119 [-0.339, 1.000], loss: 0.826110, mean_absolute_error: 0.953284, mean_q: 8.711382
 415062/1000000: episode: 1416, duration: 4.777s, episode steps: 620, steps per second: 130, episode reward: 45.023, mean reward: 0.073 [-10.392, 0.159], mean action: -0.994 [-1.156, -0.844], mean observation: 0.123 [-1.131, 2.090], loss: 0.922320, mean_absolute_error: 0.958990, mean_q: 8.650231
 416077/1000000: episode: 1417, duration: 7.660s, episode steps: 1015, steps per second: 132, episode reward: 114.781, mean reward: 0.113 [-10.251, 0.181], mean action: -1.010 [-1.159, -0.884], mean observation: 0.083 [-1.201, 1.000], loss: 0.844803, mean_absolute_error: 0.950181, mean_q: 8.640863
 416813/1000000: episode: 1418, duration: 5.516s, episode steps: 736, steps per second: 133, episode reward: -54.382, mean reward: -0.074 [-10.605, 0.108], mean action: -0.996 [-1.133, -0.873], mean observation: 0.137 [-1.137, 1.000], loss: 0.875589, mean_absolute_error: 0.958556, mean_q: 8.597497
 417771/1000000: episode: 1419, duration: 7.750s, episode steps: 958, steps per second: 124, episode reward: 136.769, mean reward: 0.143 [-10.584, 0.340], mean action: -0.997 [-1.149, -0.831], mean observation: 0.141 [-0.787, 1.696], loss: 0.879908, mean_absolute_error: 0.959439, mean_q: 8.693508
 418544/1000000: episode: 1420, duration: 6.105s, episode steps: 773, steps per second: 127, episode reward: 88.054, mean reward: 0.114 [-10.703, 0.410], mean action: -0.988 [-1.117, -0.812], mean observation: 0.109 [-2.197, 1.890], loss: 0.869382, mean_absolute_error: 0.955569, mean_q: 8.707278
 419260/1000000: episode: 1421, duration: 5.721s, episode steps: 716, steps per second: 125, episode reward: 182.265, mean reward: 0.255 [-10.505, 0.514], mean action: -0.999 [-1.189, -0.824], mean observation: 0.049 [-2.807, 1.000], loss: 0.888310, mean_absolute_error: 0.956919, mean_q: 8.696794
 419866/1000000: episode: 1422, duration: 4.636s, episode steps: 606, steps per second: 131, episode reward: 130.267, mean reward: 0.215 [-10.242, 0.309], mean action: -0.999 [-1.155, -0.885], mean observation: 0.220 [-0.420, 1.775], loss: 0.865988, mean_absolute_error: 0.947979, mean_q: 8.640779
 420469/1000000: episode: 1423, duration: 4.516s, episode steps: 603, steps per second: 134, episode reward: 149.347, mean reward: 0.248 [-10.291, 0.400], mean action: -0.996 [-1.110, -0.852], mean observation: 0.182 [-0.949, 1.689], loss: 0.851609, mean_absolute_error: 0.952720, mean_q: 8.689411
 421309/1000000: episode: 1424, duration: 6.230s, episode steps: 840, steps per second: 135, episode reward: 228.658, mean reward: 0.272 [-10.521, 0.486], mean action: -0.997 [-1.166, -0.836], mean observation: 0.111 [-1.918, 1.000], loss: 0.823034, mean_absolute_error: 0.944553, mean_q: 8.762965
 421786/1000000: episode: 1425, duration: 3.626s, episode steps: 477, steps per second: 132, episode reward: 68.225, mean reward: 0.143 [-9.806, 0.211], mean action: -0.996 [-1.135, -0.804], mean observation: 0.121 [-0.882, 1.813], loss: 0.875187, mean_absolute_error: 0.952834, mean_q: 8.808024
 422933/1000000: episode: 1426, duration: 8.601s, episode steps: 1147, steps per second: 133, episode reward: 375.175, mean reward: 0.327 [-10.291, 0.668], mean action: -0.984 [-1.146, -0.842], mean observation: 0.181 [-0.715, 2.094], loss: 0.829229, mean_absolute_error: 0.945771, mean_q: 8.863649
 424083/1000000: episode: 1427, duration: 8.612s, episode steps: 1150, steps per second: 134, episode reward: 279.482, mean reward: 0.243 [-10.415, 0.392], mean action: -0.997 [-1.196, -0.850], mean observation: 0.074 [-1.217, 1.000], loss: 0.858309, mean_absolute_error: 0.947368, mean_q: 8.895676
 424949/1000000: episode: 1428, duration: 6.683s, episode steps: 866, steps per second: 130, episode reward: 175.520, mean reward: 0.203 [-10.551, 0.391], mean action: -1.003 [-1.146, -0.871], mean observation: 0.034 [-1.812, 1.000], loss: 0.879826, mean_absolute_error: 0.948392, mean_q: 8.932860
 425914/1000000: episode: 1429, duration: 7.296s, episode steps: 965, steps per second: 132, episode reward: 363.537, mean reward: 0.377 [-10.354, 0.511], mean action: -1.005 [-1.162, -0.871], mean observation: 0.077 [-2.134, 1.000], loss: 0.856952, mean_absolute_error: 0.942181, mean_q: 8.935359
 426485/1000000: episode: 1430, duration: 4.244s, episode steps: 571, steps per second: 135, episode reward: 61.132, mean reward: 0.107 [-10.407, 0.255], mean action: -0.988 [-1.139, -0.825], mean observation: 0.161 [-1.232, 1.619], loss: 0.868889, mean_absolute_error: 0.943959, mean_q: 8.930324
 427108/1000000: episode: 1431, duration: 4.775s, episode steps: 623, steps per second: 130, episode reward: 61.323, mean reward: 0.098 [-10.309, 0.221], mean action: -0.993 [-1.113, -0.870], mean observation: 0.147 [-1.293, 1.260], loss: 0.818033, mean_absolute_error: 0.933243, mean_q: 9.022855
 428196/1000000: episode: 1432, duration: 8.108s, episode steps: 1088, steps per second: 134, episode reward: 255.556, mean reward: 0.235 [-10.364, 0.558], mean action: -0.992 [-1.179, -0.846], mean observation: 0.078 [-1.294, 1.000], loss: 0.852897, mean_absolute_error: 0.942291, mean_q: 9.104158
 428777/1000000: episode: 1433, duration: 4.314s, episode steps: 581, steps per second: 135, episode reward: 20.749, mean reward: 0.036 [-10.637, 0.228], mean action: -0.992 [-1.151, -0.831], mean observation: 0.155 [-1.088, 2.004], loss: 0.837172, mean_absolute_error: 0.938402, mean_q: 9.078486
 429665/1000000: episode: 1434, duration: 6.976s, episode steps: 888, steps per second: 127, episode reward: 298.219, mean reward: 0.336 [-10.320, 0.463], mean action: -1.003 [-1.160, -0.859], mean observation: 0.076 [-1.912, 1.200], loss: 0.874901, mean_absolute_error: 0.944981, mean_q: 9.135330
 430638/1000000: episode: 1435, duration: 7.345s, episode steps: 973, steps per second: 132, episode reward: 129.961, mean reward: 0.134 [-10.040, 0.200], mean action: -0.998 [-1.143, -0.839], mean observation: 0.131 [-0.668, 1.200], loss: 0.883211, mean_absolute_error: 0.941167, mean_q: 9.152992
 431143/1000000: episode: 1436, duration: 3.921s, episode steps: 505, steps per second: 129, episode reward: 108.301, mean reward: 0.214 [-9.824, 0.243], mean action: -1.009 [-1.109, -0.842], mean observation: 0.083 [-1.088, 1.000], loss: 0.861201, mean_absolute_error: 0.940678, mean_q: 9.171431
 431803/1000000: episode: 1437, duration: 5.124s, episode steps: 660, steps per second: 129, episode reward: 48.781, mean reward: 0.074 [-10.278, 0.198], mean action: -0.990 [-1.099, -0.812], mean observation: 0.066 [-1.204, 1.000], loss: 0.832174, mean_absolute_error: 0.934681, mean_q: 9.184929
 432908/1000000: episode: 1438, duration: 8.348s, episode steps: 1105, steps per second: 132, episode reward: 196.061, mean reward: 0.177 [-10.340, 0.307], mean action: -1.006 [-1.158, -0.882], mean observation: 0.090 [-2.044, 1.000], loss: 0.876126, mean_absolute_error: 0.942822, mean_q: 9.226952
 433655/1000000: episode: 1439, duration: 5.859s, episode steps: 747, steps per second: 127, episode reward: 98.846, mean reward: 0.132 [-9.939, 0.167], mean action: -0.996 [-1.142, -0.858], mean observation: 0.085 [-1.417, 1.000], loss: 0.876340, mean_absolute_error: 0.943621, mean_q: 9.228368
 434211/1000000: episode: 1440, duration: 4.830s, episode steps: 556, steps per second: 115, episode reward: 51.542, mean reward: 0.093 [-10.415, 0.227], mean action: -1.013 [-1.164, -0.869], mean observation: 0.046 [-2.074, 1.000], loss: 0.860241, mean_absolute_error: 0.934339, mean_q: 9.244995
 434969/1000000: episode: 1441, duration: 5.722s, episode steps: 758, steps per second: 132, episode reward: 226.187, mean reward: 0.298 [-10.499, 0.476], mean action: -0.994 [-1.136, -0.855], mean observation: 0.175 [-0.781, 2.459], loss: 0.862529, mean_absolute_error: 0.941636, mean_q: 9.335709
 435561/1000000: episode: 1442, duration: 4.529s, episode steps: 592, steps per second: 131, episode reward: 116.408, mean reward: 0.197 [-9.839, 0.292], mean action: -1.000 [-1.145, -0.852], mean observation: 0.188 [-0.398, 1.433], loss: 0.880233, mean_absolute_error: 0.944962, mean_q: 9.289554
 436318/1000000: episode: 1443, duration: 5.681s, episode steps: 757, steps per second: 133, episode reward: 106.640, mean reward: 0.141 [-10.353, 0.261], mean action: -1.008 [-1.148, -0.869], mean observation: 0.211 [-0.254, 1.496], loss: 0.845976, mean_absolute_error: 0.933671, mean_q: 9.379031
 437276/1000000: episode: 1444, duration: 7.156s, episode steps: 958, steps per second: 134, episode reward: 333.139, mean reward: 0.348 [-10.544, 0.657], mean action: -0.997 [-1.137, -0.841], mean observation: 0.058 [-2.265, 1.747], loss: 0.858490, mean_absolute_error: 0.940035, mean_q: 9.467457
 437816/1000000: episode: 1445, duration: 4.062s, episode steps: 540, steps per second: 133, episode reward: 88.687, mean reward: 0.164 [-10.365, 0.320], mean action: -0.996 [-1.129, -0.866], mean observation: 0.034 [-2.026, 1.000], loss: 0.854092, mean_absolute_error: 0.940081, mean_q: 9.495993
 438722/1000000: episode: 1446, duration: 6.975s, episode steps: 906, steps per second: 130, episode reward: 211.072, mean reward: 0.233 [-10.180, 0.349], mean action: -0.988 [-1.104, -0.854], mean observation: 0.074 [-2.254, 1.000], loss: 0.822803, mean_absolute_error: 0.936146, mean_q: 9.492427
 439529/1000000: episode: 1447, duration: 6.211s, episode steps: 807, steps per second: 130, episode reward: 190.108, mean reward: 0.236 [-10.167, 0.336], mean action: -0.996 [-1.151, -0.865], mean observation: 0.051 [-1.865, 1.000], loss: 0.905439, mean_absolute_error: 0.945127, mean_q: 9.537255
 440195/1000000: episode: 1448, duration: 5.091s, episode steps: 666, steps per second: 131, episode reward: 16.123, mean reward: 0.024 [-10.428, 0.162], mean action: -1.008 [-1.139, -0.891], mean observation: 0.035 [-1.509, 1.000], loss: 0.865297, mean_absolute_error: 0.938101, mean_q: 9.552862
 441223/1000000: episode: 1449, duration: 7.840s, episode steps: 1028, steps per second: 131, episode reward: 230.488, mean reward: 0.224 [-10.632, 0.422], mean action: -0.999 [-1.125, -0.861], mean observation: 0.146 [-0.899, 2.437], loss: 0.851418, mean_absolute_error: 0.936120, mean_q: 9.594200
 441964/1000000: episode: 1450, duration: 5.855s, episode steps: 741, steps per second: 127, episode reward: 306.087, mean reward: 0.413 [-10.351, 0.608], mean action: -1.001 [-1.099, -0.862], mean observation: 0.225 [-0.355, 1.986], loss: 0.840979, mean_absolute_error: 0.932599, mean_q: 9.639812
 442710/1000000: episode: 1451, duration: 5.791s, episode steps: 746, steps per second: 129, episode reward: -10.797, mean reward: -0.014 [-10.456, 0.120], mean action: -1.000 [-1.142, -0.882], mean observation: 0.209 [-0.580, 2.136], loss: 0.883399, mean_absolute_error: 0.939364, mean_q: 9.672771
 443043/1000000: episode: 1452, duration: 2.551s, episode steps: 333, steps per second: 131, episode reward: 87.781, mean reward: 0.264 [-9.727, 0.299], mean action: -1.001 [-1.103, -0.893], mean observation: 0.107 [-0.944, 1.000], loss: 0.883972, mean_absolute_error: 0.930324, mean_q: 9.696399
 444543/1000000: episode: 1453, duration: 11.498s, episode steps: 1500, steps per second: 130, episode reward: 512.716, mean reward: 0.342 [0.192, 0.470], mean action: -0.995 [-1.128, -0.836], mean observation: 0.130 [-0.453, 1.000], loss: 0.852481, mean_absolute_error: 0.933888, mean_q: 9.703906
 445335/1000000: episode: 1454, duration: 5.907s, episode steps: 792, steps per second: 134, episode reward: 187.712, mean reward: 0.237 [-10.722, 0.405], mean action: -1.007 [-1.164, -0.851], mean observation: 0.212 [-0.922, 2.847], loss: 0.890971, mean_absolute_error: 0.935666, mean_q: 9.723001
 446095/1000000: episode: 1455, duration: 5.647s, episode steps: 760, steps per second: 135, episode reward: 116.556, mean reward: 0.153 [-10.646, 0.322], mean action: -0.994 [-1.120, -0.850], mean observation: 0.150 [-1.012, 2.244], loss: 0.837763, mean_absolute_error: 0.926065, mean_q: 9.795811
 446918/1000000: episode: 1456, duration: 6.290s, episode steps: 823, steps per second: 131, episode reward: 144.419, mean reward: 0.175 [-10.498, 0.360], mean action: -1.025 [-1.188, -0.888], mean observation: 0.032 [-2.140, 1.000], loss: 0.845584, mean_absolute_error: 0.924088, mean_q: 9.855083
 447901/1000000: episode: 1457, duration: 7.367s, episode steps: 983, steps per second: 133, episode reward: 91.422, mean reward: 0.093 [-10.403, 0.239], mean action: -0.995 [-1.099, -0.822], mean observation: 0.199 [-0.510, 1.911], loss: 0.856878, mean_absolute_error: 0.925500, mean_q: 9.852596
 448690/1000000: episode: 1458, duration: 5.964s, episode steps: 789, steps per second: 132, episode reward: 431.268, mean reward: 0.547 [-10.337, 0.750], mean action: -0.994 [-1.140, -0.842], mean observation: 0.083 [-2.254, 1.132], loss: 0.824590, mean_absolute_error: 0.918082, mean_q: 9.928904
 449440/1000000: episode: 1459, duration: 5.654s, episode steps: 750, steps per second: 133, episode reward: 314.012, mean reward: 0.419 [-10.651, 0.711], mean action: -1.006 [-1.162, -0.874], mean observation: 0.151 [-1.328, 2.284], loss: 0.837764, mean_absolute_error: 0.921515, mean_q: 9.929890
 450109/1000000: episode: 1460, duration: 5.069s, episode steps: 669, steps per second: 132, episode reward: 108.101, mean reward: 0.162 [-10.406, 0.338], mean action: -1.003 [-1.132, -0.883], mean observation: 0.106 [-1.288, 1.425], loss: 0.856685, mean_absolute_error: 0.932531, mean_q: 10.029148
 450913/1000000: episode: 1461, duration: 6.044s, episode steps: 804, steps per second: 133, episode reward: 71.415, mean reward: 0.089 [-10.721, 0.305], mean action: -0.993 [-1.124, -0.862], mean observation: 0.131 [-1.060, 1.819], loss: 0.844855, mean_absolute_error: 0.924776, mean_q: 10.041649
 451221/1000000: episode: 1462, duration: 2.334s, episode steps: 308, steps per second: 132, episode reward: 44.878, mean reward: 0.146 [-9.878, 0.192], mean action: -0.969 [-1.066, -0.806], mean observation: 0.059 [-1.350, 1.000], loss: 0.894812, mean_absolute_error: 0.933059, mean_q: 10.046209
 452040/1000000: episode: 1463, duration: 6.156s, episode steps: 819, steps per second: 133, episode reward: 109.406, mean reward: 0.134 [-10.317, 0.269], mean action: -1.003 [-1.154, -0.882], mean observation: 0.208 [-0.190, 1.317], loss: 0.835376, mean_absolute_error: 0.916767, mean_q: 10.040920
 452837/1000000: episode: 1464, duration: 5.999s, episode steps: 797, steps per second: 133, episode reward: 148.852, mean reward: 0.187 [-10.580, 0.356], mean action: -1.009 [-1.171, -0.892], mean observation: 0.040 [-2.770, 1.000], loss: 0.818895, mean_absolute_error: 0.914244, mean_q: 10.087074
 454148/1000000: episode: 1465, duration: 9.927s, episode steps: 1311, steps per second: 132, episode reward: 319.472, mean reward: 0.244 [-10.520, 0.379], mean action: -0.988 [-1.140, -0.856], mean observation: 0.183 [-0.504, 2.062], loss: 0.856618, mean_absolute_error: 0.916236, mean_q: 10.173674
 454716/1000000: episode: 1466, duration: 4.278s, episode steps: 568, steps per second: 133, episode reward: 12.476, mean reward: 0.022 [-10.547, 0.178], mean action: -0.992 [-1.204, -0.874], mean observation: 0.237 [-0.472, 1.948], loss: 0.846376, mean_absolute_error: 0.921514, mean_q: 10.137030
 455409/1000000: episode: 1467, duration: 5.290s, episode steps: 693, steps per second: 131, episode reward: 180.447, mean reward: 0.260 [-10.269, 0.430], mean action: -0.999 [-1.161, -0.864], mean observation: 0.216 [-0.455, 1.318], loss: 0.870556, mean_absolute_error: 0.922831, mean_q: 10.239537
 456349/1000000: episode: 1468, duration: 7.552s, episode steps: 940, steps per second: 124, episode reward: 190.838, mean reward: 0.203 [-10.150, 0.283], mean action: -1.006 [-1.138, -0.860], mean observation: 0.141 [-1.859, 2.190], loss: 0.843311, mean_absolute_error: 0.917784, mean_q: 10.261951
 457088/1000000: episode: 1469, duration: 5.637s, episode steps: 739, steps per second: 131, episode reward: -83.457, mean reward: -0.113 [-10.472, 0.014], mean action: -1.009 [-1.196, -0.886], mean observation: 0.093 [-1.111, 1.000], loss: 0.807600, mean_absolute_error: 0.904925, mean_q: 10.310701
 457948/1000000: episode: 1470, duration: 6.637s, episode steps: 860, steps per second: 130, episode reward: 211.682, mean reward: 0.246 [-10.673, 0.488], mean action: -0.997 [-1.153, -0.873], mean observation: 0.129 [-1.566, 1.608], loss: 0.869783, mean_absolute_error: 0.911788, mean_q: 10.360070
 458597/1000000: episode: 1471, duration: 4.918s, episode steps: 649, steps per second: 132, episode reward: 134.994, mean reward: 0.208 [-10.530, 0.452], mean action: -0.999 [-1.140, -0.865], mean observation: 0.260 [-0.264, 1.578], loss: 0.860842, mean_absolute_error: 0.910845, mean_q: 10.359962
 458928/1000000: episode: 1472, duration: 2.529s, episode steps: 331, steps per second: 131, episode reward: 63.916, mean reward: 0.193 [-9.779, 0.225], mean action: -1.011 [-1.113, -0.907], mean observation: 0.120 [-0.298, 1.000], loss: 0.855471, mean_absolute_error: 0.904242, mean_q: 10.385185
 459759/1000000: episode: 1473, duration: 6.349s, episode steps: 831, steps per second: 131, episode reward: 86.022, mean reward: 0.104 [-10.549, 0.221], mean action: -1.012 [-1.146, -0.848], mean observation: 0.211 [-0.531, 1.977], loss: 0.814665, mean_absolute_error: 0.902189, mean_q: 10.395223
 460122/1000000: episode: 1474, duration: 2.742s, episode steps: 363, steps per second: 132, episode reward: 58.164, mean reward: 0.160 [-9.853, 0.198], mean action: -0.992 [-1.124, -0.869], mean observation: 0.094 [-0.741, 1.000], loss: 0.832467, mean_absolute_error: 0.900662, mean_q: 10.404008
 460970/1000000: episode: 1475, duration: 6.504s, episode steps: 848, steps per second: 130, episode reward: 197.692, mean reward: 0.233 [-10.804, 0.509], mean action: -1.012 [-1.160, -0.882], mean observation: 0.131 [-2.022, 1.919], loss: 0.831392, mean_absolute_error: 0.900167, mean_q: 10.439640
 461822/1000000: episode: 1476, duration: 6.624s, episode steps: 852, steps per second: 129, episode reward: 289.618, mean reward: 0.340 [-10.422, 0.473], mean action: -0.995 [-1.151, -0.831], mean observation: 0.056 [-2.415, 1.000], loss: 0.821954, mean_absolute_error: 0.900860, mean_q: 10.431964
 462761/1000000: episode: 1477, duration: 7.512s, episode steps: 939, steps per second: 125, episode reward: 211.457, mean reward: 0.225 [-10.273, 0.357], mean action: -1.003 [-1.163, -0.847], mean observation: 0.165 [-1.201, 1.000], loss: 0.841034, mean_absolute_error: 0.898730, mean_q: 10.536936
 463357/1000000: episode: 1478, duration: 4.494s, episode steps: 596, steps per second: 133, episode reward: -15.654, mean reward: -0.026 [-10.347, 0.079], mean action: -0.982 [-1.169, -0.818], mean observation: 0.090 [-1.473, 1.000], loss: 0.860973, mean_absolute_error: 0.897589, mean_q: 10.530536
 463971/1000000: episode: 1479, duration: 4.563s, episode steps: 614, steps per second: 135, episode reward: 210.633, mean reward: 0.343 [-10.321, 0.550], mean action: -1.002 [-1.134, -0.850], mean observation: 0.208 [-1.135, 2.023], loss: 0.835779, mean_absolute_error: 0.896820, mean_q: 10.548335
 464772/1000000: episode: 1480, duration: 6.137s, episode steps: 801, steps per second: 131, episode reward: 212.045, mean reward: 0.265 [-10.552, 0.438], mean action: -1.003 [-1.142, -0.781], mean observation: 0.229 [-0.462, 2.151], loss: 0.826984, mean_absolute_error: 0.894711, mean_q: 10.552451
 465521/1000000: episode: 1481, duration: 5.731s, episode steps: 749, steps per second: 131, episode reward: 263.335, mean reward: 0.352 [-10.328, 0.558], mean action: -0.998 [-1.182, -0.870], mean observation: 0.085 [-1.703, 2.270], loss: 0.827502, mean_absolute_error: 0.894591, mean_q: 10.637616
 466243/1000000: episode: 1482, duration: 5.447s, episode steps: 722, steps per second: 133, episode reward: 150.802, mean reward: 0.209 [-10.611, 0.389], mean action: -1.002 [-1.123, -0.866], mean observation: 0.127 [-1.801, 1.811], loss: 0.875689, mean_absolute_error: 0.900810, mean_q: 10.715438
 466993/1000000: episode: 1483, duration: 5.771s, episode steps: 750, steps per second: 130, episode reward: 134.795, mean reward: 0.180 [-10.323, 0.293], mean action: -0.994 [-1.155, -0.799], mean observation: 0.187 [-1.942, 1.658], loss: 0.831154, mean_absolute_error: 0.887814, mean_q: 10.673306
 467656/1000000: episode: 1484, duration: 5.063s, episode steps: 663, steps per second: 131, episode reward: -2.457, mean reward: -0.004 [-10.300, 0.085], mean action: -0.992 [-1.119, -0.842], mean observation: 0.209 [-0.457, 1.409], loss: 0.844625, mean_absolute_error: 0.891340, mean_q: 10.761789
 468479/1000000: episode: 1485, duration: 6.277s, episode steps: 823, steps per second: 131, episode reward: 208.393, mean reward: 0.253 [-10.618, 0.475], mean action: -1.001 [-1.146, -0.820], mean observation: 0.144 [-1.416, 2.293], loss: 0.835863, mean_absolute_error: 0.891547, mean_q: 10.757114
 469261/1000000: episode: 1486, duration: 5.995s, episode steps: 782, steps per second: 130, episode reward: 350.812, mean reward: 0.449 [-10.349, 0.713], mean action: -1.013 [-1.135, -0.858], mean observation: 0.203 [-0.429, 2.329], loss: 0.841571, mean_absolute_error: 0.891171, mean_q: 10.722386
 470002/1000000: episode: 1487, duration: 5.728s, episode steps: 741, steps per second: 129, episode reward: 155.567, mean reward: 0.210 [-10.222, 0.294], mean action: -1.004 [-1.177, -0.863], mean observation: 0.124 [-2.055, 1.539], loss: 0.851311, mean_absolute_error: 0.891930, mean_q: 10.787682
 470808/1000000: episode: 1488, duration: 6.141s, episode steps: 806, steps per second: 131, episode reward: 31.958, mean reward: 0.040 [-10.626, 0.236], mean action: -0.991 [-1.136, -0.847], mean observation: 0.025 [-1.236, 1.000], loss: 0.836189, mean_absolute_error: 0.886489, mean_q: 10.823282
 471959/1000000: episode: 1489, duration: 8.851s, episode steps: 1151, steps per second: 130, episode reward: 401.484, mean reward: 0.349 [-10.654, 0.572], mean action: -1.001 [-1.155, -0.862], mean observation: 0.095 [-2.793, 1.516], loss: 0.880014, mean_absolute_error: 0.894035, mean_q: 10.851179
 472840/1000000: episode: 1490, duration: 6.726s, episode steps: 881, steps per second: 131, episode reward: 171.471, mean reward: 0.195 [-10.497, 0.335], mean action: -0.993 [-1.134, -0.838], mean observation: 0.156 [-0.709, 2.118], loss: 0.804109, mean_absolute_error: 0.882348, mean_q: 10.836021
 473859/1000000: episode: 1491, duration: 7.570s, episode steps: 1019, steps per second: 135, episode reward: 325.907, mean reward: 0.320 [-10.422, 0.686], mean action: -0.999 [-1.131, -0.860], mean observation: 0.213 [-0.462, 1.461], loss: 0.857916, mean_absolute_error: 0.883940, mean_q: 10.888990
 474604/1000000: episode: 1492, duration: 5.663s, episode steps: 745, steps per second: 132, episode reward: 35.454, mean reward: 0.048 [-10.345, 0.169], mean action: -1.000 [-1.120, -0.862], mean observation: 0.184 [-0.616, 1.520], loss: 0.794532, mean_absolute_error: 0.867434, mean_q: 10.943078
 475353/1000000: episode: 1493, duration: 5.708s, episode steps: 749, steps per second: 131, episode reward: 218.534, mean reward: 0.292 [-10.370, 0.462], mean action: -1.003 [-1.157, -0.860], mean observation: 0.206 [-0.296, 1.910], loss: 0.870984, mean_absolute_error: 0.871070, mean_q: 10.972993
 476060/1000000: episode: 1494, duration: 5.368s, episode steps: 707, steps per second: 132, episode reward: 99.683, mean reward: 0.141 [-10.406, 0.243], mean action: -0.997 [-1.171, -0.825], mean observation: 0.069 [-2.235, 1.406], loss: 0.885384, mean_absolute_error: 0.880170, mean_q: 10.964522
 476904/1000000: episode: 1495, duration: 6.368s, episode steps: 844, steps per second: 133, episode reward: 205.892, mean reward: 0.244 [-10.922, 0.581], mean action: -0.998 [-1.151, -0.860], mean observation: 0.005 [-2.306, 1.000], loss: 0.829073, mean_absolute_error: 0.869081, mean_q: 11.004281
 477742/1000000: episode: 1496, duration: 6.401s, episode steps: 838, steps per second: 131, episode reward: 377.598, mean reward: 0.451 [-10.417, 0.623], mean action: -0.997 [-1.161, -0.820], mean observation: 0.074 [-2.466, 1.000], loss: 0.835611, mean_absolute_error: 0.874801, mean_q: 11.072335
 478415/1000000: episode: 1497, duration: 5.173s, episode steps: 673, steps per second: 130, episode reward: 27.954, mean reward: 0.042 [-9.835, 0.165], mean action: -1.009 [-1.168, -0.893], mean observation: 0.121 [-1.040, 1.000], loss: 0.847852, mean_absolute_error: 0.873483, mean_q: 11.108757
 479012/1000000: episode: 1498, duration: 4.693s, episode steps: 597, steps per second: 127, episode reward: 54.942, mean reward: 0.092 [-10.353, 0.221], mean action: -0.993 [-1.163, -0.844], mean observation: 0.031 [-1.540, 1.000], loss: 0.838352, mean_absolute_error: 0.871574, mean_q: 11.078265
 479858/1000000: episode: 1499, duration: 6.526s, episode steps: 846, steps per second: 130, episode reward: 33.836, mean reward: 0.040 [-10.495, 0.190], mean action: -0.997 [-1.146, -0.863], mean observation: 0.218 [-0.139, 1.168], loss: 0.841968, mean_absolute_error: 0.876538, mean_q: 11.145350
 480604/1000000: episode: 1500, duration: 5.660s, episode steps: 746, steps per second: 132, episode reward: 104.419, mean reward: 0.140 [-10.980, 0.555], mean action: -1.020 [-1.193, -0.878], mean observation: 0.135 [-1.915, 2.899], loss: 0.810043, mean_absolute_error: 0.866420, mean_q: 11.164662
 481330/1000000: episode: 1501, duration: 5.654s, episode steps: 726, steps per second: 128, episode reward: 168.742, mean reward: 0.232 [-10.326, 0.390], mean action: -1.002 [-1.172, -0.885], mean observation: 0.228 [-0.316, 1.594], loss: 0.794883, mean_absolute_error: 0.864868, mean_q: 11.137932
 482063/1000000: episode: 1502, duration: 5.554s, episode steps: 733, steps per second: 132, episode reward: 129.129, mean reward: 0.176 [-10.785, 0.390], mean action: -0.993 [-1.125, -0.862], mean observation: 0.238 [-0.740, 2.769], loss: 0.770281, mean_absolute_error: 0.861615, mean_q: 11.208138
 482739/1000000: episode: 1503, duration: 5.243s, episode steps: 676, steps per second: 129, episode reward: 104.555, mean reward: 0.155 [-10.410, 0.272], mean action: -0.993 [-1.105, -0.872], mean observation: 0.023 [-1.823, 1.000], loss: 0.829792, mean_absolute_error: 0.863051, mean_q: 11.321710
 483490/1000000: episode: 1504, duration: 5.846s, episode steps: 751, steps per second: 128, episode reward: 150.607, mean reward: 0.201 [-10.557, 0.397], mean action: -1.010 [-1.149, -0.901], mean observation: 0.096 [-2.248, 1.000], loss: 0.815003, mean_absolute_error: 0.873148, mean_q: 11.297133
 484126/1000000: episode: 1505, duration: 4.794s, episode steps: 636, steps per second: 133, episode reward: -27.468, mean reward: -0.043 [-10.625, 0.118], mean action: -1.003 [-1.188, -0.840], mean observation: 0.010 [-1.574, 1.000], loss: 0.834166, mean_absolute_error: 0.876955, mean_q: 11.315258
 484923/1000000: episode: 1506, duration: 6.146s, episode steps: 797, steps per second: 130, episode reward: 67.043, mean reward: 0.084 [-10.661, 0.308], mean action: -0.994 [-1.125, -0.861], mean observation: 0.127 [-1.709, 1.104], loss: 0.851503, mean_absolute_error: 0.875378, mean_q: 11.416189
 485629/1000000: episode: 1507, duration: 5.362s, episode steps: 706, steps per second: 132, episode reward: 66.479, mean reward: 0.094 [-10.381, 0.240], mean action: -1.012 [-1.194, -0.867], mean observation: 0.066 [-1.785, 1.000], loss: 0.866843, mean_absolute_error: 0.876908, mean_q: 11.372630
 486299/1000000: episode: 1508, duration: 5.108s, episode steps: 670, steps per second: 131, episode reward: 175.544, mean reward: 0.262 [-10.012, 0.336], mean action: -0.996 [-1.143, -0.830], mean observation: 0.077 [-1.370, 1.000], loss: 0.832528, mean_absolute_error: 0.875434, mean_q: 11.350955
 487239/1000000: episode: 1509, duration: 7.190s, episode steps: 940, steps per second: 131, episode reward: 246.567, mean reward: 0.262 [-10.771, 0.490], mean action: -1.010 [-1.168, -0.876], mean observation: 0.032 [-2.722, 1.000], loss: 0.863320, mean_absolute_error: 0.884120, mean_q: 11.459682
 488165/1000000: episode: 1510, duration: 7.142s, episode steps: 926, steps per second: 130, episode reward: 351.209, mean reward: 0.379 [-10.380, 0.537], mean action: -1.006 [-1.185, -0.869], mean observation: 0.094 [-2.080, 1.000], loss: 0.815502, mean_absolute_error: 0.877082, mean_q: 11.531946
 489011/1000000: episode: 1511, duration: 6.401s, episode steps: 846, steps per second: 132, episode reward: 141.131, mean reward: 0.167 [-10.381, 0.298], mean action: -0.999 [-1.157, -0.848], mean observation: 0.052 [-1.477, 1.000], loss: 0.876911, mean_absolute_error: 0.876355, mean_q: 11.512471
 489783/1000000: episode: 1512, duration: 5.921s, episode steps: 772, steps per second: 130, episode reward: 142.413, mean reward: 0.184 [-10.339, 0.335], mean action: -1.014 [-1.136, -0.876], mean observation: 0.168 [-0.540, 1.480], loss: 0.841521, mean_absolute_error: 0.874183, mean_q: 11.606483
 490037/1000000: episode: 1513, duration: 1.907s, episode steps: 254, steps per second: 133, episode reward: 52.622, mean reward: 0.207 [-9.766, 0.250], mean action: -1.026 [-1.132, -0.869], mean observation: 0.119 [-0.262, 1.000], loss: 0.881551, mean_absolute_error: 0.885676, mean_q: 11.652152
 490731/1000000: episode: 1514, duration: 5.222s, episode steps: 694, steps per second: 133, episode reward: 75.288, mean reward: 0.108 [-10.617, 0.278], mean action: -1.013 [-1.164, -0.884], mean observation: 0.124 [-1.515, 1.749], loss: 0.845628, mean_absolute_error: 0.869100, mean_q: 11.670292
 491430/1000000: episode: 1515, duration: 5.307s, episode steps: 699, steps per second: 132, episode reward: 147.129, mean reward: 0.210 [-10.334, 0.370], mean action: -1.004 [-1.126, -0.818], mean observation: 0.035 [-1.576, 1.000], loss: 0.835663, mean_absolute_error: 0.872509, mean_q: 11.631913
 492127/1000000: episode: 1516, duration: 5.311s, episode steps: 697, steps per second: 131, episode reward: 181.428, mean reward: 0.260 [-10.813, 0.665], mean action: -1.007 [-1.128, -0.860], mean observation: 0.100 [-2.686, 1.885], loss: 0.868123, mean_absolute_error: 0.882429, mean_q: 11.643537
 492802/1000000: episode: 1517, duration: 5.056s, episode steps: 675, steps per second: 133, episode reward: 185.399, mean reward: 0.275 [-10.673, 0.541], mean action: -1.009 [-1.162, -0.811], mean observation: 0.259 [-0.468, 1.797], loss: 0.886663, mean_absolute_error: 0.888106, mean_q: 11.730348
 493375/1000000: episode: 1518, duration: 4.299s, episode steps: 573, steps per second: 133, episode reward: 41.357, mean reward: 0.072 [-10.254, 0.193], mean action: -1.010 [-1.114, -0.878], mean observation: 0.185 [-1.126, 1.211], loss: 0.873361, mean_absolute_error: 0.886931, mean_q: 11.786858
 494043/1000000: episode: 1519, duration: 5.196s, episode steps: 668, steps per second: 129, episode reward: 67.348, mean reward: 0.101 [-10.460, 0.315], mean action: -0.976 [-1.114, -0.738], mean observation: 0.174 [-0.585, 1.780], loss: 0.834306, mean_absolute_error: 0.887274, mean_q: 11.831035
 494753/1000000: episode: 1520, duration: 5.478s, episode steps: 710, steps per second: 130, episode reward: 168.643, mean reward: 0.238 [-10.364, 0.371], mean action: -1.024 [-1.168, -0.900], mean observation: 0.102 [-1.892, 1.056], loss: 0.839616, mean_absolute_error: 0.894702, mean_q: 11.856526
 495326/1000000: episode: 1521, duration: 4.336s, episode steps: 573, steps per second: 132, episode reward: 42.578, mean reward: 0.074 [-9.981, 0.104], mean action: -1.005 [-1.114, -0.881], mean observation: 0.095 [-1.247, 1.000], loss: 0.858142, mean_absolute_error: 0.896849, mean_q: 11.915605
 495935/1000000: episode: 1522, duration: 4.765s, episode steps: 609, steps per second: 128, episode reward: 62.419, mean reward: 0.102 [-10.225, 0.217], mean action: -0.999 [-1.126, -0.850], mean observation: 0.206 [-0.338, 1.192], loss: 0.880625, mean_absolute_error: 0.899706, mean_q: 11.906229
 496534/1000000: episode: 1523, duration: 4.665s, episode steps: 599, steps per second: 128, episode reward: 68.995, mean reward: 0.115 [-10.287, 0.218], mean action: -1.005 [-1.142, -0.892], mean observation: 0.052 [-1.791, 1.000], loss: 0.855318, mean_absolute_error: 0.897822, mean_q: 12.006313
 497241/1000000: episode: 1524, duration: 5.292s, episode steps: 707, steps per second: 134, episode reward: 305.670, mean reward: 0.432 [-10.560, 0.659], mean action: -1.000 [-1.136, -0.865], mean observation: 0.235 [-0.758, 2.469], loss: 0.885816, mean_absolute_error: 0.906980, mean_q: 11.983458
 497971/1000000: episode: 1525, duration: 5.620s, episode steps: 730, steps per second: 130, episode reward: 117.752, mean reward: 0.161 [-10.476, 0.344], mean action: -1.007 [-1.145, -0.860], mean observation: 0.228 [-0.241, 1.486], loss: 0.883331, mean_absolute_error: 0.901379, mean_q: 12.018867
 498681/1000000: episode: 1526, duration: 5.437s, episode steps: 710, steps per second: 131, episode reward: 158.013, mean reward: 0.223 [-10.330, 0.351], mean action: -1.016 [-1.166, -0.856], mean observation: 0.202 [-0.436, 2.029], loss: 0.874191, mean_absolute_error: 0.909031, mean_q: 12.044826
 499322/1000000: episode: 1527, duration: 4.769s, episode steps: 641, steps per second: 134, episode reward: 65.818, mean reward: 0.103 [-10.715, 0.297], mean action: -1.018 [-1.201, -0.885], mean observation: 0.043 [-2.999, 1.000], loss: 0.897997, mean_absolute_error: 0.910057, mean_q: 12.064076
 499916/1000000: episode: 1528, duration: 4.409s, episode steps: 594, steps per second: 135, episode reward: 97.764, mean reward: 0.165 [-10.245, 0.297], mean action: -0.997 [-1.114, -0.890], mean observation: 0.076 [-1.466, 1.077], loss: 0.924547, mean_absolute_error: 0.922240, mean_q: 12.107261
 500726/1000000: episode: 1529, duration: 6.182s, episode steps: 810, steps per second: 131, episode reward: 258.749, mean reward: 0.319 [-10.414, 0.654], mean action: -1.004 [-1.168, -0.864], mean observation: 0.046 [-2.419, 1.000], loss: 0.840375, mean_absolute_error: 0.903218, mean_q: 12.220457
 501489/1000000: episode: 1530, duration: 5.866s, episode steps: 763, steps per second: 130, episode reward: 108.537, mean reward: 0.142 [-10.732, 0.404], mean action: -1.013 [-1.180, -0.887], mean observation: 0.006 [-1.912, 1.000], loss: 0.920721, mean_absolute_error: 0.914333, mean_q: 12.227832
 502251/1000000: episode: 1531, duration: 5.948s, episode steps: 762, steps per second: 128, episode reward: 122.428, mean reward: 0.161 [-10.217, 0.288], mean action: -0.997 [-1.131, -0.839], mean observation: 0.204 [-0.189, 1.036], loss: 0.893737, mean_absolute_error: 0.907851, mean_q: 12.240888
 503270/1000000: episode: 1532, duration: 7.979s, episode steps: 1019, steps per second: 128, episode reward: 294.854, mean reward: 0.289 [-10.556, 0.576], mean action: -1.012 [-1.176, -0.878], mean observation: 0.146 [-1.027, 1.977], loss: 0.898323, mean_absolute_error: 0.906448, mean_q: 12.382087
 504078/1000000: episode: 1533, duration: 6.197s, episode steps: 808, steps per second: 130, episode reward: 264.826, mean reward: 0.328 [-10.452, 0.476], mean action: -0.994 [-1.146, -0.861], mean observation: 0.181 [-1.414, 2.303], loss: 0.904975, mean_absolute_error: 0.907900, mean_q: 12.363066
 504665/1000000: episode: 1534, duration: 4.357s, episode steps: 587, steps per second: 135, episode reward: 128.138, mean reward: 0.218 [-9.783, 0.276], mean action: -0.991 [-1.133, -0.839], mean observation: 0.128 [-0.644, 1.212], loss: 0.850119, mean_absolute_error: 0.900446, mean_q: 12.395541
 505517/1000000: episode: 1535, duration: 6.365s, episode steps: 852, steps per second: 134, episode reward: 354.997, mean reward: 0.417 [-10.403, 0.746], mean action: -1.014 [-1.148, -0.850], mean observation: 0.174 [-0.766, 2.337], loss: 0.889077, mean_absolute_error: 0.907142, mean_q: 12.377560
 506467/1000000: episode: 1536, duration: 7.265s, episode steps: 950, steps per second: 131, episode reward: 289.999, mean reward: 0.305 [-10.249, 0.408], mean action: -0.988 [-1.129, -0.862], mean observation: 0.087 [-1.744, 1.000], loss: 0.880485, mean_absolute_error: 0.900178, mean_q: 12.443468
 507332/1000000: episode: 1537, duration: 6.656s, episode steps: 865, steps per second: 130, episode reward: 235.998, mean reward: 0.273 [-10.350, 0.391], mean action: -1.002 [-1.191, -0.844], mean observation: 0.208 [-0.298, 1.709], loss: 0.860090, mean_absolute_error: 0.889061, mean_q: 12.449285
 507947/1000000: episode: 1538, duration: 4.662s, episode steps: 615, steps per second: 132, episode reward: 166.459, mean reward: 0.271 [-10.318, 0.469], mean action: -0.992 [-1.128, -0.833], mean observation: 0.191 [-0.371, 1.808], loss: 0.824359, mean_absolute_error: 0.890451, mean_q: 12.412265
 508530/1000000: episode: 1539, duration: 4.627s, episode steps: 583, steps per second: 126, episode reward: 57.392, mean reward: 0.098 [-10.149, 0.167], mean action: -1.007 [-1.134, -0.886], mean observation: 0.200 [-0.235, 1.508], loss: 0.839594, mean_absolute_error: 0.882945, mean_q: 12.481042
 509162/1000000: episode: 1540, duration: 4.892s, episode steps: 632, steps per second: 129, episode reward: 70.816, mean reward: 0.112 [-10.463, 0.313], mean action: -1.004 [-1.177, -0.854], mean observation: -0.011 [-2.498, 1.000], loss: 0.885368, mean_absolute_error: 0.888105, mean_q: 12.492869
 509820/1000000: episode: 1541, duration: 5.063s, episode steps: 658, steps per second: 130, episode reward: 218.402, mean reward: 0.332 [-10.471, 0.577], mean action: -0.983 [-1.160, -0.846], mean observation: 0.114 [-1.668, 1.679], loss: 0.849239, mean_absolute_error: 0.889745, mean_q: 12.488670
 510712/1000000: episode: 1542, duration: 6.656s, episode steps: 892, steps per second: 134, episode reward: 256.866, mean reward: 0.288 [-10.339, 0.576], mean action: -1.004 [-1.135, -0.866], mean observation: 0.205 [-0.406, 2.227], loss: 0.848945, mean_absolute_error: 0.885757, mean_q: 12.514799
 511613/1000000: episode: 1543, duration: 6.819s, episode steps: 901, steps per second: 132, episode reward: 127.159, mean reward: 0.141 [-10.308, 0.287], mean action: -0.997 [-1.133, -0.826], mean observation: 0.176 [-0.890, 1.096], loss: 0.864087, mean_absolute_error: 0.881784, mean_q: 12.473094
 512362/1000000: episode: 1544, duration: 5.661s, episode steps: 749, steps per second: 132, episode reward: 278.441, mean reward: 0.372 [-10.278, 0.550], mean action: -0.984 [-1.143, -0.836], mean observation: 0.067 [-1.930, 1.000], loss: 0.845684, mean_absolute_error: 0.881394, mean_q: 12.505823
 512882/1000000: episode: 1545, duration: 3.947s, episode steps: 520, steps per second: 132, episode reward: 62.884, mean reward: 0.121 [-10.286, 0.273], mean action: -1.000 [-1.138, -0.867], mean observation: 0.205 [-0.985, 1.562], loss: 0.857048, mean_absolute_error: 0.884128, mean_q: 12.488880
 513420/1000000: episode: 1546, duration: 4.069s, episode steps: 538, steps per second: 132, episode reward: 125.017, mean reward: 0.232 [-10.095, 0.319], mean action: -0.986 [-1.136, -0.851], mean observation: 0.132 [-1.193, 1.499], loss: 0.857057, mean_absolute_error: 0.878665, mean_q: 12.445881
 513965/1000000: episode: 1547, duration: 4.142s, episode steps: 545, steps per second: 132, episode reward: 137.794, mean reward: 0.253 [-10.810, 0.437], mean action: -0.958 [-1.110, 1.012], mean observation: 0.027 [-3.835, 1.000], loss: 0.839685, mean_absolute_error: 0.872346, mean_q: 12.478810
 514300/1000000: episode: 1548, duration: 2.542s, episode steps: 335, steps per second: 132, episode reward: 37.350, mean reward: 0.111 [-11.050, 0.438], mean action: -0.905 [-1.099, 1.040], mean observation: -0.099 [-3.983, 1.000], loss: 0.868993, mean_absolute_error: 0.875706, mean_q: 12.451337
 514676/1000000: episode: 1549, duration: 2.966s, episode steps: 376, steps per second: 127, episode reward: 88.137, mean reward: 0.234 [-10.541, 0.526], mean action: -0.958 [-1.133, 0.411], mean observation: -0.018 [-2.713, 1.000], loss: 0.895903, mean_absolute_error: 0.872753, mean_q: 12.475056
 514717/1000000: episode: 1550, duration: 0.320s, episode steps: 41, steps per second: 128, episode reward: -1.015, mean reward: -0.025 [-9.781, 0.219], mean action: -1.012 [-1.077, -0.957], mean observation: 0.108 [-0.275, 1.000], loss: 0.809982, mean_absolute_error: 0.872734, mean_q: 12.497455
 515893/1000000: episode: 1551, duration: 9.080s, episode steps: 1176, steps per second: 130, episode reward: 154.579, mean reward: 0.131 [-11.047, 0.393], mean action: -0.949 [-1.177, 1.092], mean observation: 0.095 [-6.653, 1.000], loss: 0.830776, mean_absolute_error: 0.867983, mean_q: 12.499160
 516182/1000000: episode: 1552, duration: 2.251s, episode steps: 289, steps per second: 128, episode reward: 17.941, mean reward: 0.062 [-11.534, 0.723], mean action: -0.810 [-1.153, 1.054], mean observation: -0.199 [-5.740, 1.770], loss: 0.878223, mean_absolute_error: 0.877312, mean_q: 12.447839
 516416/1000000: episode: 1553, duration: 1.782s, episode steps: 234, steps per second: 131, episode reward: -28.622, mean reward: -0.122 [-10.978, 0.176], mean action: -0.843 [-1.122, 1.024], mean observation: -0.147 [-4.955, 1.000], loss: 0.898081, mean_absolute_error: 0.883984, mean_q: 12.427773
 516646/1000000: episode: 1554, duration: 1.851s, episode steps: 230, steps per second: 124, episode reward: -39.598, mean reward: -0.172 [-10.829, 0.073], mean action: -0.853 [-1.125, 1.005], mean observation: -0.087 [-4.522, 1.000], loss: 0.849279, mean_absolute_error: 0.872608, mean_q: 12.420299
 516928/1000000: episode: 1555, duration: 2.214s, episode steps: 282, steps per second: 127, episode reward: -31.819, mean reward: -0.113 [-11.419, 0.377], mean action: -0.840 [-1.089, 1.034], mean observation: -0.169 [-4.823, 1.000], loss: 0.784850, mean_absolute_error: 0.861860, mean_q: 12.458764
 517260/1000000: episode: 1556, duration: 2.549s, episode steps: 332, steps per second: 130, episode reward: -26.278, mean reward: -0.079 [-11.499, 0.400], mean action: -0.900 [-1.101, 1.007], mean observation: -0.131 [-4.502, 1.000], loss: 0.851511, mean_absolute_error: 0.875899, mean_q: 12.467179
 518061/1000000: episode: 1557, duration: 6.080s, episode steps: 801, steps per second: 132, episode reward: 196.343, mean reward: 0.245 [-10.764, 0.480], mean action: -1.002 [-1.146, -0.848], mean observation: 0.120 [-2.302, 1.411], loss: 0.863850, mean_absolute_error: 0.871498, mean_q: 12.423809
 519269/1000000: episode: 1558, duration: 9.116s, episode steps: 1208, steps per second: 133, episode reward: 308.974, mean reward: 0.256 [-10.276, 0.452], mean action: -1.005 [-1.163, -0.871], mean observation: 0.182 [-1.525, 1.243], loss: 0.842934, mean_absolute_error: 0.868611, mean_q: 12.443393
 519821/1000000: episode: 1559, duration: 4.253s, episode steps: 552, steps per second: 130, episode reward: 16.622, mean reward: 0.030 [-10.406, 0.136], mean action: -1.013 [-1.153, -0.877], mean observation: 0.001 [-1.631, 1.000], loss: 0.889313, mean_absolute_error: 0.869270, mean_q: 12.459208
 520513/1000000: episode: 1560, duration: 5.284s, episode steps: 692, steps per second: 131, episode reward: 184.482, mean reward: 0.267 [-10.777, 0.387], mean action: -0.930 [-1.109, 1.011], mean observation: 0.054 [-5.239, 1.000], loss: 0.803643, mean_absolute_error: 0.858685, mean_q: 12.444240
 520846/1000000: episode: 1561, duration: 2.559s, episode steps: 333, steps per second: 130, episode reward: -43.835, mean reward: -0.132 [-11.447, 0.293], mean action: -0.910 [-1.132, 0.981], mean observation: -0.082 [-4.494, 1.000], loss: 0.868952, mean_absolute_error: 0.865398, mean_q: 12.497515
 521039/1000000: episode: 1562, duration: 1.510s, episode steps: 193, steps per second: 128, episode reward: 9.100, mean reward: 0.047 [-10.095, 0.133], mean action: -0.744 [-1.072, 1.116], mean observation: 0.002 [-5.008, 1.000], loss: 0.850337, mean_absolute_error: 0.860874, mean_q: 12.291530
 521339/1000000: episode: 1563, duration: 2.311s, episode steps: 300, steps per second: 130, episode reward: -65.909, mean reward: -0.220 [-11.716, 0.376], mean action: -0.867 [-1.126, 1.085], mean observation: -0.171 [-5.080, 1.000], loss: 0.794036, mean_absolute_error: 0.849970, mean_q: 12.405124
 522141/1000000: episode: 1564, duration: 6.114s, episode steps: 802, steps per second: 131, episode reward: 266.103, mean reward: 0.332 [-10.247, 0.498], mean action: -0.991 [-1.128, -0.813], mean observation: 0.177 [-0.985, 1.364], loss: 0.786375, mean_absolute_error: 0.851939, mean_q: 12.447468
 522647/1000000: episode: 1565, duration: 3.977s, episode steps: 506, steps per second: 127, episode reward: 72.756, mean reward: 0.144 [-10.695, 0.304], mean action: -0.971 [-1.116, 0.904], mean observation: 0.012 [-3.126, 1.000], loss: 0.832249, mean_absolute_error: 0.856352, mean_q: 12.352402
 522933/1000000: episode: 1566, duration: 2.243s, episode steps: 286, steps per second: 128, episode reward: -8.664, mean reward: -0.030 [-11.230, 0.413], mean action: -0.765 [-1.098, 1.106], mean observation: -0.183 [-6.158, 2.570], loss: 0.827033, mean_absolute_error: 0.857649, mean_q: 12.398007
 523216/1000000: episode: 1567, duration: 2.128s, episode steps: 283, steps per second: 133, episode reward: 11.140, mean reward: 0.039 [-11.336, 0.568], mean action: -0.772 [-1.149, 1.027], mean observation: -0.194 [-6.255, 2.386], loss: 0.850522, mean_absolute_error: 0.858459, mean_q: 12.367275
 523473/1000000: episode: 1568, duration: 1.921s, episode steps: 257, steps per second: 134, episode reward: -33.643, mean reward: -0.131 [-11.544, 0.411], mean action: -0.789 [-1.134, 1.060], mean observation: -0.190 [-5.767, 1.000], loss: 0.844433, mean_absolute_error: 0.856210, mean_q: 12.419527
 523662/1000000: episode: 1569, duration: 1.405s, episode steps: 189, steps per second: 135, episode reward: 28.812, mean reward: 0.152 [-10.178, 0.287], mean action: -0.774 [-1.079, 1.020], mean observation: -0.058 [-5.171, 1.000], loss: 0.877520, mean_absolute_error: 0.870688, mean_q: 12.402335
 524654/1000000: episode: 1570, duration: 7.434s, episode steps: 992, steps per second: 133, episode reward: 456.680, mean reward: 0.460 [-10.241, 0.655], mean action: -1.007 [-1.124, -0.888], mean observation: 0.176 [-0.785, 1.434], loss: 0.781534, mean_absolute_error: 0.848634, mean_q: 12.371816
 524939/1000000: episode: 1571, duration: 2.281s, episode steps: 285, steps per second: 125, episode reward: 27.751, mean reward: 0.097 [-10.449, 0.221], mean action: -0.833 [-1.122, 1.081], mean observation: -0.003 [-5.307, 1.000], loss: 0.800700, mean_absolute_error: 0.849232, mean_q: 12.388436
 525213/1000000: episode: 1572, duration: 2.134s, episode steps: 274, steps per second: 128, episode reward: 4.178, mean reward: 0.015 [-10.393, 0.382], mean action: -0.511 [-1.106, 1.062], mean observation: -0.167 [-7.925, 5.245], loss: 0.794009, mean_absolute_error: 0.849232, mean_q: 12.384738
 525458/1000000: episode: 1573, duration: 1.919s, episode steps: 245, steps per second: 128, episode reward: -43.480, mean reward: -0.177 [-11.382, 0.341], mean action: -0.501 [-1.095, 1.036], mean observation: -0.197 [-7.895, 2.779], loss: 0.778104, mean_absolute_error: 0.843304, mean_q: 12.361492
 525701/1000000: episode: 1574, duration: 1.941s, episode steps: 243, steps per second: 125, episode reward: -41.483, mean reward: -0.171 [-11.344, 0.344], mean action: -0.489 [-1.109, 1.079], mean observation: -0.178 [-7.902, 2.826], loss: 0.760954, mean_absolute_error: 0.840045, mean_q: 12.354754
 525937/1000000: episode: 1575, duration: 1.854s, episode steps: 236, steps per second: 127, episode reward: -49.365, mean reward: -0.209 [-11.386, 0.285], mean action: -0.481 [-1.117, 1.103], mean observation: -0.194 [-8.029, 2.380], loss: 0.812645, mean_absolute_error: 0.850537, mean_q: 12.360212
 526165/1000000: episode: 1576, duration: 1.723s, episode steps: 228, steps per second: 132, episode reward: 21.798, mean reward: 0.096 [-11.042, 0.485], mean action: -0.509 [-1.096, 1.101], mean observation: -0.157 [-7.960, 1.000], loss: 0.790966, mean_absolute_error: 0.854423, mean_q: 12.326717
 526318/1000000: episode: 1577, duration: 1.164s, episode steps: 153, steps per second: 131, episode reward: 22.273, mean reward: 0.146 [-10.041, 0.252], mean action: -0.525 [-1.117, 1.052], mean observation: -0.013 [-6.009, 1.000], loss: 0.779723, mean_absolute_error: 0.845830, mean_q: 12.392107
 526536/1000000: episode: 1578, duration: 1.662s, episode steps: 218, steps per second: 131, episode reward: -28.686, mean reward: -0.132 [-11.023, 0.172], mean action: -0.474 [-1.071, 1.133], mean observation: -0.167 [-7.991, 1.000], loss: 0.809608, mean_absolute_error: 0.844870, mean_q: 12.373856
 526815/1000000: episode: 1579, duration: 2.096s, episode steps: 279, steps per second: 133, episode reward: -54.571, mean reward: -0.196 [-10.885, 0.408], mean action: -0.493 [-1.164, 1.087], mean observation: -0.182 [-7.925, 5.196], loss: 0.795816, mean_absolute_error: 0.842345, mean_q: 12.333590
 527074/1000000: episode: 1580, duration: 1.956s, episode steps: 259, steps per second: 132, episode reward: -13.317, mean reward: -0.051 [-10.888, 0.457], mean action: -0.498 [-1.173, 1.110], mean observation: -0.194 [-8.023, 4.850], loss: 0.846218, mean_absolute_error: 0.852568, mean_q: 12.319253
 527319/1000000: episode: 1581, duration: 1.989s, episode steps: 245, steps per second: 123, episode reward: -8.600, mean reward: -0.035 [-10.809, 0.326], mean action: -0.493 [-1.122, 1.125], mean observation: -0.174 [-7.993, 3.628], loss: 0.842158, mean_absolute_error: 0.841035, mean_q: 12.391753
 527539/1000000: episode: 1582, duration: 1.698s, episode steps: 220, steps per second: 130, episode reward: -27.252, mean reward: -0.124 [-11.106, 0.205], mean action: -0.489 [-1.083, 1.073], mean observation: -0.172 [-7.889, 1.000], loss: 0.870915, mean_absolute_error: 0.854877, mean_q: 12.336607
 527807/1000000: episode: 1583, duration: 2.064s, episode steps: 268, steps per second: 130, episode reward: -12.416, mean reward: -0.046 [-10.668, 0.421], mean action: -0.498 [-1.138, 1.096], mean observation: -0.165 [-7.981, 5.158], loss: 0.833439, mean_absolute_error: 0.861777, mean_q: 12.243178
 528051/1000000: episode: 1584, duration: 1.845s, episode steps: 244, steps per second: 132, episode reward: 23.482, mean reward: 0.096 [-11.038, 0.577], mean action: -0.484 [-1.083, 1.034], mean observation: -0.176 [-7.869, 3.111], loss: 0.852146, mean_absolute_error: 0.845003, mean_q: 12.384279
 528292/1000000: episode: 1585, duration: 1.968s, episode steps: 241, steps per second: 122, episode reward: -91.659, mean reward: -0.380 [-11.458, 0.117], mean action: -0.492 [-1.095, 1.088], mean observation: -0.175 [-7.933, 3.154], loss: 0.772235, mean_absolute_error: 0.842948, mean_q: 12.290776
 528493/1000000: episode: 1586, duration: 1.544s, episode steps: 201, steps per second: 130, episode reward: -12.671, mean reward: -0.063 [-10.985, 0.220], mean action: -0.506 [-1.107, 1.043], mean observation: -0.141 [-6.975, 1.000], loss: 0.813134, mean_absolute_error: 0.851716, mean_q: 12.231562
 528751/1000000: episode: 1587, duration: 1.980s, episode steps: 258, steps per second: 130, episode reward: 27.128, mean reward: 0.105 [-10.806, 0.598], mean action: -0.515 [-1.114, 1.063], mean observation: -0.163 [-7.952, 4.731], loss: 0.831438, mean_absolute_error: 0.858673, mean_q: 12.271136
 529005/1000000: episode: 1588, duration: 1.892s, episode steps: 254, steps per second: 134, episode reward: -69.067, mean reward: -0.272 [-11.286, 0.285], mean action: -0.519 [-1.150, 1.090], mean observation: -0.207 [-7.923, 4.440], loss: 0.819287, mean_absolute_error: 0.854980, mean_q: 12.288128
 529238/1000000: episode: 1589, duration: 1.785s, episode steps: 233, steps per second: 131, episode reward: -4.843, mean reward: -0.021 [-10.964, 0.346], mean action: -0.493 [-1.070, 1.109], mean observation: -0.150 [-7.942, 2.078], loss: 0.808113, mean_absolute_error: 0.849083, mean_q: 12.303522
 529495/1000000: episode: 1590, duration: 1.980s, episode steps: 257, steps per second: 130, episode reward: -55.853, mean reward: -0.217 [-11.198, 0.339], mean action: -0.514 [-1.124, 1.102], mean observation: -0.178 [-7.982, 4.604], loss: 0.804181, mean_absolute_error: 0.841053, mean_q: 12.345500
 529748/1000000: episode: 1591, duration: 1.929s, episode steps: 253, steps per second: 131, episode reward: -50.224, mean reward: -0.199 [-11.119, 0.276], mean action: -0.527 [-1.118, 1.062], mean observation: -0.203 [-7.872, 4.181], loss: 0.803972, mean_absolute_error: 0.855112, mean_q: 12.242897
 529894/1000000: episode: 1592, duration: 1.178s, episode steps: 146, steps per second: 124, episode reward: 16.213, mean reward: 0.111 [-10.008, 0.209], mean action: -0.492 [-1.070, 1.108], mean observation: 0.010 [-5.809, 1.000], loss: 0.845697, mean_absolute_error: 0.844007, mean_q: 12.227525
 530129/1000000: episode: 1593, duration: 1.837s, episode steps: 235, steps per second: 128, episode reward: -15.862, mean reward: -0.067 [-10.889, 0.247], mean action: -0.495 [-1.086, 1.061], mean observation: -0.144 [-7.894, 1.861], loss: 0.772555, mean_absolute_error: 0.833414, mean_q: 12.269917
 530319/1000000: episode: 1594, duration: 1.454s, episode steps: 190, steps per second: 131, episode reward: -8.457, mean reward: -0.045 [-10.704, 0.157], mean action: -0.486 [-1.097, 1.099], mean observation: -0.074 [-6.695, 1.000], loss: 0.757280, mean_absolute_error: 0.839386, mean_q: 12.288826
 530554/1000000: episode: 1595, duration: 1.785s, episode steps: 235, steps per second: 132, episode reward: -54.735, mean reward: -0.233 [-11.430, 0.245], mean action: -0.516 [-1.143, 1.075], mean observation: -0.178 [-7.969, 1.925], loss: 0.791433, mean_absolute_error: 0.851504, mean_q: 12.258431
 530743/1000000: episode: 1596, duration: 1.460s, episode steps: 189, steps per second: 129, episode reward: 3.052, mean reward: 0.016 [-10.505, 0.177], mean action: -0.516 [-1.123, 1.038], mean observation: -0.086 [-6.086, 1.000], loss: 0.872350, mean_absolute_error: 0.853723, mean_q: 12.272954
 530997/1000000: episode: 1597, duration: 1.966s, episode steps: 254, steps per second: 129, episode reward: -48.811, mean reward: -0.192 [-11.103, 0.317], mean action: -0.491 [-1.112, 1.095], mean observation: -0.168 [-7.954, 4.496], loss: 0.804036, mean_absolute_error: 0.842833, mean_q: 12.322112
 531221/1000000: episode: 1598, duration: 1.708s, episode steps: 224, steps per second: 131, episode reward: -7.264, mean reward: -0.032 [-11.060, 0.318], mean action: -0.492 [-1.096, 1.098], mean observation: -0.167 [-7.946, 1.000], loss: 0.865704, mean_absolute_error: 0.860449, mean_q: 12.224051
 531492/1000000: episode: 1599, duration: 2.086s, episode steps: 271, steps per second: 130, episode reward: -14.447, mean reward: -0.053 [-10.408, 0.303], mean action: -0.490 [-1.111, 1.104], mean observation: -0.162 [-7.931, 5.252], loss: 0.782066, mean_absolute_error: 0.838212, mean_q: 12.389961
 531706/1000000: episode: 1600, duration: 1.720s, episode steps: 214, steps per second: 124, episode reward: -27.476, mean reward: -0.128 [-10.944, 0.129], mean action: -0.504 [-1.115, 1.054], mean observation: -0.105 [-7.799, 1.000], loss: 0.854094, mean_absolute_error: 0.853859, mean_q: 12.318350
 531921/1000000: episode: 1601, duration: 1.688s, episode steps: 215, steps per second: 127, episode reward: -4.425, mean reward: -0.021 [-11.149, 0.363], mean action: -0.506 [-1.122, 1.077], mean observation: -0.170 [-7.915, 1.000], loss: 0.790473, mean_absolute_error: 0.848672, mean_q: 12.304128
 532174/1000000: episode: 1602, duration: 1.903s, episode steps: 253, steps per second: 133, episode reward: -62.570, mean reward: -0.247 [-11.291, 0.310], mean action: -0.500 [-1.089, 1.068], mean observation: -0.204 [-7.947, 4.294], loss: 0.778459, mean_absolute_error: 0.845579, mean_q: 12.219014
 532435/1000000: episode: 1603, duration: 1.957s, episode steps: 261, steps per second: 133, episode reward: -26.981, mean reward: -0.103 [-11.019, 0.488], mean action: -0.483 [-1.116, 1.180], mean observation: -0.199 [-8.024, 4.953], loss: 0.815294, mean_absolute_error: 0.858978, mean_q: 12.210485
 532648/1000000: episode: 1604, duration: 1.587s, episode steps: 213, steps per second: 134, episode reward: -5.432, mean reward: -0.026 [-10.845, 0.234], mean action: -0.484 [-1.098, 1.081], mean observation: -0.149 [-7.860, 1.000], loss: 0.842822, mean_absolute_error: 0.863007, mean_q: 12.276803
 532840/1000000: episode: 1605, duration: 1.492s, episode steps: 192, steps per second: 129, episode reward: -7.783, mean reward: -0.041 [-10.610, 0.136], mean action: -0.457 [-1.054, 1.139], mean observation: -0.077 [-6.653, 1.000], loss: 0.815489, mean_absolute_error: 0.846206, mean_q: 12.209056
 533105/1000000: episode: 1606, duration: 2.068s, episode steps: 265, steps per second: 128, episode reward: 12.482, mean reward: 0.047 [-10.667, 0.530], mean action: -0.491 [-1.123, 1.108], mean observation: -0.160 [-7.990, 5.131], loss: 0.818695, mean_absolute_error: 0.856242, mean_q: 12.297875
 533284/1000000: episode: 1607, duration: 1.423s, episode steps: 179, steps per second: 126, episode reward: -1.040, mean reward: -0.006 [-10.424, 0.135], mean action: -0.506 [-1.149, 1.022], mean observation: -0.059 [-6.045, 1.000], loss: 0.783578, mean_absolute_error: 0.841387, mean_q: 12.286228
 533540/1000000: episode: 1608, duration: 1.963s, episode steps: 256, steps per second: 130, episode reward: -52.745, mean reward: -0.206 [-10.828, 0.187], mean action: -0.516 [-1.126, 1.088], mean observation: -0.192 [-7.980, 4.654], loss: 0.759478, mean_absolute_error: 0.849988, mean_q: 12.271558
 533799/1000000: episode: 1609, duration: 1.930s, episode steps: 259, steps per second: 134, episode reward: 5.685, mean reward: 0.022 [-10.773, 0.493], mean action: -0.509 [-1.139, 1.109], mean observation: -0.167 [-7.963, 4.878], loss: 0.842292, mean_absolute_error: 0.844383, mean_q: 12.195727
 533982/1000000: episode: 1610, duration: 1.387s, episode steps: 183, steps per second: 132, episode reward: -11.116, mean reward: -0.061 [-10.780, 0.166], mean action: -0.505 [-1.141, 1.093], mean observation: -0.111 [-6.018, 1.000], loss: 0.771568, mean_absolute_error: 0.857343, mean_q: 12.281013
 534220/1000000: episode: 1611, duration: 1.848s, episode steps: 238, steps per second: 129, episode reward: -63.405, mean reward: -0.266 [-11.407, 0.208], mean action: -0.533 [-1.148, 1.028], mean observation: -0.174 [-7.904, 2.400], loss: 0.827220, mean_absolute_error: 0.859969, mean_q: 12.265953
 534400/1000000: episode: 1612, duration: 1.344s, episode steps: 180, steps per second: 134, episode reward: 29.188, mean reward: 0.162 [-10.387, 0.335], mean action: -0.521 [-1.122, 1.060], mean observation: -0.071 [-6.003, 1.000], loss: 0.774816, mean_absolute_error: 0.843910, mean_q: 12.281343
 534654/1000000: episode: 1613, duration: 2.021s, episode steps: 254, steps per second: 126, episode reward: -17.166, mean reward: -0.068 [-10.651, 0.278], mean action: -0.504 [-1.174, 1.098], mean observation: -0.176 [-7.996, 4.508], loss: 0.845546, mean_absolute_error: 0.850074, mean_q: 12.268476
 534866/1000000: episode: 1614, duration: 1.635s, episode steps: 212, steps per second: 130, episode reward: -28.536, mean reward: -0.135 [-11.219, 0.223], mean action: -0.539 [-1.122, 1.031], mean observation: -0.170 [-7.826, 1.000], loss: 0.816095, mean_absolute_error: 0.860664, mean_q: 12.323610
 535116/1000000: episode: 1615, duration: 1.930s, episode steps: 250, steps per second: 130, episode reward: 21.928, mean reward: 0.088 [-10.904, 0.528], mean action: -0.516 [-1.164, 1.044], mean observation: -0.170 [-7.860, 3.814], loss: 0.815731, mean_absolute_error: 0.846063, mean_q: 12.257718
 535373/1000000: episode: 1616, duration: 1.983s, episode steps: 257, steps per second: 130, episode reward: -78.434, mean reward: -0.305 [-11.256, 0.256], mean action: -0.498 [-1.103, 1.128], mean observation: -0.210 [-7.942, 4.695], loss: 0.858276, mean_absolute_error: 0.858548, mean_q: 12.299863
 535630/1000000: episode: 1617, duration: 1.965s, episode steps: 257, steps per second: 131, episode reward: -60.376, mean reward: -0.235 [-11.181, 0.310], mean action: -0.488 [-1.095, 1.099], mean observation: -0.212 [-7.955, 4.646], loss: 0.820260, mean_absolute_error: 0.850402, mean_q: 12.264101
 535792/1000000: episode: 1618, duration: 1.312s, episode steps: 162, steps per second: 123, episode reward: -9.304, mean reward: -0.057 [-10.428, 0.092], mean action: -0.492 [-1.115, 1.089], mean observation: -0.063 [-5.962, 1.000], loss: 0.787269, mean_absolute_error: 0.848000, mean_q: 12.178420
 536048/1000000: episode: 1619, duration: 2.065s, episode steps: 256, steps per second: 124, episode reward: -54.176, mean reward: -0.212 [-11.279, 0.359], mean action: -0.488 [-1.092, 1.082], mean observation: -0.185 [-7.912, 4.383], loss: 0.832031, mean_absolute_error: 0.860437, mean_q: 12.154078
 536326/1000000: episode: 1620, duration: 2.205s, episode steps: 278, steps per second: 126, episode reward: -37.842, mean reward: -0.136 [-10.571, 0.332], mean action: -0.486 [-1.147, 1.096], mean observation: -0.192 [-7.920, 5.252], loss: 0.813926, mean_absolute_error: 0.850024, mean_q: 12.189477
 536566/1000000: episode: 1621, duration: 1.848s, episode steps: 240, steps per second: 130, episode reward: -19.526, mean reward: -0.081 [-10.864, 0.253], mean action: -0.486 [-1.115, 1.097], mean observation: -0.163 [-7.971, 2.869], loss: 0.752713, mean_absolute_error: 0.835208, mean_q: 12.283606
 536673/1000000: episode: 1622, duration: 0.835s, episode steps: 107, steps per second: 128, episode reward: 0.239, mean reward: 0.002 [-9.924, 0.098], mean action: -0.500 [-1.063, 1.025], mean observation: 0.093 [-3.907, 1.000], loss: 0.800243, mean_absolute_error: 0.859682, mean_q: 12.187447
 536914/1000000: episode: 1623, duration: 1.877s, episode steps: 241, steps per second: 128, episode reward: -47.734, mean reward: -0.198 [-11.178, 0.255], mean action: -0.484 [-1.081, 1.159], mean observation: -0.159 [-8.017, 3.340], loss: 0.815706, mean_absolute_error: 0.851192, mean_q: 12.272823
 537157/1000000: episode: 1624, duration: 1.871s, episode steps: 243, steps per second: 130, episode reward: -64.541, mean reward: -0.266 [-11.360, 0.227], mean action: -0.487 [-1.070, 1.039], mean observation: -0.207 [-7.894, 3.135], loss: 0.894702, mean_absolute_error: 0.857160, mean_q: 12.183725
 537377/1000000: episode: 1625, duration: 1.680s, episode steps: 220, steps per second: 131, episode reward: -4.122, mean reward: -0.019 [-11.003, 0.310], mean action: -0.496 [-1.122, 1.087], mean observation: -0.139 [-7.993, 1.000], loss: 0.875872, mean_absolute_error: 0.860453, mean_q: 12.170897
 537562/1000000: episode: 1626, duration: 1.505s, episode steps: 185, steps per second: 123, episode reward: -19.934, mean reward: -0.108 [-10.869, 0.130], mean action: -0.513 [-1.128, 1.064], mean observation: -0.133 [-6.274, 1.000], loss: 0.872784, mean_absolute_error: 0.869237, mean_q: 12.202465
 537742/1000000: episode: 1627, duration: 1.359s, episode steps: 180, steps per second: 132, episode reward: -11.789, mean reward: -0.065 [-10.497, 0.079], mean action: -0.482 [-1.089, 1.056], mean observation: -0.046 [-6.013, 1.000], loss: 0.793807, mean_absolute_error: 0.842021, mean_q: 12.135483
 538014/1000000: episode: 1628, duration: 2.163s, episode steps: 272, steps per second: 126, episode reward: -10.898, mean reward: -0.040 [-10.492, 0.354], mean action: -0.492 [-1.084, 1.101], mean observation: -0.173 [-7.904, 5.274], loss: 0.960936, mean_absolute_error: 0.879568, mean_q: 12.206699
 538202/1000000: episode: 1629, duration: 1.407s, episode steps: 188, steps per second: 134, episode reward: 1.084, mean reward: 0.006 [-10.802, 0.254], mean action: -0.492 [-1.077, 1.161], mean observation: -0.113 [-6.553, 1.000], loss: 0.800424, mean_absolute_error: 0.843166, mean_q: 12.211003
 538476/1000000: episode: 1630, duration: 2.067s, episode steps: 274, steps per second: 133, episode reward: 3.997, mean reward: 0.015 [-10.415, 0.411], mean action: -0.510 [-1.132, 1.138], mean observation: -0.149 [-7.958, 5.204], loss: 0.794510, mean_absolute_error: 0.843148, mean_q: 12.196070
 538707/1000000: episode: 1631, duration: 1.777s, episode steps: 231, steps per second: 130, episode reward: 10.951, mean reward: 0.047 [-11.003, 0.424], mean action: -0.511 [-1.134, 1.086], mean observation: -0.155 [-7.892, 1.305], loss: 0.788804, mean_absolute_error: 0.853536, mean_q: 12.301301
 538917/1000000: episode: 1632, duration: 1.597s, episode steps: 210, steps per second: 131, episode reward: -28.092, mean reward: -0.134 [-11.113, 0.177], mean action: -0.490 [-1.102, 1.062], mean observation: -0.166 [-7.755, 1.000], loss: 0.811356, mean_absolute_error: 0.858902, mean_q: 12.230533
 539120/1000000: episode: 1633, duration: 1.553s, episode steps: 203, steps per second: 131, episode reward: 24.744, mean reward: 0.122 [-10.741, 0.378], mean action: -0.488 [-1.052, 1.069], mean observation: -0.113 [-7.219, 1.000], loss: 0.918858, mean_absolute_error: 0.875460, mean_q: 12.142510
 539337/1000000: episode: 1634, duration: 1.673s, episode steps: 217, steps per second: 130, episode reward: 1.615, mean reward: 0.007 [-10.950, 0.312], mean action: -0.506 [-1.151, 1.065], mean observation: -0.132 [-7.851, 1.000], loss: 0.744659, mean_absolute_error: 0.840887, mean_q: 12.216764
 539580/1000000: episode: 1635, duration: 1.843s, episode steps: 243, steps per second: 132, episode reward: -46.825, mean reward: -0.193 [-11.357, 0.327], mean action: -0.515 [-1.097, 1.140], mean observation: -0.194 [-7.904, 3.006], loss: 0.857375, mean_absolute_error: 0.851700, mean_q: 12.220378
 539830/1000000: episode: 1636, duration: 1.885s, episode steps: 250, steps per second: 133, episode reward: -47.416, mean reward: -0.190 [-10.844, 0.183], mean action: -0.506 [-1.133, 1.115], mean observation: -0.184 [-7.983, 4.259], loss: 0.877660, mean_absolute_error: 0.863667, mean_q: 12.129683
 540088/1000000: episode: 1637, duration: 1.953s, episode steps: 258, steps per second: 132, episode reward: -7.410, mean reward: -0.029 [-10.668, 0.335], mean action: -0.495 [-1.067, 1.038], mean observation: -0.155 [-7.925, 4.651], loss: 0.778338, mean_absolute_error: 0.851832, mean_q: 12.228283
 540349/1000000: episode: 1638, duration: 2.003s, episode steps: 261, steps per second: 130, episode reward: 27.411, mean reward: 0.105 [-10.697, 0.553], mean action: -0.495 [-1.087, 1.100], mean observation: -0.172 [-7.924, 4.926], loss: 0.906611, mean_absolute_error: 0.869997, mean_q: 12.254581
 540591/1000000: episode: 1639, duration: 1.968s, episode steps: 242, steps per second: 123, episode reward: 26.981, mean reward: 0.111 [-10.940, 0.547], mean action: -0.509 [-1.148, 1.071], mean observation: -0.167 [-7.959, 3.115], loss: 0.798827, mean_absolute_error: 0.851358, mean_q: 12.209539
 540845/1000000: episode: 1640, duration: 2.069s, episode steps: 254, steps per second: 123, episode reward: -42.710, mean reward: -0.168 [-10.901, 0.234], mean action: -0.516 [-1.132, 1.052], mean observation: -0.148 [-7.918, 4.344], loss: 0.886636, mean_absolute_error: 0.870591, mean_q: 12.117001
 541063/1000000: episode: 1641, duration: 1.671s, episode steps: 218, steps per second: 130, episode reward: 2.549, mean reward: 0.012 [-11.120, 0.399], mean action: -0.509 [-1.118, 1.087], mean observation: -0.161 [-7.959, 1.000], loss: 0.816968, mean_absolute_error: 0.864593, mean_q: 12.232864
 541161/1000000: episode: 1642, duration: 0.837s, episode steps: 98, steps per second: 117, episode reward: 16.183, mean reward: 0.165 [-9.752, 0.270], mean action: -0.503 [-1.091, 1.035], mean observation: 0.069 [-3.308, 1.000], loss: 0.806081, mean_absolute_error: 0.854302, mean_q: 12.337353
 541377/1000000: episode: 1643, duration: 1.743s, episode steps: 216, steps per second: 124, episode reward: 6.567, mean reward: 0.030 [-10.790, 0.285], mean action: -0.513 [-1.126, 1.063], mean observation: -0.123 [-7.824, 1.000], loss: 0.844939, mean_absolute_error: 0.865414, mean_q: 12.266351
 541614/1000000: episode: 1644, duration: 1.808s, episode steps: 237, steps per second: 131, episode reward: -0.720, mean reward: -0.003 [-11.144, 0.453], mean action: -0.501 [-1.129, 1.071], mean observation: -0.186 [-7.888, 2.252], loss: 0.755736, mean_absolute_error: 0.854533, mean_q: 12.225501
 541850/1000000: episode: 1645, duration: 1.871s, episode steps: 236, steps per second: 126, episode reward: -7.372, mean reward: -0.031 [-10.800, 0.285], mean action: -0.511 [-1.085, 1.098], mean observation: -0.151 [-7.991, 2.517], loss: 0.835167, mean_absolute_error: 0.857048, mean_q: 12.234793
 542116/1000000: episode: 1646, duration: 2.089s, episode steps: 266, steps per second: 127, episode reward: -21.256, mean reward: -0.080 [-10.574, 0.330], mean action: -0.514 [-1.129, 1.067], mean observation: -0.181 [-7.980, 5.173], loss: 0.859138, mean_absolute_error: 0.869410, mean_q: 12.178092
 542261/1000000: episode: 1647, duration: 1.114s, episode steps: 145, steps per second: 130, episode reward: 13.159, mean reward: 0.091 [-10.116, 0.213], mean action: -0.508 [-1.082, 1.066], mean observation: -0.025 [-5.854, 1.000], loss: 0.853608, mean_absolute_error: 0.855290, mean_q: 12.208295
 542506/1000000: episode: 1648, duration: 1.898s, episode steps: 245, steps per second: 129, episode reward: -69.288, mean reward: -0.283 [-11.331, 0.242], mean action: -0.512 [-1.129, 1.071], mean observation: -0.172 [-7.971, 3.733], loss: 0.849490, mean_absolute_error: 0.866693, mean_q: 12.148932
 542767/1000000: episode: 1649, duration: 2.067s, episode steps: 261, steps per second: 126, episode reward: -15.524, mean reward: -0.059 [-11.030, 0.531], mean action: -0.514 [-1.156, 1.109], mean observation: -0.185 [-7.928, 4.895], loss: 0.780826, mean_absolute_error: 0.857212, mean_q: 12.276449
 542998/1000000: episode: 1650, duration: 1.728s, episode steps: 231, steps per second: 134, episode reward: -9.530, mean reward: -0.041 [-11.232, 0.426], mean action: -0.502 [-1.092, 1.058], mean observation: -0.182 [-7.940, 1.685], loss: 0.798998, mean_absolute_error: 0.858400, mean_q: 12.263603
 543227/1000000: episode: 1651, duration: 1.837s, episode steps: 229, steps per second: 125, episode reward: 19.551, mean reward: 0.085 [-10.973, 0.475], mean action: -0.502 [-1.173, 1.134], mean observation: -0.169 [-8.026, 1.562], loss: 0.832396, mean_absolute_error: 0.865682, mean_q: 12.327574
 543472/1000000: episode: 1652, duration: 1.873s, episode steps: 245, steps per second: 131, episode reward: -19.498, mean reward: -0.080 [-11.163, 0.415], mean action: -0.497 [-1.089, 1.077], mean observation: -0.175 [-7.934, 3.427], loss: 0.785464, mean_absolute_error: 0.860641, mean_q: 12.233989
 543709/1000000: episode: 1653, duration: 1.802s, episode steps: 237, steps per second: 131, episode reward: 4.048, mean reward: 0.017 [-11.113, 0.453], mean action: -0.504 [-1.154, 1.076], mean observation: -0.187 [-7.948, 2.066], loss: 0.766460, mean_absolute_error: 0.855175, mean_q: 12.144204
 543943/1000000: episode: 1654, duration: 1.862s, episode steps: 234, steps per second: 126, episode reward: -30.657, mean reward: -0.131 [-11.218, 0.288], mean action: -0.509 [-1.120, 1.070], mean observation: -0.190 [-7.883, 1.761], loss: 0.756227, mean_absolute_error: 0.844613, mean_q: 12.223145
 544221/1000000: episode: 1655, duration: 2.084s, episode steps: 278, steps per second: 133, episode reward: -41.618, mean reward: -0.150 [-10.831, 0.481], mean action: -0.494 [-1.078, 1.040], mean observation: -0.194 [-7.916, 5.271], loss: 0.867470, mean_absolute_error: 0.875633, mean_q: 12.190774
 544466/1000000: episode: 1656, duration: 1.833s, episode steps: 245, steps per second: 134, episode reward: -45.049, mean reward: -0.184 [-11.302, 0.356], mean action: -0.502 [-1.150, 1.103], mean observation: -0.187 [-7.952, 3.554], loss: 0.886802, mean_absolute_error: 0.878956, mean_q: 12.341155
 544660/1000000: episode: 1657, duration: 1.443s, episode steps: 194, steps per second: 134, episode reward: 25.372, mean reward: 0.131 [-10.641, 0.360], mean action: -0.501 [-1.092, 1.013], mean observation: -0.115 [-6.568, 1.000], loss: 0.784887, mean_absolute_error: 0.862163, mean_q: 12.170103
 544893/1000000: episode: 1658, duration: 1.736s, episode steps: 233, steps per second: 134, episode reward: 8.562, mean reward: 0.037 [-11.067, 0.453], mean action: -0.475 [-1.117, 1.076], mean observation: -0.160 [-7.928, 1.741], loss: 0.795195, mean_absolute_error: 0.858633, mean_q: 12.214819
 545116/1000000: episode: 1659, duration: 1.659s, episode steps: 223, steps per second: 134, episode reward: -16.178, mean reward: -0.073 [-11.068, 0.279], mean action: -0.509 [-1.151, 1.109], mean observation: -0.176 [-8.003, 1.000], loss: 0.764690, mean_absolute_error: 0.852918, mean_q: 12.350429
 545365/1000000: episode: 1660, duration: 1.935s, episode steps: 249, steps per second: 129, episode reward: -54.271, mean reward: -0.218 [-11.320, 0.336], mean action: -0.499 [-1.076, 1.099], mean observation: -0.186 [-7.945, 3.932], loss: 0.816810, mean_absolute_error: 0.863037, mean_q: 12.229022
 545563/1000000: episode: 1661, duration: 1.610s, episode steps: 198, steps per second: 123, episode reward: -28.652, mean reward: -0.145 [-11.060, 0.138], mean action: -0.488 [-1.078, 1.092], mean observation: -0.147 [-6.963, 1.000], loss: 0.827892, mean_absolute_error: 0.870882, mean_q: 12.127569
 545793/1000000: episode: 1662, duration: 1.944s, episode steps: 230, steps per second: 118, episode reward: -12.273, mean reward: -0.053 [-10.842, 0.239], mean action: -0.491 [-1.126, 1.050], mean observation: -0.156 [-7.934, 1.347], loss: 0.791142, mean_absolute_error: 0.859478, mean_q: 12.205395
 546063/1000000: episode: 1663, duration: 2.101s, episode steps: 270, steps per second: 129, episode reward: -82.409, mean reward: -0.305 [-11.028, 0.278], mean action: -0.498 [-1.119, 1.089], mean observation: -0.213 [-7.937, 5.240], loss: 0.818526, mean_absolute_error: 0.864111, mean_q: 12.169320
 546339/1000000: episode: 1664, duration: 2.142s, episode steps: 276, steps per second: 129, episode reward: -52.239, mean reward: -0.189 [-10.794, 0.394], mean action: -0.502 [-1.113, 1.056], mean observation: -0.175 [-7.977, 5.211], loss: 0.897290, mean_absolute_error: 0.882718, mean_q: 12.173770
 546511/1000000: episode: 1665, duration: 1.377s, episode steps: 172, steps per second: 125, episode reward: -5.803, mean reward: -0.034 [-10.571, 0.150], mean action: -0.490 [-1.109, 1.105], mean observation: -0.076 [-6.014, 1.000], loss: 0.791988, mean_absolute_error: 0.865089, mean_q: 12.145732
 546786/1000000: episode: 1666, duration: 2.134s, episode steps: 275, steps per second: 129, episode reward: 3.379, mean reward: 0.012 [-10.583, 0.490], mean action: -0.505 [-1.107, 1.059], mean observation: -0.179 [-7.900, 5.281], loss: 0.889872, mean_absolute_error: 0.875763, mean_q: 12.161585
 547057/1000000: episode: 1667, duration: 2.059s, episode steps: 271, steps per second: 132, episode reward: -27.157, mean reward: -0.100 [-10.468, 0.289], mean action: -0.486 [-1.098, 1.113], mean observation: -0.146 [-7.983, 5.211], loss: 0.781207, mean_absolute_error: 0.866444, mean_q: 12.254090
 547324/1000000: episode: 1668, duration: 2.096s, episode steps: 267, steps per second: 127, episode reward: -67.391, mean reward: -0.252 [-10.882, 0.306], mean action: -0.503 [-1.141, 1.141], mean observation: -0.200 [-8.038, 5.115], loss: 0.897243, mean_absolute_error: 0.873807, mean_q: 12.262542
 547569/1000000: episode: 1669, duration: 2.046s, episode steps: 245, steps per second: 120, episode reward: 4.695, mean reward: 0.019 [-10.800, 0.389], mean action: -0.498 [-1.175, 1.130], mean observation: -0.174 [-7.951, 3.654], loss: 0.905149, mean_absolute_error: 0.878858, mean_q: 12.233452
 547652/1000000: episode: 1670, duration: 0.657s, episode steps: 83, steps per second: 126, episode reward: -7.443, mean reward: -0.090 [-9.982, 0.034], mean action: -0.517 [-1.075, 1.027], mean observation: 0.058 [-2.526, 1.000], loss: 0.853936, mean_absolute_error: 0.875958, mean_q: 12.270253
 547913/1000000: episode: 1671, duration: 2.049s, episode steps: 261, steps per second: 127, episode reward: -74.800, mean reward: -0.287 [-11.161, 0.286], mean action: -0.485 [-1.123, 1.158], mean observation: -0.181 [-7.980, 4.983], loss: 0.798441, mean_absolute_error: 0.868645, mean_q: 12.163455
 548118/1000000: episode: 1672, duration: 1.626s, episode steps: 205, steps per second: 126, episode reward: -10.237, mean reward: -0.050 [-11.078, 0.275], mean action: -0.505 [-1.125, 1.114], mean observation: -0.147 [-7.608, 1.000], loss: 0.856612, mean_absolute_error: 0.876385, mean_q: 12.187120
 548310/1000000: episode: 1673, duration: 1.459s, episode steps: 192, steps per second: 132, episode reward: 14.695, mean reward: 0.077 [-10.670, 0.301], mean action: -0.506 [-1.090, 1.084], mean observation: -0.120 [-6.739, 1.000], loss: 0.871396, mean_absolute_error: 0.886191, mean_q: 12.226686
 548530/1000000: episode: 1674, duration: 1.689s, episode steps: 220, steps per second: 130, episode reward: -23.048, mean reward: -0.105 [-10.935, 0.165], mean action: -0.508 [-1.101, 1.039], mean observation: -0.152 [-7.873, 1.000], loss: 0.848988, mean_absolute_error: 0.874722, mean_q: 12.175423
 548754/1000000: episode: 1675, duration: 1.750s, episode steps: 224, steps per second: 128, episode reward: -29.196, mean reward: -0.130 [-10.927, 0.142], mean action: -0.507 [-1.125, 1.079], mean observation: -0.123 [-7.929, 1.000], loss: 0.864353, mean_absolute_error: 0.878496, mean_q: 12.343986
 548993/1000000: episode: 1676, duration: 2.120s, episode steps: 239, steps per second: 113, episode reward: -60.239, mean reward: -0.252 [-11.438, 0.240], mean action: -0.496 [-1.093, 1.148], mean observation: -0.198 [-7.944, 2.419], loss: 0.986532, mean_absolute_error: 0.889181, mean_q: 12.232615
 549226/1000000: episode: 1677, duration: 2.064s, episode steps: 233, steps per second: 113, episode reward: -23.928, mean reward: -0.103 [-10.916, 0.222], mean action: -0.514 [-1.179, 1.120], mean observation: -0.176 [-8.024, 2.082], loss: 0.824065, mean_absolute_error: 0.870514, mean_q: 12.349613
 549357/1000000: episode: 1678, duration: 1.056s, episode steps: 131, steps per second: 124, episode reward: 0.398, mean reward: 0.003 [-10.071, 0.108], mean action: -0.498 [-1.096, 1.078], mean observation: -0.008 [-5.283, 1.000], loss: 0.920221, mean_absolute_error: 0.887914, mean_q: 12.290797
 549580/1000000: episode: 1679, duration: 1.750s, episode steps: 223, steps per second: 127, episode reward: -53.560, mean reward: -0.240 [-11.345, 0.152], mean action: -0.513 [-1.108, 1.066], mean observation: -0.149 [-7.938, 1.000], loss: 0.921455, mean_absolute_error: 0.888292, mean_q: 12.296569
 549838/1000000: episode: 1680, duration: 1.934s, episode steps: 258, steps per second: 133, episode reward: -43.614, mean reward: -0.169 [-11.097, 0.429], mean action: -0.538 [-1.171, 1.073], mean observation: -0.198 [-8.005, 4.927], loss: 0.901384, mean_absolute_error: 0.883326, mean_q: 12.247526
 549976/1000000: episode: 1681, duration: 1.044s, episode steps: 138, steps per second: 132, episode reward: 1.025, mean reward: 0.007 [-10.076, 0.107], mean action: -0.527 [-1.083, 0.999], mean observation: 0.033 [-5.468, 1.000], loss: 0.880045, mean_absolute_error: 0.876682, mean_q: 12.169062
 550238/1000000: episode: 1682, duration: 1.982s, episode steps: 262, steps per second: 132, episode reward: -48.788, mean reward: -0.186 [-10.929, 0.319], mean action: -0.501 [-1.085, 1.147], mean observation: -0.196 [-7.995, 5.019], loss: 0.857345, mean_absolute_error: 0.879707, mean_q: 12.271173
 550491/1000000: episode: 1683, duration: 1.885s, episode steps: 253, steps per second: 134, episode reward: -32.023, mean reward: -0.127 [-10.778, 0.233], mean action: -0.491 [-1.095, 1.059], mean observation: -0.147 [-7.903, 4.298], loss: 0.808378, mean_absolute_error: 0.861631, mean_q: 12.254159
 550759/1000000: episode: 1684, duration: 2.024s, episode steps: 268, steps per second: 132, episode reward: 36.608, mean reward: 0.137 [-10.703, 0.729], mean action: -0.528 [-1.127, 1.071], mean observation: -0.174 [-7.981, 5.183], loss: 0.847252, mean_absolute_error: 0.884154, mean_q: 12.269613
 550992/1000000: episode: 1685, duration: 1.745s, episode steps: 233, steps per second: 134, episode reward: -28.533, mean reward: -0.122 [-11.058, 0.241], mean action: -0.524 [-1.209, 1.059], mean observation: -0.151 [-7.978, 1.923], loss: 0.859903, mean_absolute_error: 0.878375, mean_q: 12.180878
 551268/1000000: episode: 1686, duration: 2.087s, episode steps: 276, steps per second: 132, episode reward: -6.890, mean reward: -0.025 [-10.755, 0.628], mean action: -0.514 [-1.101, 1.060], mean observation: -0.186 [-7.949, 5.239], loss: 0.764459, mean_absolute_error: 0.864011, mean_q: 12.275684
 551481/1000000: episode: 1687, duration: 1.600s, episode steps: 213, steps per second: 133, episode reward: -9.556, mean reward: -0.045 [-10.919, 0.225], mean action: -0.478 [-1.086, 1.113], mean observation: -0.112 [-7.803, 1.000], loss: 0.860154, mean_absolute_error: 0.886596, mean_q: 12.249266
 551702/1000000: episode: 1688, duration: 1.658s, episode steps: 221, steps per second: 133, episode reward: 18.029, mean reward: 0.082 [-11.007, 0.445], mean action: -0.492 [-1.104, 1.094], mean observation: -0.143 [-7.993, 1.000], loss: 0.878953, mean_absolute_error: 0.881561, mean_q: 12.103422
 551912/1000000: episode: 1689, duration: 1.576s, episode steps: 210, steps per second: 133, episode reward: 4.335, mean reward: 0.021 [-10.946, 0.323], mean action: -0.509 [-1.161, 1.019], mean observation: -0.132 [-7.683, 1.000], loss: 0.820758, mean_absolute_error: 0.870136, mean_q: 12.284120
 552137/1000000: episode: 1690, duration: 1.697s, episode steps: 225, steps per second: 133, episode reward: -23.748, mean reward: -0.106 [-11.286, 0.343], mean action: -0.495 [-1.107, 1.100], mean observation: -0.180 [-7.984, 1.052], loss: 0.951560, mean_absolute_error: 0.900652, mean_q: 12.100472
 552396/1000000: episode: 1691, duration: 1.933s, episode steps: 259, steps per second: 134, episode reward: -45.260, mean reward: -0.175 [-11.144, 0.417], mean action: -0.463 [-1.052, 1.090], mean observation: -0.194 [-7.954, 4.835], loss: 0.817408, mean_absolute_error: 0.876341, mean_q: 12.314463
 552639/1000000: episode: 1692, duration: 1.817s, episode steps: 243, steps per second: 134, episode reward: -2.104, mean reward: -0.009 [-10.976, 0.408], mean action: -0.504 [-1.102, 1.071], mean observation: -0.185 [-7.930, 3.220], loss: 0.812069, mean_absolute_error: 0.881641, mean_q: 12.231325
 552837/1000000: episode: 1693, duration: 1.482s, episode steps: 198, steps per second: 134, episode reward: 9.470, mean reward: 0.048 [-10.619, 0.248], mean action: -0.499 [-1.072, 1.040], mean observation: -0.098 [-6.982, 1.000], loss: 0.828799, mean_absolute_error: 0.878195, mean_q: 12.275661
 553053/1000000: episode: 1694, duration: 1.621s, episode steps: 216, steps per second: 133, episode reward: 25.202, mean reward: 0.117 [-10.914, 0.440], mean action: -0.481 [-1.104, 1.117], mean observation: -0.151 [-7.910, 1.000], loss: 0.866151, mean_absolute_error: 0.879519, mean_q: 12.297904
 553274/1000000: episode: 1695, duration: 1.658s, episode steps: 221, steps per second: 133, episode reward: -39.407, mean reward: -0.178 [-11.347, 0.219], mean action: -0.496 [-1.077, 1.043], mean observation: -0.171 [-7.865, 1.000], loss: 0.753840, mean_absolute_error: 0.865236, mean_q: 12.182590
 553491/1000000: episode: 1696, duration: 1.640s, episode steps: 217, steps per second: 132, episode reward: 13.018, mean reward: 0.060 [-10.841, 0.352], mean action: -0.496 [-1.106, 1.096], mean observation: -0.148 [-7.991, 1.000], loss: 0.870770, mean_absolute_error: 0.889226, mean_q: 12.318996
 553718/1000000: episode: 1697, duration: 1.704s, episode steps: 227, steps per second: 133, episode reward: -48.345, mean reward: -0.213 [-11.324, 0.225], mean action: -0.512 [-1.135, 1.128], mean observation: -0.194 [-8.062, 1.597], loss: 0.858269, mean_absolute_error: 0.879145, mean_q: 12.260580
 553950/1000000: episode: 1698, duration: 1.728s, episode steps: 232, steps per second: 134, episode reward: 4.515, mean reward: 0.019 [-10.842, 0.337], mean action: -0.519 [-1.177, 1.070], mean observation: -0.157 [-7.988, 1.726], loss: 0.897274, mean_absolute_error: 0.889533, mean_q: 12.183295
 554221/1000000: episode: 1699, duration: 2.062s, episode steps: 271, steps per second: 131, episode reward: 5.509, mean reward: 0.020 [-10.792, 0.660], mean action: -0.497 [-1.104, 1.108], mean observation: -0.190 [-7.947, 5.239], loss: 0.797200, mean_absolute_error: 0.874828, mean_q: 12.195543
 554438/1000000: episode: 1700, duration: 1.621s, episode steps: 217, steps per second: 134, episode reward: 18.144, mean reward: 0.084 [-10.851, 0.385], mean action: -0.508 [-1.117, 1.088], mean observation: -0.152 [-7.903, 1.000], loss: 0.859117, mean_absolute_error: 0.887595, mean_q: 12.166555
 554624/1000000: episode: 1701, duration: 1.398s, episode steps: 186, steps per second: 133, episode reward: 15.961, mean reward: 0.086 [-10.547, 0.278], mean action: -0.503 [-1.105, 1.116], mean observation: -0.071 [-6.160, 1.000], loss: 0.821114, mean_absolute_error: 0.881109, mean_q: 12.217498
 554897/1000000: episode: 1702, duration: 2.044s, episode steps: 273, steps per second: 134, episode reward: -50.263, mean reward: -0.184 [-10.757, 0.408], mean action: -0.513 [-1.179, 1.175], mean observation: -0.195 [-8.069, 5.130], loss: 0.826369, mean_absolute_error: 0.876173, mean_q: 12.286868
 555112/1000000: episode: 1703, duration: 1.609s, episode steps: 215, steps per second: 134, episode reward: -26.304, mean reward: -0.122 [-11.199, 0.230], mean action: -0.501 [-1.088, 1.084], mean observation: -0.143 [-7.741, 1.000], loss: 0.827382, mean_absolute_error: 0.870667, mean_q: 12.350986
 555360/1000000: episode: 1704, duration: 1.865s, episode steps: 248, steps per second: 133, episode reward: -27.927, mean reward: -0.113 [-11.063, 0.319], mean action: -0.525 [-1.100, 1.025], mean observation: -0.156 [-7.891, 3.520], loss: 0.842750, mean_absolute_error: 0.879127, mean_q: 12.289599
 555540/1000000: episode: 1705, duration: 1.367s, episode steps: 180, steps per second: 132, episode reward: 14.340, mean reward: 0.080 [-10.391, 0.233], mean action: -0.493 [-1.077, 1.096], mean observation: -0.064 [-6.010, 1.000], loss: 0.858398, mean_absolute_error: 0.889731, mean_q: 12.286879
 555747/1000000: episode: 1706, duration: 1.552s, episode steps: 207, steps per second: 133, episode reward: 10.181, mean reward: 0.049 [-10.936, 0.357], mean action: -0.494 [-1.126, 1.121], mean observation: -0.146 [-7.750, 1.000], loss: 0.913277, mean_absolute_error: 0.895374, mean_q: 12.270425
 555992/1000000: episode: 1707, duration: 1.930s, episode steps: 245, steps per second: 127, episode reward: -7.246, mean reward: -0.030 [-11.147, 0.465], mean action: -0.516 [-1.120, 1.034], mean observation: -0.174 [-7.902, 3.257], loss: 0.833226, mean_absolute_error: 0.881753, mean_q: 12.331964
 556165/1000000: episode: 1708, duration: 1.315s, episode steps: 173, steps per second: 132, episode reward: 13.825, mean reward: 0.080 [-10.466, 0.261], mean action: -0.485 [-1.128, 1.160], mean observation: -0.066 [-6.070, 1.000], loss: 0.768984, mean_absolute_error: 0.881405, mean_q: 12.240770
 556401/1000000: episode: 1709, duration: 1.787s, episode steps: 236, steps per second: 132, episode reward: -17.938, mean reward: -0.076 [-11.204, 0.399], mean action: -0.515 [-1.119, 1.111], mean observation: -0.191 [-7.986, 2.563], loss: 0.884027, mean_absolute_error: 0.889320, mean_q: 12.247645
 556649/1000000: episode: 1710, duration: 1.944s, episode steps: 248, steps per second: 128, episode reward: 8.449, mean reward: 0.034 [-10.777, 0.399], mean action: -0.509 [-1.131, 1.080], mean observation: -0.155 [-7.944, 3.781], loss: 0.811093, mean_absolute_error: 0.879803, mean_q: 12.444491
 556884/1000000: episode: 1711, duration: 1.794s, episode steps: 235, steps per second: 131, episode reward: -62.731, mean reward: -0.267 [-11.411, 0.220], mean action: -0.506 [-1.093, 1.096], mean observation: -0.197 [-7.997, 2.421], loss: 0.951570, mean_absolute_error: 0.905259, mean_q: 12.225629
 557137/1000000: episode: 1712, duration: 1.943s, episode steps: 253, steps per second: 130, episode reward: -1.046, mean reward: -0.004 [-11.012, 0.529], mean action: -0.501 [-1.110, 1.084], mean observation: -0.189 [-8.011, 4.369], loss: 0.943844, mean_absolute_error: 0.898641, mean_q: 12.287130
 557393/1000000: episode: 1713, duration: 1.928s, episode steps: 256, steps per second: 133, episode reward: 6.264, mean reward: 0.024 [-10.716, 0.396], mean action: -0.500 [-1.101, 1.080], mean observation: -0.171 [-7.894, 4.418], loss: 0.875895, mean_absolute_error: 0.890449, mean_q: 12.411472
 557566/1000000: episode: 1714, duration: 1.307s, episode steps: 173, steps per second: 132, episode reward: -14.099, mean reward: -0.081 [-10.631, 0.104], mean action: -0.485 [-1.108, 1.083], mean observation: -0.088 [-6.015, 1.000], loss: 0.881536, mean_absolute_error: 0.890048, mean_q: 12.395144
 557791/1000000: episode: 1715, duration: 1.678s, episode steps: 225, steps per second: 134, episode reward: -2.661, mean reward: -0.012 [-11.117, 0.366], mean action: -0.499 [-1.121, 1.088], mean observation: -0.172 [-7.901, 1.000], loss: 0.848679, mean_absolute_error: 0.887242, mean_q: 12.238684
 558046/1000000: episode: 1716, duration: 1.904s, episode steps: 255, steps per second: 134, episode reward: -26.973, mean reward: -0.106 [-10.729, 0.242], mean action: -0.521 [-1.134, 1.071], mean observation: -0.172 [-7.909, 4.360], loss: 0.892449, mean_absolute_error: 0.883336, mean_q: 12.342854
 558258/1000000: episode: 1717, duration: 1.587s, episode steps: 212, steps per second: 134, episode reward: -9.782, mean reward: -0.046 [-11.162, 0.331], mean action: -0.521 [-1.150, 1.054], mean observation: -0.165 [-7.923, 1.000], loss: 0.769104, mean_absolute_error: 0.879072, mean_q: 12.376563
 558471/1000000: episode: 1718, duration: 1.606s, episode steps: 213, steps per second: 133, episode reward: -28.463, mean reward: -0.134 [-11.228, 0.224], mean action: -0.530 [-1.141, 1.079], mean observation: -0.171 [-7.848, 1.000], loss: 0.816813, mean_absolute_error: 0.880374, mean_q: 12.362942
 558692/1000000: episode: 1719, duration: 1.647s, episode steps: 221, steps per second: 134, episode reward: -24.623, mean reward: -0.111 [-10.904, 0.158], mean action: -0.497 [-1.141, 1.056], mean observation: -0.147 [-7.933, 1.000], loss: 0.898630, mean_absolute_error: 0.888996, mean_q: 12.332374
 558943/1000000: episode: 1720, duration: 1.884s, episode steps: 251, steps per second: 133, episode reward: 9.458, mean reward: 0.038 [-10.932, 0.513], mean action: -0.494 [-1.139, 1.091], mean observation: -0.163 [-7.981, 4.139], loss: 0.828412, mean_absolute_error: 0.873364, mean_q: 12.295079
 559207/1000000: episode: 1721, duration: 1.989s, episode steps: 264, steps per second: 133, episode reward: -6.701, mean reward: -0.025 [-10.853, 0.504], mean action: -0.506 [-1.093, 1.067], mean observation: -0.192 [-7.952, 5.043], loss: 0.815143, mean_absolute_error: 0.880401, mean_q: 12.295429
 559449/1000000: episode: 1722, duration: 1.816s, episode steps: 242, steps per second: 133, episode reward: -55.517, mean reward: -0.229 [-11.397, 0.287], mean action: -0.490 [-1.073, 1.098], mean observation: -0.192 [-7.953, 3.011], loss: 0.915714, mean_absolute_error: 0.895462, mean_q: 12.452469
 559697/1000000: episode: 1723, duration: 1.856s, episode steps: 248, steps per second: 134, episode reward: 6.157, mean reward: 0.025 [-10.977, 0.503], mean action: -0.501 [-1.122, 1.125], mean observation: -0.185 [-7.934, 3.921], loss: 0.877646, mean_absolute_error: 0.892395, mean_q: 12.425228
 559852/1000000: episode: 1724, duration: 1.159s, episode steps: 155, steps per second: 134, episode reward: 11.915, mean reward: 0.077 [-10.160, 0.194], mean action: -0.510 [-1.137, 1.052], mean observation: -0.007 [-6.028, 1.000], loss: 0.809362, mean_absolute_error: 0.880876, mean_q: 12.283196
 560069/1000000: episode: 1725, duration: 1.623s, episode steps: 217, steps per second: 134, episode reward: -11.829, mean reward: -0.055 [-11.091, 0.294], mean action: -0.497 [-1.091, 1.068], mean observation: -0.171 [-7.954, 1.000], loss: 0.830279, mean_absolute_error: 0.883954, mean_q: 12.324605
 560295/1000000: episode: 1726, duration: 1.692s, episode steps: 226, steps per second: 134, episode reward: -39.821, mean reward: -0.176 [-11.382, 0.254], mean action: -0.517 [-1.141, 1.044], mean observation: -0.167 [-7.910, 1.000], loss: 0.919438, mean_absolute_error: 0.886788, mean_q: 12.314217
 560532/1000000: episode: 1727, duration: 1.783s, episode steps: 237, steps per second: 133, episode reward: -39.423, mean reward: -0.166 [-11.032, 0.179], mean action: -0.512 [-1.090, 1.067], mean observation: -0.174 [-7.899, 2.153], loss: 0.765347, mean_absolute_error: 0.864497, mean_q: 12.301564
 560771/1000000: episode: 1728, duration: 1.791s, episode steps: 239, steps per second: 133, episode reward: -52.966, mean reward: -0.222 [-10.962, 0.103], mean action: -0.492 [-1.157, 1.121], mean observation: -0.170 [-7.940, 2.640], loss: 0.873802, mean_absolute_error: 0.885884, mean_q: 12.323236
 561011/1000000: episode: 1729, duration: 1.791s, episode steps: 240, steps per second: 134, episode reward: 25.016, mean reward: 0.104 [-10.901, 0.512], mean action: -0.495 [-1.162, 1.092], mean observation: -0.177 [-7.996, 2.988], loss: 0.950478, mean_absolute_error: 0.891444, mean_q: 12.230659
 561232/1000000: episode: 1730, duration: 1.660s, episode steps: 221, steps per second: 133, episode reward: -50.558, mean reward: -0.229 [-11.403, 0.191], mean action: -0.517 [-1.123, 1.067], mean observation: -0.177 [-7.930, 1.000], loss: 0.808712, mean_absolute_error: 0.880554, mean_q: 12.395948
 561368/1000000: episode: 1731, duration: 1.028s, episode steps: 136, steps per second: 132, episode reward: -4.673, mean reward: -0.034 [-10.142, 0.073], mean action: -0.512 [-1.141, 1.084], mean observation: 0.019 [-5.499, 1.000], loss: 0.683510, mean_absolute_error: 0.854854, mean_q: 12.489258
 561583/1000000: episode: 1732, duration: 1.613s, episode steps: 215, steps per second: 133, episode reward: -47.444, mean reward: -0.221 [-11.326, 0.150], mean action: -0.501 [-1.112, 1.045], mean observation: -0.184 [-7.860, 1.000], loss: 0.903140, mean_absolute_error: 0.898645, mean_q: 12.324592
 561780/1000000: episode: 1733, duration: 1.485s, episode steps: 197, steps per second: 133, episode reward: -10.671, mean reward: -0.054 [-10.728, 0.151], mean action: -0.493 [-1.078, 1.106], mean observation: -0.115 [-7.046, 1.000], loss: 0.832521, mean_absolute_error: 0.887381, mean_q: 12.365545
 561969/1000000: episode: 1734, duration: 1.422s, episode steps: 189, steps per second: 133, episode reward: 22.304, mean reward: 0.118 [-10.543, 0.315], mean action: -0.494 [-1.113, 1.097], mean observation: -0.102 [-6.302, 1.000], loss: 0.787632, mean_absolute_error: 0.865161, mean_q: 12.277891
 562242/1000000: episode: 1735, duration: 2.055s, episode steps: 273, steps per second: 133, episode reward: 12.622, mean reward: 0.046 [-10.589, 0.583], mean action: -0.517 [-1.117, 1.056], mean observation: -0.163 [-7.973, 5.216], loss: 0.877483, mean_absolute_error: 0.885054, mean_q: 12.388978
 562495/1000000: episode: 1736, duration: 1.889s, episode steps: 253, steps per second: 134, episode reward: -4.508, mean reward: -0.018 [-10.621, 0.337], mean action: -0.482 [-1.101, 1.115], mean observation: -0.164 [-8.011, 4.608], loss: 0.863995, mean_absolute_error: 0.887050, mean_q: 12.330746
 562711/1000000: episode: 1737, duration: 1.624s, episode steps: 216, steps per second: 133, episode reward: 0.072, mean reward: 0.000 [-10.995, 0.323], mean action: -0.472 [-1.056, 1.114], mean observation: -0.162 [-7.892, 1.000], loss: 0.810592, mean_absolute_error: 0.883300, mean_q: 12.354139
 562945/1000000: episode: 1738, duration: 1.757s, episode steps: 234, steps per second: 133, episode reward: -5.052, mean reward: -0.022 [-11.139, 0.420], mean action: -0.491 [-1.097, 1.086], mean observation: -0.170 [-7.931, 2.067], loss: 0.827490, mean_absolute_error: 0.883360, mean_q: 12.413518
 563204/1000000: episode: 1739, duration: 1.956s, episode steps: 259, steps per second: 132, episode reward: 19.413, mean reward: 0.075 [-10.906, 0.618], mean action: -0.499 [-1.119, 1.024], mean observation: -0.177 [-7.887, 4.741], loss: 0.935683, mean_absolute_error: 0.903120, mean_q: 12.273854
 563455/1000000: episode: 1740, duration: 1.894s, episode steps: 251, steps per second: 133, episode reward: -85.723, mean reward: -0.342 [-11.384, 0.190], mean action: -0.513 [-1.124, 1.029], mean observation: -0.213 [-7.904, 4.004], loss: 0.831683, mean_absolute_error: 0.885597, mean_q: 12.454233
 563648/1000000: episode: 1741, duration: 1.448s, episode steps: 193, steps per second: 133, episode reward: -8.745, mean reward: -0.045 [-10.828, 0.193], mean action: -0.496 [-1.080, 1.073], mean observation: -0.129 [-6.613, 1.000], loss: 0.833020, mean_absolute_error: 0.876902, mean_q: 12.378624
 563850/1000000: episode: 1742, duration: 1.514s, episode steps: 202, steps per second: 133, episode reward: 13.151, mean reward: 0.065 [-10.864, 0.348], mean action: -0.541 [-1.206, 1.034], mean observation: -0.126 [-7.003, 1.000], loss: 0.799214, mean_absolute_error: 0.878974, mean_q: 12.276459
 564108/1000000: episode: 1743, duration: 1.927s, episode steps: 258, steps per second: 134, episode reward: -64.468, mean reward: -0.250 [-10.792, 0.115], mean action: -0.498 [-1.166, 1.102], mean observation: -0.146 [-7.950, 4.664], loss: 0.877688, mean_absolute_error: 0.883552, mean_q: 12.336814
 564287/1000000: episode: 1744, duration: 1.343s, episode steps: 179, steps per second: 133, episode reward: 10.121, mean reward: 0.057 [-10.375, 0.201], mean action: -0.501 [-1.125, 1.047], mean observation: -0.055 [-5.996, 1.000], loss: 0.917276, mean_absolute_error: 0.898105, mean_q: 12.256040
 564529/1000000: episode: 1745, duration: 1.813s, episode steps: 242, steps per second: 133, episode reward: 3.977, mean reward: 0.016 [-10.788, 0.369], mean action: -0.521 [-1.163, 1.123], mean observation: -0.166 [-7.993, 3.380], loss: 0.799280, mean_absolute_error: 0.875769, mean_q: 12.427262
 564765/1000000: episode: 1746, duration: 1.770s, episode steps: 236, steps per second: 133, episode reward: -43.144, mean reward: -0.183 [-11.365, 0.304], mean action: -0.512 [-1.080, 1.076], mean observation: -0.179 [-7.944, 2.098], loss: 0.858071, mean_absolute_error: 0.887504, mean_q: 12.344832
 565027/1000000: episode: 1747, duration: 1.957s, episode steps: 262, steps per second: 134, episode reward: -44.191, mean reward: -0.169 [-11.105, 0.424], mean action: -0.485 [-1.086, 1.085], mean observation: -0.183 [-7.901, 4.979], loss: 0.900686, mean_absolute_error: 0.898880, mean_q: 12.433782
 565251/1000000: episode: 1748, duration: 1.698s, episode steps: 224, steps per second: 132, episode reward: 6.602, mean reward: 0.029 [-11.069, 0.408], mean action: -0.479 [-1.177, 1.112], mean observation: -0.181 [-7.954, 1.000], loss: 0.957781, mean_absolute_error: 0.895778, mean_q: 12.406466
 565519/1000000: episode: 1749, duration: 2.011s, episode steps: 268, steps per second: 133, episode reward: -50.956, mean reward: -0.190 [-10.926, 0.328], mean action: -0.491 [-1.113, 1.080], mean observation: -0.167 [-7.893, 5.152], loss: 0.908560, mean_absolute_error: 0.884234, mean_q: 12.336453
 565774/1000000: episode: 1750, duration: 1.900s, episode steps: 255, steps per second: 134, episode reward: 6.206, mean reward: 0.024 [-10.992, 0.569], mean action: -0.504 [-1.116, 1.075], mean observation: -0.176 [-7.933, 4.484], loss: 0.866481, mean_absolute_error: 0.880448, mean_q: 12.349678
 565988/1000000: episode: 1751, duration: 1.607s, episode steps: 214, steps per second: 133, episode reward: 3.052, mean reward: 0.014 [-11.045, 0.357], mean action: -0.472 [-1.130, 1.114], mean observation: -0.156 [-7.817, 1.000], loss: 0.884504, mean_absolute_error: 0.884480, mean_q: 12.353754
 566257/1000000: episode: 1752, duration: 2.013s, episode steps: 269, steps per second: 134, episode reward: 18.856, mean reward: 0.070 [-10.654, 0.575], mean action: -0.507 [-1.099, 1.071], mean observation: -0.183 [-7.956, 5.208], loss: 0.831948, mean_absolute_error: 0.871463, mean_q: 12.385426
 566509/1000000: episode: 1753, duration: 1.900s, episode steps: 252, steps per second: 133, episode reward: -58.623, mean reward: -0.233 [-11.178, 0.313], mean action: -0.505 [-1.126, 1.123], mean observation: -0.177 [-7.994, 4.547], loss: 0.886468, mean_absolute_error: 0.890288, mean_q: 12.432196
 566730/1000000: episode: 1754, duration: 1.654s, episode steps: 221, steps per second: 134, episode reward: 6.381, mean reward: 0.029 [-11.074, 0.392], mean action: -0.497 [-1.093, 1.079], mean observation: -0.166 [-7.897, 1.000], loss: 0.914475, mean_absolute_error: 0.896415, mean_q: 12.408751
 566969/1000000: episode: 1755, duration: 1.795s, episode steps: 239, steps per second: 133, episode reward: 24.238, mean reward: 0.101 [-10.994, 0.542], mean action: -0.481 [-1.097, 1.154], mean observation: -0.162 [-7.989, 2.695], loss: 0.856766, mean_absolute_error: 0.882920, mean_q: 12.409737
 567243/1000000: episode: 1756, duration: 2.056s, episode steps: 274, steps per second: 133, episode reward: -0.624, mean reward: -0.002 [-10.779, 0.647], mean action: -0.499 [-1.072, 1.053], mean observation: -0.187 [-7.944, 5.243], loss: 0.936975, mean_absolute_error: 0.900141, mean_q: 12.385240
 567482/1000000: episode: 1757, duration: 1.795s, episode steps: 239, steps per second: 133, episode reward: 11.139, mean reward: 0.047 [-10.959, 0.432], mean action: -0.492 [-1.070, 1.131], mean observation: -0.175 [-7.913, 2.408], loss: 0.865218, mean_absolute_error: 0.883630, mean_q: 12.318606
 567724/1000000: episode: 1758, duration: 1.805s, episode steps: 242, steps per second: 134, episode reward: -35.327, mean reward: -0.146 [-11.361, 0.358], mean action: -0.497 [-1.132, 1.031], mean observation: -0.185 [-7.864, 2.507], loss: 0.833444, mean_absolute_error: 0.878776, mean_q: 12.467052
 567938/1000000: episode: 1759, duration: 1.599s, episode steps: 214, steps per second: 134, episode reward: -3.495, mean reward: -0.016 [-11.058, 0.325], mean action: -0.505 [-1.184, 1.113], mean observation: -0.164 [-7.916, 1.000], loss: 0.827199, mean_absolute_error: 0.874703, mean_q: 12.473621
 568179/1000000: episode: 1760, duration: 1.811s, episode steps: 241, steps per second: 133, episode reward: -94.991, mean reward: -0.394 [-11.479, 0.100], mean action: -0.522 [-1.167, 1.077], mean observation: -0.216 [-7.973, 3.122], loss: 0.875213, mean_absolute_error: 0.883755, mean_q: 12.518023
 568410/1000000: episode: 1761, duration: 1.722s, episode steps: 231, steps per second: 134, episode reward: -36.533, mean reward: -0.158 [-11.231, 0.218], mean action: -0.502 [-1.061, 1.043], mean observation: -0.138 [-7.831, 1.000], loss: 0.902702, mean_absolute_error: 0.883213, mean_q: 12.346494
 568607/1000000: episode: 1762, duration: 1.484s, episode steps: 197, steps per second: 133, episode reward: 19.451, mean reward: 0.099 [-10.617, 0.313], mean action: -0.510 [-1.126, 1.046], mean observation: -0.105 [-6.980, 1.000], loss: 0.818552, mean_absolute_error: 0.880535, mean_q: 12.375319
 568821/1000000: episode: 1763, duration: 1.605s, episode steps: 214, steps per second: 133, episode reward: -0.081, mean reward: -0.000 [-10.853, 0.272], mean action: -0.487 [-1.096, 1.091], mean observation: -0.151 [-7.960, 1.000], loss: 0.915700, mean_absolute_error: 0.892080, mean_q: 12.406837
 569072/1000000: episode: 1764, duration: 1.882s, episode steps: 251, steps per second: 133, episode reward: -36.666, mean reward: -0.146 [-10.940, 0.282], mean action: -0.503 [-1.098, 1.115], mean observation: -0.148 [-7.983, 4.278], loss: 0.876505, mean_absolute_error: 0.883607, mean_q: 12.381104
 569241/1000000: episode: 1765, duration: 1.271s, episode steps: 169, steps per second: 133, episode reward: -9.780, mean reward: -0.058 [-10.387, 0.069], mean action: -0.510 [-1.125, 1.075], mean observation: -0.026 [-6.059, 1.000], loss: 0.907697, mean_absolute_error: 0.887546, mean_q: 12.413230
 569506/1000000: episode: 1766, duration: 1.987s, episode steps: 265, steps per second: 133, episode reward: -15.470, mean reward: -0.058 [-10.794, 0.465], mean action: -0.512 [-1.127, 1.076], mean observation: -0.163 [-7.932, 5.175], loss: 0.863572, mean_absolute_error: 0.887454, mean_q: 12.531822
 569780/1000000: episode: 1767, duration: 2.039s, episode steps: 274, steps per second: 134, episode reward: -48.600, mean reward: -0.177 [-10.678, 0.276], mean action: -0.490 [-1.067, 1.087], mean observation: -0.142 [-7.898, 5.276], loss: 0.861302, mean_absolute_error: 0.884340, mean_q: 12.342718
 570019/1000000: episode: 1768, duration: 1.802s, episode steps: 239, steps per second: 133, episode reward: -42.056, mean reward: -0.176 [-11.249, 0.275], mean action: -0.493 [-1.132, 1.127], mean observation: -0.196 [-7.973, 2.650], loss: 0.971657, mean_absolute_error: 0.897233, mean_q: 12.352280
 570282/1000000: episode: 1769, duration: 1.979s, episode steps: 263, steps per second: 133, episode reward: -42.914, mean reward: -0.163 [-11.147, 0.451], mean action: -0.486 [-1.062, 1.057], mean observation: -0.194 [-7.881, 4.967], loss: 0.863026, mean_absolute_error: 0.889801, mean_q: 12.452791
 570456/1000000: episode: 1770, duration: 1.306s, episode steps: 174, steps per second: 133, episode reward: -11.045, mean reward: -0.063 [-10.612, 0.120], mean action: -0.495 [-1.068, 1.068], mean observation: -0.093 [-6.018, 1.000], loss: 0.904607, mean_absolute_error: 0.879887, mean_q: 12.428851
 570669/1000000: episode: 1771, duration: 1.594s, episode steps: 213, steps per second: 134, episode reward: -7.181, mean reward: -0.034 [-10.834, 0.218], mean action: -0.511 [-1.117, 1.078], mean observation: -0.116 [-7.806, 1.000], loss: 0.799667, mean_absolute_error: 0.878901, mean_q: 12.431103
 570926/1000000: episode: 1772, duration: 1.929s, episode steps: 257, steps per second: 133, episode reward: -24.571, mean reward: -0.096 [-10.728, 0.259], mean action: -0.478 [-1.084, 1.110], mean observation: -0.149 [-7.896, 4.469], loss: 0.835215, mean_absolute_error: 0.883023, mean_q: 12.472198
 571093/1000000: episode: 1773, duration: 1.258s, episode steps: 167, steps per second: 133, episode reward: 32.379, mean reward: 0.194 [-10.163, 0.329], mean action: -0.498 [-1.116, 1.039], mean observation: -0.040 [-5.979, 1.000], loss: 0.879014, mean_absolute_error: 0.885991, mean_q: 12.390534
 571299/1000000: episode: 1774, duration: 1.552s, episode steps: 206, steps per second: 133, episode reward: 8.153, mean reward: 0.040 [-10.917, 0.338], mean action: -0.497 [-1.115, 1.077], mean observation: -0.127 [-7.643, 1.000], loss: 0.941192, mean_absolute_error: 0.890204, mean_q: 12.432845
 571547/1000000: episode: 1775, duration: 1.861s, episode steps: 248, steps per second: 133, episode reward: 7.765, mean reward: 0.031 [-10.836, 0.431], mean action: -0.511 [-1.115, 1.070], mean observation: -0.173 [-7.949, 3.863], loss: 0.876912, mean_absolute_error: 0.881885, mean_q: 12.513774
 571804/1000000: episode: 1776, duration: 1.921s, episode steps: 257, steps per second: 134, episode reward: 3.256, mean reward: 0.013 [-10.970, 0.608], mean action: -0.466 [-1.090, 1.164], mean observation: -0.185 [-7.947, 4.839], loss: 0.789726, mean_absolute_error: 0.873720, mean_q: 12.398406
 572038/1000000: episode: 1777, duration: 1.753s, episode steps: 234, steps per second: 133, episode reward: -57.531, mean reward: -0.246 [-11.441, 0.236], mean action: -0.494 [-1.074, 1.086], mean observation: -0.195 [-7.953, 1.932], loss: 0.982330, mean_absolute_error: 0.895317, mean_q: 12.352302
 572262/1000000: episode: 1778, duration: 1.676s, episode steps: 224, steps per second: 134, episode reward: -53.924, mean reward: -0.241 [-11.353, 0.167], mean action: -0.503 [-1.150, 1.088], mean observation: -0.195 [-8.002, 1.000], loss: 0.771507, mean_absolute_error: 0.872747, mean_q: 12.394876
 572530/1000000: episode: 1779, duration: 2.002s, episode steps: 268, steps per second: 134, episode reward: -43.482, mean reward: -0.162 [-10.481, 0.206], mean action: -0.490 [-1.104, 1.136], mean observation: -0.145 [-7.976, 5.211], loss: 0.808891, mean_absolute_error: 0.871723, mean_q: 12.466768
 572806/1000000: episode: 1780, duration: 2.085s, episode steps: 276, steps per second: 132, episode reward: -43.527, mean reward: -0.158 [-10.518, 0.232], mean action: -0.471 [-1.057, 1.141], mean observation: -0.138 [-7.880, 5.298], loss: 0.889908, mean_absolute_error: 0.875324, mean_q: 12.393153
 573021/1000000: episode: 1781, duration: 1.613s, episode steps: 215, steps per second: 133, episode reward: -2.784, mean reward: -0.013 [-11.127, 0.351], mean action: -0.498 [-1.091, 1.113], mean observation: -0.152 [-7.853, 1.000], loss: 0.928188, mean_absolute_error: 0.894463, mean_q: 12.306355
 573266/1000000: episode: 1782, duration: 1.835s, episode steps: 245, steps per second: 134, episode reward: -21.424, mean reward: -0.087 [-11.167, 0.452], mean action: -0.489 [-1.096, 1.139], mean observation: -0.184 [-8.019, 3.876], loss: 0.820003, mean_absolute_error: 0.874450, mean_q: 12.352461
 573534/1000000: episode: 1783, duration: 2.013s, episode steps: 268, steps per second: 133, episode reward: -15.224, mean reward: -0.057 [-10.906, 0.554], mean action: -0.478 [-1.091, 1.074], mean observation: -0.177 [-7.915, 5.221], loss: 0.891898, mean_absolute_error: 0.887927, mean_q: 12.470191
 573778/1000000: episode: 1784, duration: 1.828s, episode steps: 244, steps per second: 133, episode reward: -36.178, mean reward: -0.148 [-11.160, 0.283], mean action: -0.497 [-1.102, 1.063], mean observation: -0.147 [-7.902, 2.985], loss: 0.886200, mean_absolute_error: 0.880592, mean_q: 12.376384
 573989/1000000: episode: 1785, duration: 1.575s, episode steps: 211, steps per second: 134, episode reward: -13.911, mean reward: -0.066 [-11.037, 0.235], mean action: -0.510 [-1.111, 1.036], mean observation: -0.157 [-7.440, 1.000], loss: 0.949773, mean_absolute_error: 0.884765, mean_q: 12.438444
 574223/1000000: episode: 1786, duration: 1.759s, episode steps: 234, steps per second: 133, episode reward: 16.014, mean reward: 0.068 [-11.077, 0.486], mean action: -0.492 [-1.074, 1.042], mean observation: -0.174 [-7.841, 1.390], loss: 0.861067, mean_absolute_error: 0.880609, mean_q: 12.311441
 574472/1000000: episode: 1787, duration: 1.859s, episode steps: 249, steps per second: 134, episode reward: -85.325, mean reward: -0.343 [-11.400, 0.187], mean action: -0.519 [-1.147, 1.041], mean observation: -0.209 [-7.912, 3.875], loss: 0.890151, mean_absolute_error: 0.885229, mean_q: 12.420345
 574712/1000000: episode: 1788, duration: 1.794s, episode steps: 240, steps per second: 134, episode reward: -22.591, mean reward: -0.094 [-11.274, 0.404], mean action: -0.508 [-1.122, 1.052], mean observation: -0.186 [-7.916, 2.651], loss: 0.848357, mean_absolute_error: 0.869089, mean_q: 12.488353
 574923/1000000: episode: 1789, duration: 1.596s, episode steps: 211, steps per second: 132, episode reward: -27.155, mean reward: -0.129 [-10.944, 0.134], mean action: -0.515 [-1.105, 1.067], mean observation: -0.113 [-7.851, 1.000], loss: 0.985217, mean_absolute_error: 0.895184, mean_q: 12.422785
 575127/1000000: episode: 1790, duration: 1.529s, episode steps: 204, steps per second: 133, episode reward: -6.011, mean reward: -0.029 [-10.696, 0.171], mean action: -0.508 [-1.077, 1.033], mean observation: -0.108 [-7.162, 1.000], loss: 0.817198, mean_absolute_error: 0.874712, mean_q: 12.393483
 575370/1000000: episode: 1791, duration: 1.818s, episode steps: 243, steps per second: 134, episode reward: -60.466, mean reward: -0.249 [-11.376, 0.222], mean action: -0.494 [-1.135, 1.064], mean observation: -0.201 [-7.851, 2.629], loss: 0.867080, mean_absolute_error: 0.876680, mean_q: 12.490046
 575505/1000000: episode: 1792, duration: 1.031s, episode steps: 135, steps per second: 131, episode reward: 21.064, mean reward: 0.156 [-9.939, 0.261], mean action: -0.503 [-1.097, 1.021], mean observation: 0.006 [-5.406, 1.000], loss: 0.759138, mean_absolute_error: 0.868446, mean_q: 12.456315
 575735/1000000: episode: 1793, duration: 1.725s, episode steps: 230, steps per second: 133, episode reward: -32.051, mean reward: -0.139 [-11.298, 0.322], mean action: -0.497 [-1.128, 1.117], mean observation: -0.181 [-7.995, 1.784], loss: 0.936540, mean_absolute_error: 0.888992, mean_q: 12.444966
 575995/1000000: episode: 1794, duration: 1.942s, episode steps: 260, steps per second: 134, episode reward: -50.772, mean reward: -0.195 [-11.113, 0.369], mean action: -0.501 [-1.116, 1.108], mean observation: -0.205 [-7.930, 4.884], loss: 0.864990, mean_absolute_error: 0.871107, mean_q: 12.419088
 576238/1000000: episode: 1795, duration: 1.819s, episode steps: 243, steps per second: 134, episode reward: 16.260, mean reward: 0.067 [-11.010, 0.529], mean action: -0.493 [-1.090, 1.072], mean observation: -0.168 [-7.943, 3.213], loss: 0.890961, mean_absolute_error: 0.881291, mean_q: 12.526082
 576469/1000000: episode: 1796, duration: 1.732s, episode steps: 231, steps per second: 133, episode reward: -2.741, mean reward: -0.012 [-10.852, 0.298], mean action: -0.490 [-1.088, 1.135], mean observation: -0.154 [-7.927, 1.508], loss: 0.911291, mean_absolute_error: 0.877735, mean_q: 12.487329
 576722/1000000: episode: 1797, duration: 1.892s, episode steps: 253, steps per second: 134, episode reward: -53.212, mean reward: -0.210 [-11.073, 0.299], mean action: -0.507 [-1.158, 1.108], mean observation: -0.203 [-8.016, 4.616], loss: 0.873467, mean_absolute_error: 0.875791, mean_q: 12.454581
 576918/1000000: episode: 1798, duration: 1.485s, episode steps: 196, steps per second: 132, episode reward: -10.467, mean reward: -0.053 [-10.927, 0.215], mean action: -0.492 [-1.082, 1.079], mean observation: -0.140 [-7.101, 1.000], loss: 0.830645, mean_absolute_error: 0.873562, mean_q: 12.345409
 577166/1000000: episode: 1799, duration: 1.851s, episode steps: 248, steps per second: 134, episode reward: -21.479, mean reward: -0.087 [-10.791, 0.249], mean action: -0.495 [-1.100, 1.049], mean observation: -0.144 [-7.923, 3.637], loss: 0.886867, mean_absolute_error: 0.883841, mean_q: 12.355102
 577405/1000000: episode: 1800, duration: 1.799s, episode steps: 239, steps per second: 133, episode reward: -57.055, mean reward: -0.239 [-10.943, 0.097], mean action: -0.488 [-1.094, 1.101], mean observation: -0.178 [-8.006, 3.076], loss: 0.910825, mean_absolute_error: 0.879998, mean_q: 12.395952
 577660/1000000: episode: 1801, duration: 1.912s, episode steps: 255, steps per second: 133, episode reward: -56.545, mean reward: -0.222 [-11.307, 0.306], mean action: -0.490 [-1.050, 1.048], mean observation: -0.211 [-7.841, 3.951], loss: 0.821130, mean_absolute_error: 0.870833, mean_q: 12.447461
 577932/1000000: episode: 1802, duration: 2.034s, episode steps: 272, steps per second: 134, episode reward: 0.094, mean reward: 0.000 [-10.432, 0.438], mean action: -0.506 [-1.125, 1.140], mean observation: -0.182 [-8.037, 5.156], loss: 0.871216, mean_absolute_error: 0.873318, mean_q: 12.387266
 578176/1000000: episode: 1803, duration: 1.831s, episode steps: 244, steps per second: 133, episode reward: -56.723, mean reward: -0.232 [-11.364, 0.294], mean action: -0.509 [-1.138, 1.119], mean observation: -0.198 [-7.935, 3.313], loss: 0.792450, mean_absolute_error: 0.862132, mean_q: 12.381286
 578378/1000000: episode: 1804, duration: 1.516s, episode steps: 202, steps per second: 133, episode reward: 29.181, mean reward: 0.144 [-10.718, 0.400], mean action: -0.513 [-1.094, 1.022], mean observation: -0.121 [-7.128, 1.000], loss: 0.820366, mean_absolute_error: 0.860634, mean_q: 12.502025
 578559/1000000: episode: 1805, duration: 1.354s, episode steps: 181, steps per second: 134, episode reward: 26.236, mean reward: 0.145 [-10.382, 0.309], mean action: -0.509 [-1.163, 1.131], mean observation: -0.079 [-6.045, 1.000], loss: 0.794908, mean_absolute_error: 0.869277, mean_q: 12.601061
 578755/1000000: episode: 1806, duration: 1.470s, episode steps: 196, steps per second: 133, episode reward: -33.053, mean reward: -0.169 [-10.982, 0.079], mean action: -0.514 [-1.109, 1.013], mean observation: -0.139 [-6.550, 1.000], loss: 0.865126, mean_absolute_error: 0.879648, mean_q: 12.326149
 579005/1000000: episode: 1807, duration: 1.870s, episode steps: 250, steps per second: 134, episode reward: -7.194, mean reward: -0.029 [-11.012, 0.495], mean action: -0.489 [-1.109, 1.119], mean observation: -0.196 [-7.995, 4.339], loss: 0.840611, mean_absolute_error: 0.880302, mean_q: 12.520690
 579218/1000000: episode: 1808, duration: 1.608s, episode steps: 213, steps per second: 132, episode reward: -19.539, mean reward: -0.092 [-11.060, 0.218], mean action: -0.515 [-1.116, 1.054], mean observation: -0.123 [-7.767, 1.000], loss: 0.888858, mean_absolute_error: 0.877411, mean_q: 12.426582
 579476/1000000: episode: 1809, duration: 1.931s, episode steps: 258, steps per second: 134, episode reward: -45.532, mean reward: -0.176 [-11.049, 0.326], mean action: -0.484 [-1.144, 1.104], mean observation: -0.156 [-7.932, 4.653], loss: 0.935395, mean_absolute_error: 0.881003, mean_q: 12.516049
 579722/1000000: episode: 1810, duration: 1.846s, episode steps: 246, steps per second: 133, episode reward: -69.990, mean reward: -0.285 [-11.364, 0.224], mean action: -0.499 [-1.160, 1.083], mean observation: -0.199 [-7.974, 3.534], loss: 0.900268, mean_absolute_error: 0.882360, mean_q: 12.474036
 579936/1000000: episode: 1811, duration: 1.595s, episode steps: 214, steps per second: 134, episode reward: -35.652, mean reward: -0.167 [-11.260, 0.196], mean action: -0.502 [-1.097, 1.102], mean observation: -0.180 [-7.800, 1.000], loss: 0.865968, mean_absolute_error: 0.871898, mean_q: 12.500764
 580190/1000000: episode: 1812, duration: 1.893s, episode steps: 254, steps per second: 134, episode reward: -37.065, mean reward: -0.146 [-10.843, 0.228], mean action: -0.501 [-1.132, 1.060], mean observation: -0.139 [-7.916, 4.240], loss: 0.873029, mean_absolute_error: 0.871482, mean_q: 12.602418
 580468/1000000: episode: 1813, duration: 2.073s, episode steps: 278, steps per second: 134, episode reward: -52.584, mean reward: -0.189 [-10.687, 0.339], mean action: -0.462 [-1.091, 1.129], mean observation: -0.166 [-7.976, 5.178], loss: 0.771897, mean_absolute_error: 0.857876, mean_q: 12.506961
 580687/1000000: episode: 1814, duration: 1.633s, episode steps: 219, steps per second: 134, episode reward: -71.206, mean reward: -0.325 [-11.440, 0.057], mean action: -0.500 [-1.110, 1.056], mean observation: -0.190 [-7.887, 1.000], loss: 0.900891, mean_absolute_error: 0.875462, mean_q: 12.443559
 580928/1000000: episode: 1815, duration: 1.808s, episode steps: 241, steps per second: 133, episode reward: -56.916, mean reward: -0.236 [-11.358, 0.268], mean action: -0.494 [-1.115, 1.062], mean observation: -0.179 [-7.962, 2.979], loss: 0.909553, mean_absolute_error: 0.870754, mean_q: 12.439792
 581167/1000000: episode: 1816, duration: 1.794s, episode steps: 239, steps per second: 133, episode reward: 18.303, mean reward: 0.077 [-11.100, 0.539], mean action: -0.480 [-1.104, 1.080], mean observation: -0.173 [-7.870, 2.214], loss: 0.836069, mean_absolute_error: 0.868066, mean_q: 12.496922
 581419/1000000: episode: 1817, duration: 1.897s, episode steps: 252, steps per second: 133, episode reward: 18.900, mean reward: 0.075 [-10.976, 0.610], mean action: -0.493 [-1.088, 1.064], mean observation: -0.185 [-7.930, 4.255], loss: 0.871191, mean_absolute_error: 0.864384, mean_q: 12.511565
 581674/1000000: episode: 1818, duration: 1.908s, episode steps: 255, steps per second: 134, episode reward: -60.492, mean reward: -0.237 [-11.235, 0.300], mean action: -0.522 [-1.149, 1.059], mean observation: -0.181 [-7.929, 4.392], loss: 0.883974, mean_absolute_error: 0.870802, mean_q: 12.465588
 581919/1000000: episode: 1819, duration: 1.838s, episode steps: 245, steps per second: 133, episode reward: -55.211, mean reward: -0.225 [-10.954, 0.136], mean action: -0.497 [-1.141, 1.101], mean observation: -0.133 [-7.985, 3.580], loss: 0.857372, mean_absolute_error: 0.870926, mean_q: 12.448252
 582187/1000000: episode: 1820, duration: 1.994s, episode steps: 268, steps per second: 134, episode reward: 3.698, mean reward: 0.014 [-10.549, 0.459], mean action: -0.509 [-1.124, 1.094], mean observation: -0.165 [-7.973, 5.205], loss: 0.778901, mean_absolute_error: 0.851493, mean_q: 12.534059
 582459/1000000: episode: 1821, duration: 2.036s, episode steps: 272, steps per second: 134, episode reward: -20.868, mean reward: -0.077 [-10.843, 0.530], mean action: -0.514 [-1.144, 1.069], mean observation: -0.192 [-7.901, 5.280], loss: 0.785752, mean_absolute_error: 0.861679, mean_q: 12.594265
 582567/1000000: episode: 1822, duration: 0.818s, episode steps: 108, steps per second: 132, episode reward: 0.302, mean reward: 0.003 [-9.940, 0.101], mean action: -0.483 [-1.118, 1.175], mean observation: 0.084 [-4.099, 1.000], loss: 0.811259, mean_absolute_error: 0.866710, mean_q: 12.517458
 582733/1000000: episode: 1823, duration: 1.243s, episode steps: 166, steps per second: 134, episode reward: -23.750, mean reward: -0.143 [-10.433, -0.022], mean action: -0.480 [-1.091, 1.095], mean observation: -0.010 [-6.016, 1.000], loss: 0.841042, mean_absolute_error: 0.856361, mean_q: 12.568609
 582974/1000000: episode: 1824, duration: 1.802s, episode steps: 241, steps per second: 134, episode reward: -61.736, mean reward: -0.256 [-11.376, 0.235], mean action: -0.505 [-1.102, 1.048], mean observation: -0.200 [-7.930, 2.912], loss: 0.812131, mean_absolute_error: 0.860288, mean_q: 12.441730
 583237/1000000: episode: 1825, duration: 1.964s, episode steps: 263, steps per second: 134, episode reward: -27.113, mean reward: -0.103 [-11.047, 0.496], mean action: -0.483 [-1.075, 1.084], mean observation: -0.197 [-7.933, 4.995], loss: 0.778122, mean_absolute_error: 0.860162, mean_q: 12.536388
 583391/1000000: episode: 1826, duration: 1.160s, episode steps: 154, steps per second: 133, episode reward: 16.717, mean reward: 0.109 [-10.045, 0.207], mean action: -0.515 [-1.130, 1.021], mean observation: -0.012 [-5.912, 1.000], loss: 0.817377, mean_absolute_error: 0.869405, mean_q: 12.395511
 583647/1000000: episode: 1827, duration: 1.916s, episode steps: 256, steps per second: 134, episode reward: 17.107, mean reward: 0.067 [-10.765, 0.544], mean action: -0.501 [-1.118, 1.130], mean observation: -0.171 [-8.001, 4.798], loss: 0.793602, mean_absolute_error: 0.854801, mean_q: 12.491225
 583911/1000000: episode: 1828, duration: 1.975s, episode steps: 264, steps per second: 134, episode reward: -39.792, mean reward: -0.151 [-10.822, 0.315], mean action: -0.496 [-1.091, 1.076], mean observation: -0.195 [-7.948, 5.070], loss: 0.846317, mean_absolute_error: 0.865152, mean_q: 12.477460
 584130/1000000: episode: 1829, duration: 1.642s, episode steps: 219, steps per second: 133, episode reward: -29.496, mean reward: -0.135 [-11.124, 0.205], mean action: -0.507 [-1.095, 1.097], mean observation: -0.173 [-7.931, 1.000], loss: 0.804359, mean_absolute_error: 0.864824, mean_q: 12.457319
 584316/1000000: episode: 1830, duration: 1.393s, episode steps: 186, steps per second: 133, episode reward: -36.990, mean reward: -0.199 [-10.911, 0.022], mean action: -0.522 [-1.119, 1.050], mean observation: -0.085 [-6.023, 1.000], loss: 0.803406, mean_absolute_error: 0.850649, mean_q: 12.490728
 584565/1000000: episode: 1831, duration: 1.857s, episode steps: 249, steps per second: 134, episode reward: 5.477, mean reward: 0.022 [-11.035, 0.495], mean action: -0.471 [-1.088, 1.085], mean observation: -0.179 [-7.901, 3.587], loss: 0.835862, mean_absolute_error: 0.858881, mean_q: 12.482616
 584787/1000000: episode: 1832, duration: 1.662s, episode steps: 222, steps per second: 134, episode reward: -20.853, mean reward: -0.094 [-10.994, 0.208], mean action: -0.503 [-1.101, 1.065], mean observation: -0.164 [-7.951, 1.000], loss: 0.789601, mean_absolute_error: 0.853490, mean_q: 12.406500
 585039/1000000: episode: 1833, duration: 1.881s, episode steps: 252, steps per second: 134, episode reward: -53.126, mean reward: -0.211 [-10.900, 0.160], mean action: -0.518 [-1.093, 1.084], mean observation: -0.144 [-7.952, 4.110], loss: 0.783471, mean_absolute_error: 0.852848, mean_q: 12.504662
 585254/1000000: episode: 1834, duration: 1.619s, episode steps: 215, steps per second: 133, episode reward: -52.085, mean reward: -0.242 [-11.363, 0.137], mean action: -0.493 [-1.108, 1.125], mean observation: -0.156 [-7.940, 1.000], loss: 0.774210, mean_absolute_error: 0.853987, mean_q: 12.470613
 585471/1000000: episode: 1835, duration: 1.626s, episode steps: 217, steps per second: 133, episode reward: 9.706, mean reward: 0.045 [-10.841, 0.340], mean action: -0.508 [-1.138, 1.116], mean observation: -0.150 [-8.026, 1.000], loss: 0.834681, mean_absolute_error: 0.860550, mean_q: 12.509891
 585666/1000000: episode: 1836, duration: 1.458s, episode steps: 195, steps per second: 134, episode reward: 6.008, mean reward: 0.031 [-10.842, 0.299], mean action: -0.514 [-1.107, 1.080], mean observation: -0.132 [-6.986, 1.000], loss: 0.905272, mean_absolute_error: 0.871664, mean_q: 12.601542
 585887/1000000: episode: 1837, duration: 1.650s, episode steps: 221, steps per second: 134, episode reward: 18.714, mean reward: 0.085 [-11.034, 0.462], mean action: -0.490 [-1.089, 1.031], mean observation: -0.161 [-7.908, 1.000], loss: 0.931864, mean_absolute_error: 0.871250, mean_q: 12.505797
 586097/1000000: episode: 1838, duration: 1.581s, episode steps: 210, steps per second: 133, episode reward: -4.493, mean reward: -0.021 [-10.827, 0.225], mean action: -0.494 [-1.087, 1.055], mean observation: -0.111 [-7.599, 1.000], loss: 0.878141, mean_absolute_error: 0.860425, mean_q: 12.471648
 586354/1000000: episode: 1839, duration: 1.933s, episode steps: 257, steps per second: 133, episode reward: -29.019, mean reward: -0.113 [-10.879, 0.300], mean action: -0.493 [-1.098, 1.060], mean observation: -0.183 [-7.835, 4.459], loss: 0.900842, mean_absolute_error: 0.863182, mean_q: 12.572972
 586621/1000000: episode: 1840, duration: 2.006s, episode steps: 267, steps per second: 133, episode reward: 24.075, mean reward: 0.090 [-10.635, 0.545], mean action: -0.504 [-1.164, 1.143], mean observation: -0.172 [-7.988, 5.100], loss: 0.868955, mean_absolute_error: 0.858838, mean_q: 12.596045
 586877/1000000: episode: 1841, duration: 1.911s, episode steps: 256, steps per second: 134, episode reward: -80.455, mean reward: -0.314 [-11.231, 0.236], mean action: -0.510 [-1.132, 1.058], mean observation: -0.212 [-7.949, 4.717], loss: 0.930857, mean_absolute_error: 0.864048, mean_q: 12.569576
 587086/1000000: episode: 1842, duration: 1.571s, episode steps: 209, steps per second: 133, episode reward: 26.836, mean reward: 0.128 [-10.809, 0.413], mean action: -0.475 [-1.076, 1.090], mean observation: -0.125 [-7.709, 1.000], loss: 0.815175, mean_absolute_error: 0.845909, mean_q: 12.542308
 587278/1000000: episode: 1843, duration: 1.440s, episode steps: 192, steps per second: 133, episode reward: 4.854, mean reward: 0.025 [-10.807, 0.280], mean action: -0.509 [-1.155, 1.118], mean observation: -0.112 [-6.698, 1.000], loss: 0.845666, mean_absolute_error: 0.848037, mean_q: 12.502471
 587486/1000000: episode: 1844, duration: 1.568s, episode steps: 208, steps per second: 133, episode reward: 25.034, mean reward: 0.120 [-10.800, 0.400], mean action: -0.506 [-1.100, 1.056], mean observation: -0.128 [-7.668, 1.000], loss: 0.904622, mean_absolute_error: 0.861090, mean_q: 12.595806
 587733/1000000: episode: 1845, duration: 1.844s, episode steps: 247, steps per second: 134, episode reward: -70.265, mean reward: -0.284 [-11.344, 0.250], mean action: -0.524 [-1.140, 1.105], mean observation: -0.209 [-7.972, 3.885], loss: 0.788158, mean_absolute_error: 0.848858, mean_q: 12.525782
 587920/1000000: episode: 1846, duration: 1.397s, episode steps: 187, steps per second: 134, episode reward: -10.730, mean reward: -0.057 [-10.814, 0.177], mean action: -0.513 [-1.150, 1.072], mean observation: -0.122 [-6.127, 1.000], loss: 0.895270, mean_absolute_error: 0.863785, mean_q: 12.529962
 588066/1000000: episode: 1847, duration: 1.102s, episode steps: 146, steps per second: 133, episode reward: 27.724, mean reward: 0.190 [-9.953, 0.294], mean action: -0.504 [-1.096, 1.054], mean observation: 0.002 [-5.822, 1.000], loss: 0.741703, mean_absolute_error: 0.840782, mean_q: 12.444471
 588288/1000000: episode: 1848, duration: 1.662s, episode steps: 222, steps per second: 134, episode reward: 23.197, mean reward: 0.104 [-11.002, 0.460], mean action: -0.491 [-1.074, 1.007], mean observation: -0.147 [-7.777, 1.000], loss: 0.818468, mean_absolute_error: 0.846145, mean_q: 12.502102
 588498/1000000: episode: 1849, duration: 1.566s, episode steps: 210, steps per second: 134, episode reward: -31.561, mean reward: -0.150 [-11.144, 0.161], mean action: -0.477 [-1.114, 1.145], mean observation: -0.154 [-7.688, 1.000], loss: 0.917882, mean_absolute_error: 0.865376, mean_q: 12.576251
 588768/1000000: episode: 1850, duration: 2.015s, episode steps: 270, steps per second: 134, episode reward: -23.941, mean reward: -0.089 [-10.800, 0.473], mean action: -0.523 [-1.122, 1.072], mean observation: -0.173 [-7.913, 5.263], loss: 0.866720, mean_absolute_error: 0.855621, mean_q: 12.639848
 589005/1000000: episode: 1851, duration: 1.769s, episode steps: 237, steps per second: 134, episode reward: -16.034, mean reward: -0.068 [-11.202, 0.380], mean action: -0.503 [-1.094, 1.038], mean observation: -0.179 [-7.923, 2.115], loss: 0.903740, mean_absolute_error: 0.866643, mean_q: 12.558187
 589246/1000000: episode: 1852, duration: 1.828s, episode steps: 241, steps per second: 132, episode reward: -18.817, mean reward: -0.078 [-10.913, 0.303], mean action: -0.508 [-1.120, 1.125], mean observation: -0.150 [-8.003, 3.357], loss: 0.807521, mean_absolute_error: 0.844632, mean_q: 12.597047
 589486/1000000: episode: 1853, duration: 1.806s, episode steps: 240, steps per second: 133, episode reward: -18.799, mean reward: -0.078 [-11.180, 0.392], mean action: -0.495 [-1.104, 1.072], mean observation: -0.198 [-7.905, 2.831], loss: 0.916810, mean_absolute_error: 0.857680, mean_q: 12.570263
 589686/1000000: episode: 1854, duration: 1.516s, episode steps: 200, steps per second: 132, episode reward: -3.096, mean reward: -0.015 [-10.672, 0.184], mean action: -0.488 [-1.081, 1.092], mean observation: -0.091 [-7.086, 1.000], loss: 0.974355, mean_absolute_error: 0.880877, mean_q: 12.508559
 589871/1000000: episode: 1855, duration: 1.386s, episode steps: 185, steps per second: 133, episode reward: 6.034, mean reward: 0.033 [-10.498, 0.201], mean action: -0.511 [-1.103, 1.083], mean observation: -0.083 [-6.304, 1.000], loss: 0.821713, mean_absolute_error: 0.858032, mean_q: 12.538286
 590137/1000000: episode: 1856, duration: 1.988s, episode steps: 266, steps per second: 134, episode reward: -7.785, mean reward: -0.029 [-10.930, 0.584], mean action: -0.487 [-1.100, 1.089], mean observation: -0.175 [-7.949, 5.128], loss: 0.908629, mean_absolute_error: 0.857229, mean_q: 12.622618
 590333/1000000: episode: 1857, duration: 1.478s, episode steps: 196, steps per second: 133, episode reward: -0.409, mean reward: -0.002 [-10.596, 0.179], mean action: -0.488 [-1.087, 1.107], mean observation: -0.090 [-6.771, 1.000], loss: 0.784959, mean_absolute_error: 0.851884, mean_q: 12.527617
 590592/1000000: episode: 1858, duration: 1.935s, episode steps: 259, steps per second: 134, episode reward: -86.613, mean reward: -0.334 [-11.277, 0.214], mean action: -0.505 [-1.088, 1.042], mean observation: -0.208 [-7.915, 4.687], loss: 0.763839, mean_absolute_error: 0.845624, mean_q: 12.650506
 590849/1000000: episode: 1859, duration: 1.938s, episode steps: 257, steps per second: 133, episode reward: -50.205, mean reward: -0.195 [-11.268, 0.385], mean action: -0.502 [-1.130, 1.097], mean observation: -0.193 [-7.891, 4.452], loss: 0.868383, mean_absolute_error: 0.851345, mean_q: 12.631600
 590975/1000000: episode: 1860, duration: 0.948s, episode steps: 126, steps per second: 133, episode reward: 7.728, mean reward: 0.061 [-9.956, 0.157], mean action: -0.500 [-1.073, 1.109], mean observation: 0.049 [-4.940, 1.000], loss: 0.755361, mean_absolute_error: 0.843568, mean_q: 12.607066
 591252/1000000: episode: 1861, duration: 2.079s, episode steps: 277, steps per second: 133, episode reward: -10.041, mean reward: -0.036 [-10.366, 0.319], mean action: -0.485 [-1.066, 1.077], mean observation: -0.148 [-7.909, 5.268], loss: 0.861769, mean_absolute_error: 0.853468, mean_q: 12.563055
 591525/1000000: episode: 1862, duration: 2.051s, episode steps: 273, steps per second: 133, episode reward: 19.770, mean reward: 0.072 [-10.461, 0.505], mean action: -0.491 [-1.157, 1.094], mean observation: -0.165 [-7.973, 5.181], loss: 0.813013, mean_absolute_error: 0.854136, mean_q: 12.573465
 591785/1000000: episode: 1863, duration: 1.942s, episode steps: 260, steps per second: 134, episode reward: -50.970, mean reward: -0.196 [-11.129, 0.402], mean action: -0.493 [-1.122, 1.080], mean observation: -0.188 [-7.974, 4.939], loss: 0.859895, mean_absolute_error: 0.851890, mean_q: 12.616353
 592015/1000000: episode: 1864, duration: 1.721s, episode steps: 230, steps per second: 134, episode reward: -31.096, mean reward: -0.135 [-10.937, 0.171], mean action: -0.500 [-1.104, 1.115], mean observation: -0.165 [-7.966, 1.475], loss: 0.813695, mean_absolute_error: 0.845225, mean_q: 12.464177
 592164/1000000: episode: 1865, duration: 1.127s, episode steps: 149, steps per second: 132, episode reward: 4.469, mean reward: 0.030 [-10.075, 0.123], mean action: -0.500 [-1.115, 1.042], mean observation: 0.013 [-5.848, 1.000], loss: 0.895465, mean_absolute_error: 0.861416, mean_q: 12.625342
 592379/1000000: episode: 1866, duration: 1.612s, episode steps: 215, steps per second: 133, episode reward: -19.538, mean reward: -0.091 [-10.957, 0.187], mean action: -0.491 [-1.067, 1.082], mean observation: -0.154 [-7.795, 1.000], loss: 0.984376, mean_absolute_error: 0.873948, mean_q: 12.518827
 592637/1000000: episode: 1867, duration: 1.927s, episode steps: 258, steps per second: 134, episode reward: -0.085, mean reward: -0.000 [-10.843, 0.446], mean action: -0.479 [-1.104, 1.066], mean observation: -0.167 [-7.827, 4.609], loss: 0.832116, mean_absolute_error: 0.851747, mean_q: 12.594788
 592869/1000000: episode: 1868, duration: 1.738s, episode steps: 232, steps per second: 134, episode reward: 11.150, mean reward: 0.048 [-10.895, 0.394], mean action: -0.514 [-1.181, 1.070], mean observation: -0.154 [-7.973, 1.748], loss: 0.890969, mean_absolute_error: 0.856509, mean_q: 12.471762
 593140/1000000: episode: 1869, duration: 2.021s, episode steps: 271, steps per second: 134, episode reward: 37.954, mean reward: 0.140 [-10.713, 0.658], mean action: -0.485 [-1.098, 1.071], mean observation: -0.181 [-7.900, 5.154], loss: 0.860861, mean_absolute_error: 0.845449, mean_q: 12.543044
 593280/1000000: episode: 1870, duration: 1.058s, episode steps: 140, steps per second: 132, episode reward: 0.192, mean reward: 0.001 [-10.111, 0.105], mean action: -0.510 [-1.093, 1.035], mean observation: 0.022 [-5.617, 1.000], loss: 0.760065, mean_absolute_error: 0.839545, mean_q: 12.628856
 593533/1000000: episode: 1871, duration: 1.905s, episode steps: 253, steps per second: 133, episode reward: -54.454, mean reward: -0.215 [-11.291, 0.354], mean action: -0.511 [-1.104, 1.053], mean observation: -0.193 [-7.916, 4.262], loss: 0.863800, mean_absolute_error: 0.848840, mean_q: 12.691246
 593695/1000000: episode: 1872, duration: 1.214s, episode steps: 162, steps per second: 133, episode reward: -0.809, mean reward: -0.005 [-10.232, 0.104], mean action: -0.502 [-1.132, 1.106], mean observation: -0.011 [-6.013, 1.000], loss: 0.881914, mean_absolute_error: 0.851425, mean_q: 12.607847
 593875/1000000: episode: 1873, duration: 1.349s, episode steps: 180, steps per second: 133, episode reward: -15.298, mean reward: -0.085 [-10.659, 0.101], mean action: -0.500 [-1.086, 1.060], mean observation: -0.105 [-5.960, 1.000], loss: 0.903269, mean_absolute_error: 0.860385, mean_q: 12.645427
 594085/1000000: episode: 1874, duration: 1.569s, episode steps: 210, steps per second: 134, episode reward: 22.842, mean reward: 0.109 [-10.897, 0.418], mean action: -0.503 [-1.095, 1.079], mean observation: -0.135 [-7.621, 1.000], loss: 0.840883, mean_absolute_error: 0.850776, mean_q: 12.604408
 594342/1000000: episode: 1875, duration: 1.945s, episode steps: 257, steps per second: 132, episode reward: 3.853, mean reward: 0.015 [-10.879, 0.510], mean action: -0.489 [-1.089, 1.080], mean observation: -0.172 [-7.939, 4.681], loss: 0.863458, mean_absolute_error: 0.851555, mean_q: 12.620770
 594469/1000000: episode: 1876, duration: 0.960s, episode steps: 127, steps per second: 132, episode reward: -7.821, mean reward: -0.062 [-10.046, 0.026], mean action: -0.524 [-1.091, 1.014], mean observation: 0.039 [-5.008, 1.000], loss: 0.901213, mean_absolute_error: 0.850382, mean_q: 12.608534
 594677/1000000: episode: 1877, duration: 1.552s, episode steps: 208, steps per second: 134, episode reward: 12.157, mean reward: 0.058 [-10.852, 0.335], mean action: -0.509 [-1.127, 1.150], mean observation: -0.117 [-7.698, 1.000], loss: 0.770828, mean_absolute_error: 0.833596, mean_q: 12.640944
 594915/1000000: episode: 1878, duration: 1.782s, episode steps: 238, steps per second: 134, episode reward: -4.792, mean reward: -0.020 [-10.999, 0.371], mean action: -0.517 [-1.102, 1.056], mean observation: -0.187 [-7.914, 2.489], loss: 0.801650, mean_absolute_error: 0.843664, mean_q: 12.593424
 595181/1000000: episode: 1879, duration: 1.993s, episode steps: 266, steps per second: 133, episode reward: 7.853, mean reward: 0.030 [-10.511, 0.425], mean action: -0.492 [-1.137, 1.097], mean observation: -0.170 [-7.965, 5.183], loss: 0.824868, mean_absolute_error: 0.849337, mean_q: 12.578681
 595423/1000000: episode: 1880, duration: 1.823s, episode steps: 242, steps per second: 133, episode reward: -12.205, mean reward: -0.050 [-11.010, 0.405], mean action: -0.505 [-1.140, 1.174], mean observation: -0.198 [-8.058, 3.654], loss: 0.821409, mean_absolute_error: 0.846041, mean_q: 12.633996
 595688/1000000: episode: 1881, duration: 1.989s, episode steps: 265, steps per second: 133, episode reward: -40.704, mean reward: -0.154 [-10.676, 0.258], mean action: -0.491 [-1.119, 1.110], mean observation: -0.179 [-7.937, 5.140], loss: 0.950018, mean_absolute_error: 0.858369, mean_q: 12.600792
 595870/1000000: episode: 1882, duration: 1.366s, episode steps: 182, steps per second: 133, episode reward: 12.918, mean reward: 0.071 [-10.588, 0.278], mean action: -0.474 [-1.079, 1.088], mean observation: -0.088 [-5.997, 1.000], loss: 0.781628, mean_absolute_error: 0.843837, mean_q: 12.557618
 596124/1000000: episode: 1883, duration: 1.899s, episode steps: 254, steps per second: 134, episode reward: 1.711, mean reward: 0.007 [-10.704, 0.404], mean action: -0.513 [-1.147, 1.098], mean observation: -0.154 [-8.018, 4.605], loss: 0.762713, mean_absolute_error: 0.835372, mean_q: 12.713755
 596315/1000000: episode: 1884, duration: 1.439s, episode steps: 191, steps per second: 133, episode reward: 15.566, mean reward: 0.081 [-10.695, 0.317], mean action: -0.499 [-1.090, 1.093], mean observation: -0.118 [-6.584, 1.000], loss: 0.807801, mean_absolute_error: 0.836441, mean_q: 12.551846
 596558/1000000: episode: 1885, duration: 1.841s, episode steps: 243, steps per second: 132, episode reward: -46.658, mean reward: -0.192 [-11.310, 0.334], mean action: -0.482 [-1.134, 1.147], mean observation: -0.176 [-8.015, 3.459], loss: 0.828989, mean_absolute_error: 0.841881, mean_q: 12.668214
 596776/1000000: episode: 1886, duration: 1.708s, episode steps: 218, steps per second: 128, episode reward: 3.461, mean reward: 0.016 [-10.960, 0.342], mean action: -0.512 [-1.153, 1.055], mean observation: -0.161 [-7.982, 1.000], loss: 0.906448, mean_absolute_error: 0.845482, mean_q: 12.665089
 597046/1000000: episode: 1887, duration: 2.020s, episode steps: 270, steps per second: 134, episode reward: 18.349, mean reward: 0.068 [-10.546, 0.489], mean action: -0.488 [-1.090, 1.093], mean observation: -0.167 [-7.908, 5.234], loss: 0.825319, mean_absolute_error: 0.834311, mean_q: 12.650476
 597229/1000000: episode: 1888, duration: 1.373s, episode steps: 183, steps per second: 133, episode reward: 11.920, mean reward: 0.065 [-10.560, 0.260], mean action: -0.490 [-1.102, 1.089], mean observation: -0.078 [-6.019, 1.000], loss: 0.796907, mean_absolute_error: 0.845959, mean_q: 12.731911
 597495/1000000: episode: 1889, duration: 1.995s, episode steps: 266, steps per second: 133, episode reward: -14.102, mean reward: -0.053 [-10.992, 0.561], mean action: -0.500 [-1.083, 1.025], mean observation: -0.183 [-7.887, 5.108], loss: 0.900092, mean_absolute_error: 0.855590, mean_q: 12.566094
 597758/1000000: episode: 1890, duration: 1.982s, episode steps: 263, steps per second: 133, episode reward: -16.872, mean reward: -0.064 [-10.775, 0.358], mean action: -0.504 [-1.209, 1.054], mean observation: -0.188 [-7.917, 4.856], loss: 0.861125, mean_absolute_error: 0.845013, mean_q: 12.687094
 597974/1000000: episode: 1891, duration: 1.618s, episode steps: 216, steps per second: 133, episode reward: -2.333, mean reward: -0.011 [-10.861, 0.265], mean action: -0.483 [-1.091, 1.114], mean observation: -0.144 [-7.931, 1.000], loss: 0.850828, mean_absolute_error: 0.836290, mean_q: 12.744760
 598208/1000000: episode: 1892, duration: 1.756s, episode steps: 234, steps per second: 133, episode reward: -3.794, mean reward: -0.016 [-11.127, 0.422], mean action: -0.508 [-1.117, 1.082], mean observation: -0.164 [-7.954, 2.069], loss: 0.784489, mean_absolute_error: 0.828809, mean_q: 12.641167
 598420/1000000: episode: 1893, duration: 1.584s, episode steps: 212, steps per second: 134, episode reward: -48.194, mean reward: -0.227 [-11.272, 0.116], mean action: -0.517 [-1.130, 1.108], mean observation: -0.142 [-7.817, 1.000], loss: 0.767635, mean_absolute_error: 0.829433, mean_q: 12.749643
 598676/1000000: episode: 1894, duration: 1.911s, episode steps: 256, steps per second: 134, episode reward: -81.760, mean reward: -0.319 [-11.280, 0.236], mean action: -0.503 [-1.096, 1.069], mean observation: -0.173 [-7.931, 4.599], loss: 0.726261, mean_absolute_error: 0.818404, mean_q: 12.660384
 598917/1000000: episode: 1895, duration: 1.809s, episode steps: 241, steps per second: 133, episode reward: -17.397, mean reward: -0.072 [-10.787, 0.270], mean action: -0.512 [-1.136, 1.085], mean observation: -0.149 [-8.038, 3.482], loss: 0.800273, mean_absolute_error: 0.832330, mean_q: 12.637170
 599169/1000000: episode: 1896, duration: 1.890s, episode steps: 252, steps per second: 133, episode reward: -0.411, mean reward: -0.002 [-10.653, 0.375], mean action: -0.492 [-1.134, 1.138], mean observation: -0.158 [-8.027, 4.604], loss: 0.826890, mean_absolute_error: 0.837954, mean_q: 12.641657
 599429/1000000: episode: 1897, duration: 1.951s, episode steps: 260, steps per second: 133, episode reward: -54.451, mean reward: -0.209 [-10.833, 0.189], mean action: -0.486 [-1.078, 1.074], mean observation: -0.184 [-7.887, 4.777], loss: 0.921856, mean_absolute_error: 0.843373, mean_q: 12.685342
 599619/1000000: episode: 1898, duration: 1.424s, episode steps: 190, steps per second: 133, episode reward: 9.387, mean reward: 0.049 [-10.767, 0.300], mean action: -0.497 [-1.110, 1.098], mean observation: -0.116 [-6.564, 1.000], loss: 0.879645, mean_absolute_error: 0.844205, mean_q: 12.577508
 599845/1000000: episode: 1899, duration: 1.686s, episode steps: 226, steps per second: 134, episode reward: -32.447, mean reward: -0.144 [-11.343, 0.299], mean action: -0.537 [-1.159, 1.077], mean observation: -0.177 [-7.923, 1.000], loss: 0.909237, mean_absolute_error: 0.844814, mean_q: 12.728992
 600075/1000000: episode: 1900, duration: 1.721s, episode steps: 230, steps per second: 134, episode reward: -51.775, mean reward: -0.225 [-11.394, 0.220], mean action: -0.481 [-1.072, 1.124], mean observation: -0.199 [-7.927, 1.282], loss: 0.876765, mean_absolute_error: 0.845823, mean_q: 12.621675
 600294/1000000: episode: 1901, duration: 1.638s, episode steps: 219, steps per second: 134, episode reward: -46.338, mean reward: -0.212 [-10.997, 0.062], mean action: -0.492 [-1.104, 1.096], mean observation: -0.120 [-7.987, 1.000], loss: 0.745753, mean_absolute_error: 0.826827, mean_q: 12.751768
 600540/1000000: episode: 1902, duration: 1.841s, episode steps: 246, steps per second: 134, episode reward: -12.126, mean reward: -0.049 [-11.151, 0.462], mean action: -0.512 [-1.122, 1.080], mean observation: -0.170 [-7.963, 3.565], loss: 0.843155, mean_absolute_error: 0.832817, mean_q: 12.708180
 600785/1000000: episode: 1903, duration: 1.834s, episode steps: 245, steps per second: 134, episode reward: -20.739, mean reward: -0.085 [-10.936, 0.301], mean action: -0.481 [-1.101, 1.105], mean observation: -0.178 [-7.961, 3.441], loss: 0.788056, mean_absolute_error: 0.816442, mean_q: 12.863036
 601054/1000000: episode: 1904, duration: 2.007s, episode steps: 269, steps per second: 134, episode reward: -74.495, mean reward: -0.277 [-11.099, 0.315], mean action: -0.509 [-1.131, 1.094], mean observation: -0.211 [-7.918, 5.197], loss: 0.869067, mean_absolute_error: 0.845345, mean_q: 12.804316
 601311/1000000: episode: 1905, duration: 1.930s, episode steps: 257, steps per second: 133, episode reward: -16.457, mean reward: -0.064 [-10.724, 0.308], mean action: -0.506 [-1.157, 1.114], mean observation: -0.152 [-7.969, 4.555], loss: 0.785069, mean_absolute_error: 0.832570, mean_q: 12.744653
 601554/1000000: episode: 1906, duration: 1.819s, episode steps: 243, steps per second: 134, episode reward: -8.464, mean reward: -0.035 [-10.934, 0.339], mean action: -0.517 [-1.111, 1.100], mean observation: -0.179 [-7.970, 2.987], loss: 0.872511, mean_absolute_error: 0.832431, mean_q: 12.714490
 601801/1000000: episode: 1907, duration: 1.853s, episode steps: 247, steps per second: 133, episode reward: -67.924, mean reward: -0.275 [-11.342, 0.230], mean action: -0.477 [-1.082, 1.058], mean observation: -0.214 [-7.930, 3.580], loss: 0.719223, mean_absolute_error: 0.813386, mean_q: 12.680565
 602067/1000000: episode: 1908, duration: 1.981s, episode steps: 266, steps per second: 134, episode reward: 15.434, mean reward: 0.058 [-10.862, 0.600], mean action: -0.513 [-1.123, 1.018], mean observation: -0.173 [-7.845, 5.013], loss: 0.949287, mean_absolute_error: 0.856940, mean_q: 12.765563
 602336/1000000: episode: 1909, duration: 2.011s, episode steps: 269, steps per second: 134, episode reward: -42.925, mean reward: -0.160 [-10.883, 0.368], mean action: -0.507 [-1.088, 1.047], mean observation: -0.197 [-7.879, 5.230], loss: 0.942637, mean_absolute_error: 0.848892, mean_q: 12.754046
 602490/1000000: episode: 1910, duration: 1.156s, episode steps: 154, steps per second: 133, episode reward: 19.613, mean reward: 0.127 [-10.159, 0.259], mean action: -0.495 [-1.074, 1.154], mean observation: -0.029 [-6.043, 1.000], loss: 1.034874, mean_absolute_error: 0.853351, mean_q: 12.711351
 602727/1000000: episode: 1911, duration: 1.776s, episode steps: 237, steps per second: 133, episode reward: -8.108, mean reward: -0.034 [-10.848, 0.285], mean action: -0.505 [-1.163, 1.143], mean observation: -0.162 [-7.960, 2.265], loss: 0.884010, mean_absolute_error: 0.843237, mean_q: 12.616271
 602991/1000000: episode: 1912, duration: 1.977s, episode steps: 264, steps per second: 134, episode reward: -3.346, mean reward: -0.013 [-10.868, 0.549], mean action: -0.518 [-1.139, 1.035], mean observation: -0.174 [-7.962, 5.078], loss: 0.902378, mean_absolute_error: 0.847121, mean_q: 12.709412
 603213/1000000: episode: 1913, duration: 1.668s, episode steps: 222, steps per second: 133, episode reward: 8.707, mean reward: 0.039 [-10.956, 0.367], mean action: -0.510 [-1.090, 1.057], mean observation: -0.157 [-7.929, 1.000], loss: 0.809615, mean_absolute_error: 0.828670, mean_q: 12.886267
 603437/1000000: episode: 1914, duration: 1.682s, episode steps: 224, steps per second: 133, episode reward: 8.992, mean reward: 0.040 [-11.116, 0.432], mean action: -0.503 [-1.112, 1.108], mean observation: -0.163 [-7.928, 1.000], loss: 0.812019, mean_absolute_error: 0.826360, mean_q: 12.674448
 603633/1000000: episode: 1915, duration: 1.473s, episode steps: 196, steps per second: 133, episode reward: 17.723, mean reward: 0.090 [-10.596, 0.293], mean action: -0.516 [-1.163, 1.036], mean observation: -0.104 [-6.911, 1.000], loss: 0.837703, mean_absolute_error: 0.825337, mean_q: 12.762708
 603855/1000000: episode: 1916, duration: 1.669s, episode steps: 222, steps per second: 133, episode reward: -46.989, mean reward: -0.212 [-11.386, 0.204], mean action: -0.503 [-1.099, 1.095], mean observation: -0.180 [-7.983, 1.000], loss: 0.864237, mean_absolute_error: 0.837835, mean_q: 12.693182
 604132/1000000: episode: 1917, duration: 2.066s, episode steps: 277, steps per second: 134, episode reward: -52.324, mean reward: -0.189 [-10.837, 0.358], mean action: -0.534 [-1.158, 1.001], mean observation: -0.186 [-7.922, 5.252], loss: 0.915959, mean_absolute_error: 0.843387, mean_q: 12.714539
 604348/1000000: episode: 1918, duration: 1.619s, episode steps: 216, steps per second: 133, episode reward: 6.598, mean reward: 0.031 [-11.065, 0.396], mean action: -0.518 [-1.144, 1.111], mean observation: -0.148 [-7.963, 1.000], loss: 0.808106, mean_absolute_error: 0.819087, mean_q: 12.769242
 604560/1000000: episode: 1919, duration: 1.584s, episode steps: 212, steps per second: 134, episode reward: -9.802, mean reward: -0.046 [-10.897, 0.219], mean action: -0.517 [-1.118, 1.064], mean observation: -0.111 [-7.751, 1.000], loss: 0.942988, mean_absolute_error: 0.838706, mean_q: 12.783076
 604736/1000000: episode: 1920, duration: 1.316s, episode steps: 176, steps per second: 134, episode reward: 6.298, mean reward: 0.036 [-10.364, 0.175], mean action: -0.472 [-1.066, 1.104], mean observation: -0.066 [-6.008, 1.000], loss: 0.807867, mean_absolute_error: 0.824705, mean_q: 12.752931
 604973/1000000: episode: 1921, duration: 1.776s, episode steps: 237, steps per second: 133, episode reward: -69.192, mean reward: -0.292 [-11.436, 0.183], mean action: -0.507 [-1.110, 1.049], mean observation: -0.204 [-7.933, 2.340], loss: 0.816726, mean_absolute_error: 0.834135, mean_q: 12.743558
 605237/1000000: episode: 1922, duration: 1.970s, episode steps: 264, steps per second: 134, episode reward: -39.164, mean reward: -0.148 [-11.054, 0.447], mean action: -0.510 [-1.150, 1.067], mean observation: -0.193 [-7.945, 5.048], loss: 0.889698, mean_absolute_error: 0.840304, mean_q: 12.779898
 605437/1000000: episode: 1923, duration: 1.507s, episode steps: 200, steps per second: 133, episode reward: 21.797, mean reward: 0.109 [-10.682, 0.346], mean action: -0.516 [-1.130, 1.079], mean observation: -0.116 [-7.426, 1.000], loss: 0.784587, mean_absolute_error: 0.818972, mean_q: 12.739411
 605674/1000000: episode: 1924, duration: 1.767s, episode steps: 237, steps per second: 134, episode reward: 20.953, mean reward: 0.088 [-10.945, 0.501], mean action: -0.517 [-1.129, 1.091], mean observation: -0.166 [-7.935, 2.638], loss: 0.869720, mean_absolute_error: 0.826827, mean_q: 12.709812
 605897/1000000: episode: 1925, duration: 1.671s, episode steps: 223, steps per second: 133, episode reward: -44.944, mean reward: -0.202 [-11.383, 0.213], mean action: -0.486 [-1.092, 1.047], mean observation: -0.180 [-7.914, 1.000], loss: 0.880833, mean_absolute_error: 0.829608, mean_q: 12.685932
 606088/1000000: episode: 1926, duration: 1.434s, episode steps: 191, steps per second: 133, episode reward: -9.071, mean reward: -0.047 [-10.674, 0.144], mean action: -0.516 [-1.203, 1.048], mean observation: -0.081 [-6.476, 1.000], loss: 0.891536, mean_absolute_error: 0.845451, mean_q: 12.866771
 606301/1000000: episode: 1927, duration: 1.604s, episode steps: 213, steps per second: 133, episode reward: 12.992, mean reward: 0.061 [-10.767, 0.313], mean action: -0.489 [-1.094, 1.102], mean observation: -0.129 [-7.800, 1.000], loss: 0.885802, mean_absolute_error: 0.831788, mean_q: 12.820642
 606532/1000000: episode: 1928, duration: 1.724s, episode steps: 231, steps per second: 134, episode reward: -76.522, mean reward: -0.331 [-11.506, 0.110], mean action: -0.520 [-1.150, 1.043], mean observation: -0.162 [-7.922, 1.157], loss: 0.799769, mean_absolute_error: 0.821171, mean_q: 12.760373
 606800/1000000: episode: 1929, duration: 2.003s, episode steps: 268, steps per second: 134, episode reward: -33.902, mean reward: -0.127 [-10.703, 0.341], mean action: -0.505 [-1.100, 1.144], mean observation: -0.156 [-7.999, 5.175], loss: 0.840213, mean_absolute_error: 0.829754, mean_q: 12.755221
 607031/1000000: episode: 1930, duration: 1.729s, episode steps: 231, steps per second: 134, episode reward: -27.833, mean reward: -0.120 [-11.276, 0.332], mean action: -0.494 [-1.142, 1.209], mean observation: -0.166 [-8.007, 1.706], loss: 0.873299, mean_absolute_error: 0.827710, mean_q: 12.637298
 607289/1000000: episode: 1931, duration: 1.934s, episode steps: 258, steps per second: 133, episode reward: -6.855, mean reward: -0.027 [-10.711, 0.377], mean action: -0.505 [-1.094, 1.080], mean observation: -0.164 [-7.954, 4.765], loss: 0.810221, mean_absolute_error: 0.824195, mean_q: 12.852846
 607506/1000000: episode: 1932, duration: 1.620s, episode steps: 217, steps per second: 134, episode reward: -33.175, mean reward: -0.153 [-11.301, 0.242], mean action: -0.513 [-1.133, 1.074], mean observation: -0.169 [-7.996, 1.000], loss: 0.856518, mean_absolute_error: 0.827847, mean_q: 12.853246
 607771/1000000: episode: 1933, duration: 1.976s, episode steps: 265, steps per second: 134, episode reward: -39.789, mean reward: -0.150 [-10.596, 0.248], mean action: -0.499 [-1.120, 1.156], mean observation: -0.149 [-7.987, 5.164], loss: 0.826871, mean_absolute_error: 0.833601, mean_q: 12.876863
 607916/1000000: episode: 1934, duration: 1.089s, episode steps: 145, steps per second: 133, episode reward: 3.594, mean reward: 0.025 [-10.073, 0.119], mean action: -0.488 [-1.091, 1.041], mean observation: 0.015 [-5.819, 1.000], loss: 0.908832, mean_absolute_error: 0.847686, mean_q: 12.630164
 608184/1000000: episode: 1935, duration: 2.004s, episode steps: 268, steps per second: 134, episode reward: 29.748, mean reward: 0.111 [-10.602, 0.632], mean action: -0.480 [-1.160, 1.083], mean observation: -0.167 [-7.980, 5.203], loss: 0.776741, mean_absolute_error: 0.820764, mean_q: 12.753668
 608392/1000000: episode: 1936, duration: 1.561s, episode steps: 208, steps per second: 133, episode reward: -27.821, mean reward: -0.134 [-11.176, 0.200], mean action: -0.508 [-1.100, 1.084], mean observation: -0.136 [-7.734, 1.000], loss: 0.799806, mean_absolute_error: 0.822989, mean_q: 12.818353
 608589/1000000: episode: 1937, duration: 1.472s, episode steps: 197, steps per second: 134, episode reward: -8.425, mean reward: -0.043 [-10.886, 0.213], mean action: -0.501 [-1.121, 1.055], mean observation: -0.101 [-6.836, 1.000], loss: 0.755727, mean_absolute_error: 0.809800, mean_q: 12.693433
 608845/1000000: episode: 1938, duration: 1.931s, episode steps: 256, steps per second: 133, episode reward: -54.575, mean reward: -0.213 [-11.232, 0.361], mean action: -0.507 [-1.163, 1.081], mean observation: -0.201 [-7.938, 4.554], loss: 0.949044, mean_absolute_error: 0.847560, mean_q: 12.808563
 609116/1000000: episode: 1939, duration: 2.023s, episode steps: 271, steps per second: 134, episode reward: -59.809, mean reward: -0.221 [-10.716, 0.194], mean action: -0.508 [-1.115, 1.137], mean observation: -0.188 [-7.889, 5.225], loss: 0.879877, mean_absolute_error: 0.824737, mean_q: 12.846341
 609386/1000000: episode: 1940, duration: 2.021s, episode steps: 270, steps per second: 134, episode reward: -50.349, mean reward: -0.186 [-10.776, 0.309], mean action: -0.490 [-1.083, 1.101], mean observation: -0.153 [-7.931, 5.247], loss: 0.905452, mean_absolute_error: 0.835269, mean_q: 12.717072
 609644/1000000: episode: 1941, duration: 1.931s, episode steps: 258, steps per second: 134, episode reward: -18.556, mean reward: -0.072 [-10.624, 0.280], mean action: -0.499 [-1.113, 1.046], mean observation: -0.165 [-7.901, 4.786], loss: 0.810297, mean_absolute_error: 0.824209, mean_q: 12.802638
 609902/1000000: episode: 1942, duration: 1.924s, episode steps: 258, steps per second: 134, episode reward: -23.211, mean reward: -0.090 [-10.714, 0.304], mean action: -0.487 [-1.131, 1.192], mean observation: -0.155 [-7.944, 4.832], loss: 0.755433, mean_absolute_error: 0.811261, mean_q: 12.695419
 610179/1000000: episode: 1943, duration: 2.084s, episode steps: 277, steps per second: 133, episode reward: -17.346, mean reward: -0.063 [-10.657, 0.506], mean action: -0.503 [-1.112, 1.041], mean observation: -0.187 [-7.959, 5.231], loss: 0.920324, mean_absolute_error: 0.831467, mean_q: 12.827762
 610437/1000000: episode: 1944, duration: 1.922s, episode steps: 258, steps per second: 134, episode reward: 13.836, mean reward: 0.054 [-10.897, 0.627], mean action: -0.508 [-1.129, 1.132], mean observation: -0.189 [-7.962, 4.877], loss: 0.860475, mean_absolute_error: 0.830477, mean_q: 12.711170
 610677/1000000: episode: 1945, duration: 1.790s, episode steps: 240, steps per second: 134, episode reward: -40.767, mean reward: -0.170 [-11.009, 0.176], mean action: -0.492 [-1.108, 1.079], mean observation: -0.181 [-7.877, 2.483], loss: 0.900779, mean_absolute_error: 0.832745, mean_q: 12.816287
 610943/1000000: episode: 1946, duration: 1.988s, episode steps: 266, steps per second: 134, episode reward: 13.993, mean reward: 0.053 [-10.473, 0.449], mean action: -0.501 [-1.166, 1.068], mean observation: -0.168 [-7.995, 5.185], loss: 0.852076, mean_absolute_error: 0.825950, mean_q: 12.789293
 611130/1000000: episode: 1947, duration: 1.407s, episode steps: 187, steps per second: 133, episode reward: 21.059, mean reward: 0.113 [-10.584, 0.322], mean action: -0.497 [-1.181, 1.118], mean observation: -0.110 [-6.648, 1.000], loss: 0.843338, mean_absolute_error: 0.822736, mean_q: 12.843006
 611328/1000000: episode: 1948, duration: 1.502s, episode steps: 198, steps per second: 132, episode reward: 5.013, mean reward: 0.025 [-10.645, 0.226], mean action: -0.502 [-1.084, 1.048], mean observation: -0.112 [-6.972, 1.000], loss: 0.816649, mean_absolute_error: 0.825513, mean_q: 12.773435
 611598/1000000: episode: 1949, duration: 2.032s, episode steps: 270, steps per second: 133, episode reward: -26.944, mean reward: -0.100 [-10.722, 0.372], mean action: -0.501 [-1.124, 1.069], mean observation: -0.186 [-7.917, 5.230], loss: 0.867691, mean_absolute_error: 0.826531, mean_q: 12.783925
 611828/1000000: episode: 1950, duration: 1.808s, episode steps: 230, steps per second: 127, episode reward: 5.177, mean reward: 0.023 [-11.061, 0.422], mean action: -0.502 [-1.124, 1.069], mean observation: -0.155 [-7.924, 1.385], loss: 0.823695, mean_absolute_error: 0.820047, mean_q: 12.777069
 612051/1000000: episode: 1951, duration: 1.662s, episode steps: 223, steps per second: 134, episode reward: -7.416, mean reward: -0.033 [-10.831, 0.247], mean action: -0.491 [-1.120, 1.099], mean observation: -0.140 [-7.979, 1.000], loss: 0.828744, mean_absolute_error: 0.825321, mean_q: 12.870302
 612296/1000000: episode: 1952, duration: 1.836s, episode steps: 245, steps per second: 133, episode reward: -91.597, mean reward: -0.374 [-11.421, 0.132], mean action: -0.487 [-1.066, 1.089], mean observation: -0.213 [-7.960, 3.564], loss: 0.805064, mean_absolute_error: 0.819854, mean_q: 12.805589
 612569/1000000: episode: 1953, duration: 2.047s, episode steps: 273, steps per second: 133, episode reward: 12.511, mean reward: 0.046 [-10.587, 0.529], mean action: -0.497 [-1.083, 1.107], mean observation: -0.176 [-7.937, 5.248], loss: 0.835767, mean_absolute_error: 0.816953, mean_q: 12.791278
 612762/1000000: episode: 1954, duration: 1.449s, episode steps: 193, steps per second: 133, episode reward: 18.470, mean reward: 0.096 [-10.540, 0.285], mean action: -0.514 [-1.106, 1.050], mean observation: -0.094 [-6.589, 1.000], loss: 0.740642, mean_absolute_error: 0.813776, mean_q: 12.926057
 613018/1000000: episode: 1955, duration: 1.916s, episode steps: 256, steps per second: 134, episode reward: -0.079, mean reward: -0.000 [-10.686, 0.357], mean action: -0.501 [-1.091, 1.039], mean observation: -0.157 [-7.935, 4.449], loss: 0.897933, mean_absolute_error: 0.832500, mean_q: 12.776014
 613271/1000000: episode: 1956, duration: 1.912s, episode steps: 253, steps per second: 132, episode reward: -40.725, mean reward: -0.161 [-11.132, 0.366], mean action: -0.487 [-1.102, 1.121], mean observation: -0.204 [-7.899, 4.408], loss: 0.893624, mean_absolute_error: 0.821285, mean_q: 12.846743
 613534/1000000: episode: 1957, duration: 1.980s, episode steps: 263, steps per second: 133, episode reward: -41.264, mean reward: -0.157 [-11.191, 0.444], mean action: -0.497 [-1.104, 1.038], mean observation: -0.200 [-7.807, 4.816], loss: 0.785901, mean_absolute_error: 0.813605, mean_q: 12.830064
 613747/1000000: episode: 1958, duration: 1.589s, episode steps: 213, steps per second: 134, episode reward: 25.599, mean reward: 0.120 [-10.829, 0.411], mean action: -0.500 [-1.089, 1.068], mean observation: -0.142 [-7.820, 1.000], loss: 0.785389, mean_absolute_error: 0.813130, mean_q: 13.020336
 613991/1000000: episode: 1959, duration: 1.822s, episode steps: 244, steps per second: 134, episode reward: -9.167, mean reward: -0.038 [-11.105, 0.442], mean action: -0.502 [-1.104, 1.148], mean observation: -0.188 [-7.977, 3.367], loss: 0.809539, mean_absolute_error: 0.819514, mean_q: 12.900615
 614242/1000000: episode: 1960, duration: 1.876s, episode steps: 251, steps per second: 134, episode reward: -55.963, mean reward: -0.223 [-11.307, 0.332], mean action: -0.521 [-1.146, 1.050], mean observation: -0.183 [-7.913, 4.089], loss: 0.941081, mean_absolute_error: 0.825027, mean_q: 12.838212
 614473/1000000: episode: 1961, duration: 1.734s, episode steps: 231, steps per second: 133, episode reward: 15.585, mean reward: 0.067 [-10.976, 0.449], mean action: -0.503 [-1.164, 1.117], mean observation: -0.160 [-7.978, 1.596], loss: 0.834661, mean_absolute_error: 0.818812, mean_q: 12.844569
 614748/1000000: episode: 1962, duration: 2.056s, episode steps: 275, steps per second: 134, episode reward: -21.708, mean reward: -0.079 [-10.544, 0.314], mean action: -0.505 [-1.193, 1.064], mean observation: -0.177 [-7.879, 5.264], loss: 0.786689, mean_absolute_error: 0.803097, mean_q: 12.899315
 614964/1000000: episode: 1963, duration: 1.620s, episode steps: 216, steps per second: 133, episode reward: 6.578, mean reward: 0.030 [-11.080, 0.400], mean action: -0.508 [-1.148, 1.163], mean observation: -0.147 [-7.987, 1.000], loss: 0.721877, mean_absolute_error: 0.798840, mean_q: 12.852009
 615227/1000000: episode: 1964, duration: 1.967s, episode steps: 263, steps per second: 134, episode reward: 22.497, mean reward: 0.086 [-10.856, 0.638], mean action: -0.516 [-1.176, 1.043], mean observation: -0.189 [-7.875, 4.963], loss: 0.889650, mean_absolute_error: 0.828283, mean_q: 12.894931
 615490/1000000: episode: 1965, duration: 1.974s, episode steps: 263, steps per second: 133, episode reward: -14.482, mean reward: -0.055 [-10.787, 0.450], mean action: -0.494 [-1.103, 1.123], mean observation: -0.190 [-7.959, 5.112], loss: 0.769639, mean_absolute_error: 0.804584, mean_q: 12.901898
 615671/1000000: episode: 1966, duration: 1.367s, episode steps: 181, steps per second: 132, episode reward: -15.779, mean reward: -0.087 [-10.561, 0.068], mean action: -0.510 [-1.105, 1.131], mean observation: -0.085 [-6.042, 1.000], loss: 0.817586, mean_absolute_error: 0.814201, mean_q: 12.912608
 615905/1000000: episode: 1967, duration: 1.746s, episode steps: 234, steps per second: 134, episode reward: -70.749, mean reward: -0.302 [-11.409, 0.170], mean action: -0.506 [-1.109, 1.109], mean observation: -0.171 [-8.028, 2.366], loss: 0.895794, mean_absolute_error: 0.819855, mean_q: 12.945424
 616122/1000000: episode: 1968, duration: 1.622s, episode steps: 217, steps per second: 134, episode reward: -25.227, mean reward: -0.116 [-11.261, 0.273], mean action: -0.492 [-1.158, 1.094], mean observation: -0.182 [-7.959, 1.000], loss: 0.820361, mean_absolute_error: 0.818228, mean_q: 12.879474
 616383/1000000: episode: 1969, duration: 1.946s, episode steps: 261, steps per second: 134, episode reward: -11.966, mean reward: -0.046 [-10.723, 0.425], mean action: -0.490 [-1.138, 1.143], mean observation: -0.193 [-8.026, 5.049], loss: 0.909064, mean_absolute_error: 0.816067, mean_q: 12.865548
 616629/1000000: episode: 1970, duration: 1.843s, episode steps: 246, steps per second: 134, episode reward: 7.311, mean reward: 0.030 [-10.976, 0.485], mean action: -0.508 [-1.122, 1.060], mean observation: -0.184 [-7.939, 3.622], loss: 0.859939, mean_absolute_error: 0.815145, mean_q: 12.808578
 616894/1000000: episode: 1971, duration: 1.978s, episode steps: 265, steps per second: 134, episode reward: -83.774, mean reward: -0.316 [-11.045, 0.255], mean action: -0.492 [-1.123, 1.105], mean observation: -0.214 [-8.006, 5.162], loss: 0.794219, mean_absolute_error: 0.799806, mean_q: 12.947605
 617099/1000000: episode: 1972, duration: 1.535s, episode steps: 205, steps per second: 134, episode reward: -5.600, mean reward: -0.027 [-10.992, 0.272], mean action: -0.484 [-1.130, 1.109], mean observation: -0.146 [-7.500, 1.000], loss: 0.807782, mean_absolute_error: 0.805138, mean_q: 12.899840
 617340/1000000: episode: 1973, duration: 1.807s, episode steps: 241, steps per second: 133, episode reward: -51.344, mean reward: -0.213 [-11.357, 0.281], mean action: -0.497 [-1.148, 1.059], mean observation: -0.206 [-7.939, 2.840], loss: 0.804214, mean_absolute_error: 0.811595, mean_q: 12.883512
 617596/1000000: episode: 1974, duration: 1.913s, episode steps: 256, steps per second: 134, episode reward: -47.410, mean reward: -0.185 [-10.911, 0.268], mean action: -0.496 [-1.087, 1.091], mean observation: -0.153 [-7.953, 4.769], loss: 0.825702, mean_absolute_error: 0.811141, mean_q: 12.866440
 617800/1000000: episode: 1975, duration: 1.526s, episode steps: 204, steps per second: 134, episode reward: -16.784, mean reward: -0.082 [-10.893, 0.165], mean action: -0.481 [-1.060, 1.070], mean observation: -0.102 [-7.457, 1.000], loss: 0.795481, mean_absolute_error: 0.813653, mean_q: 12.864871
 618033/1000000: episode: 1976, duration: 1.739s, episode steps: 233, steps per second: 134, episode reward: -44.034, mean reward: -0.189 [-11.367, 0.295], mean action: -0.477 [-1.099, 1.126], mean observation: -0.187 [-7.961, 1.976], loss: 0.831860, mean_absolute_error: 0.815983, mean_q: 12.836066
 618244/1000000: episode: 1977, duration: 1.582s, episode steps: 211, steps per second: 133, episode reward: -19.267, mean reward: -0.091 [-10.979, 0.186], mean action: -0.506 [-1.125, 1.060], mean observation: -0.111 [-7.778, 1.000], loss: 0.881417, mean_absolute_error: 0.813193, mean_q: 12.866035
 618480/1000000: episode: 1978, duration: 1.765s, episode steps: 236, steps per second: 134, episode reward: -49.291, mean reward: -0.209 [-11.410, 0.284], mean action: -0.491 [-1.115, 1.081], mean observation: -0.203 [-7.980, 2.209], loss: 0.885648, mean_absolute_error: 0.812239, mean_q: 13.003124
 618650/1000000: episode: 1979, duration: 1.275s, episode steps: 170, steps per second: 133, episode reward: 4.146, mean reward: 0.024 [-10.424, 0.181], mean action: -0.480 [-1.102, 1.178], mean observation: -0.051 [-6.067, 1.000], loss: 0.863290, mean_absolute_error: 0.808159, mean_q: 12.862213
 618885/1000000: episode: 1980, duration: 1.768s, episode steps: 235, steps per second: 133, episode reward: -78.009, mean reward: -0.332 [-11.452, 0.130], mean action: -0.509 [-1.139, 1.071], mean observation: -0.170 [-7.987, 2.129], loss: 0.781428, mean_absolute_error: 0.797471, mean_q: 12.905554
 619143/1000000: episode: 1981, duration: 1.923s, episode steps: 258, steps per second: 134, episode reward: -34.276, mean reward: -0.133 [-11.026, 0.403], mean action: -0.498 [-1.127, 1.122], mean observation: -0.171 [-7.965, 4.803], loss: 0.861458, mean_absolute_error: 0.809396, mean_q: 12.913678
 619267/1000000: episode: 1982, duration: 0.938s, episode steps: 124, steps per second: 132, episode reward: 22.580, mean reward: 0.182 [-9.824, 0.278], mean action: -0.487 [-1.068, 1.043], mean observation: 0.044 [-4.756, 1.000], loss: 0.911547, mean_absolute_error: 0.812457, mean_q: 12.980259
 619515/1000000: episode: 1983, duration: 1.858s, episode steps: 248, steps per second: 133, episode reward: -34.346, mean reward: -0.138 [-11.284, 0.398], mean action: -0.489 [-1.081, 1.102], mean observation: -0.188 [-7.946, 3.569], loss: 0.820616, mean_absolute_error: 0.792887, mean_q: 12.911733
 619616/1000000: episode: 1984, duration: 0.766s, episode steps: 101, steps per second: 132, episode reward: 6.167, mean reward: 0.061 [-9.875, 0.167], mean action: -0.529 [-1.124, 1.075], mean observation: 0.044 [-3.651, 1.000], loss: 0.816041, mean_absolute_error: 0.801047, mean_q: 12.865569
 619888/1000000: episode: 1985, duration: 2.032s, episode steps: 272, steps per second: 134, episode reward: 2.757, mean reward: 0.010 [-10.431, 0.378], mean action: -0.471 [-1.087, 1.057], mean observation: -0.166 [-7.893, 5.287], loss: 0.828758, mean_absolute_error: 0.805927, mean_q: 12.926548
 620113/1000000: episode: 1986, duration: 1.684s, episode steps: 225, steps per second: 134, episode reward: -2.093, mean reward: -0.009 [-10.860, 0.277], mean action: -0.481 [-1.098, 1.078], mean observation: -0.136 [-7.902, 1.000], loss: 0.890296, mean_absolute_error: 0.812590, mean_q: 12.840551
 620309/1000000: episode: 1987, duration: 1.458s, episode steps: 196, steps per second: 134, episode reward: 3.095, mean reward: 0.016 [-10.651, 0.218], mean action: -0.488 [-1.142, 1.097], mean observation: -0.092 [-7.056, 1.000], loss: 0.816606, mean_absolute_error: 0.800117, mean_q: 12.832253
 620510/1000000: episode: 1988, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: 12.875, mean reward: 0.064 [-10.879, 0.353], mean action: -0.520 [-1.118, 1.038], mean observation: -0.125 [-7.150, 1.000], loss: 0.847991, mean_absolute_error: 0.803008, mean_q: 12.892636
 620735/1000000: episode: 1989, duration: 1.686s, episode steps: 225, steps per second: 133, episode reward: -30.578, mean reward: -0.136 [-11.282, 0.252], mean action: -0.509 [-1.099, 1.074], mean observation: -0.186 [-7.920, 1.000], loss: 0.899450, mean_absolute_error: 0.808424, mean_q: 12.967631
 620947/1000000: episode: 1990, duration: 1.581s, episode steps: 212, steps per second: 134, episode reward: -26.055, mean reward: -0.123 [-11.213, 0.227], mean action: -0.503 [-1.105, 1.043], mean observation: -0.153 [-7.673, 1.000], loss: 0.909888, mean_absolute_error: 0.795314, mean_q: 12.942139
 621165/1000000: episode: 1991, duration: 1.644s, episode steps: 218, steps per second: 133, episode reward: 25.037, mean reward: 0.115 [-10.905, 0.451], mean action: -0.511 [-1.096, 1.076], mean observation: -0.154 [-7.979, 1.000], loss: 0.826665, mean_absolute_error: 0.805234, mean_q: 12.938013
 621378/1000000: episode: 1992, duration: 1.597s, episode steps: 213, steps per second: 133, episode reward: 25.727, mean reward: 0.121 [-10.907, 0.447], mean action: -0.515 [-1.101, 1.053], mean observation: -0.148 [-7.868, 1.000], loss: 0.797789, mean_absolute_error: 0.798288, mean_q: 12.988453
 621601/1000000: episode: 1993, duration: 1.676s, episode steps: 223, steps per second: 133, episode reward: -30.200, mean reward: -0.135 [-11.264, 0.246], mean action: -0.464 [-1.092, 1.111], mean observation: -0.175 [-7.902, 1.000], loss: 0.924096, mean_absolute_error: 0.808725, mean_q: 12.897417
 621732/1000000: episode: 1994, duration: 0.988s, episode steps: 131, steps per second: 133, episode reward: 7.225, mean reward: 0.055 [-10.010, 0.158], mean action: -0.505 [-1.085, 1.066], mean observation: 0.024 [-5.191, 1.000], loss: 0.823012, mean_absolute_error: 0.801967, mean_q: 12.853823
 621952/1000000: episode: 1995, duration: 1.648s, episode steps: 220, steps per second: 134, episode reward: 1.428, mean reward: 0.006 [-10.890, 0.289], mean action: -0.501 [-1.126, 1.107], mean observation: -0.143 [-7.948, 1.000], loss: 0.814668, mean_absolute_error: 0.791911, mean_q: 12.912926
 622220/1000000: episode: 1996, duration: 2.000s, episode steps: 268, steps per second: 134, episode reward: -24.869, mean reward: -0.093 [-10.928, 0.517], mean action: -0.482 [-1.085, 1.082], mean observation: -0.176 [-7.948, 5.189], loss: 0.834076, mean_absolute_error: 0.801031, mean_q: 12.917393
 622492/1000000: episode: 1997, duration: 2.035s, episode steps: 272, steps per second: 134, episode reward: -54.360, mean reward: -0.200 [-10.789, 0.317], mean action: -0.504 [-1.175, 1.091], mean observation: -0.150 [-7.947, 5.201], loss: 0.794552, mean_absolute_error: 0.786781, mean_q: 12.959216
 622744/1000000: episode: 1998, duration: 1.876s, episode steps: 252, steps per second: 134, episode reward: -2.729, mean reward: -0.011 [-11.142, 0.543], mean action: -0.510 [-1.098, 1.007], mean observation: -0.186 [-7.885, 3.951], loss: 0.743811, mean_absolute_error: 0.789938, mean_q: 12.816624
 622905/1000000: episode: 1999, duration: 1.216s, episode steps: 161, steps per second: 132, episode reward: 13.685, mean reward: 0.085 [-10.201, 0.206], mean action: -0.504 [-1.153, 1.143], mean observation: -0.033 [-6.125, 1.000], loss: 0.776301, mean_absolute_error: 0.800125, mean_q: 13.075524
 623072/1000000: episode: 2000, duration: 1.254s, episode steps: 167, steps per second: 133, episode reward: -12.503, mean reward: -0.075 [-10.532, 0.090], mean action: -0.510 [-1.092, 1.062], mean observation: -0.083 [-6.035, 1.000], loss: 0.791926, mean_absolute_error: 0.800073, mean_q: 12.914790
 623271/1000000: episode: 2001, duration: 1.490s, episode steps: 199, steps per second: 134, episode reward: -19.649, mean reward: -0.099 [-11.037, 0.192], mean action: -0.495 [-1.118, 1.032], mean observation: -0.137 [-7.047, 1.000], loss: 0.825831, mean_absolute_error: 0.792092, mean_q: 12.906899
 623529/1000000: episode: 2002, duration: 1.933s, episode steps: 258, steps per second: 133, episode reward: -49.173, mean reward: -0.191 [-11.189, 0.387], mean action: -0.469 [-1.091, 1.151], mean observation: -0.191 [-7.975, 4.631], loss: 0.847083, mean_absolute_error: 0.799844, mean_q: 12.912403
 623776/1000000: episode: 2003, duration: 1.842s, episode steps: 247, steps per second: 134, episode reward: 4.785, mean reward: 0.019 [-11.088, 0.554], mean action: -0.482 [-1.081, 1.116], mean observation: -0.189 [-7.942, 3.805], loss: 0.776822, mean_absolute_error: 0.797330, mean_q: 12.883581
 624014/1000000: episode: 2004, duration: 1.792s, episode steps: 238, steps per second: 133, episode reward: -2.643, mean reward: -0.011 [-10.804, 0.311], mean action: -0.503 [-1.114, 1.069], mean observation: -0.153 [-7.973, 2.644], loss: 0.767296, mean_absolute_error: 0.790737, mean_q: 12.965957
 624243/1000000: episode: 2005, duration: 1.704s, episode steps: 229, steps per second: 134, episode reward: -69.800, mean reward: -0.305 [-11.456, 0.132], mean action: -0.508 [-1.115, 1.098], mean observation: -0.200 [-7.970, 1.235], loss: 0.903809, mean_absolute_error: 0.801425, mean_q: 12.960368
 624502/1000000: episode: 2006, duration: 1.935s, episode steps: 259, steps per second: 134, episode reward: -26.678, mean reward: -0.103 [-11.099, 0.486], mean action: -0.486 [-1.127, 1.164], mean observation: -0.185 [-7.944, 4.787], loss: 0.868832, mean_absolute_error: 0.800107, mean_q: 12.859695
 624742/1000000: episode: 2007, duration: 1.793s, episode steps: 240, steps per second: 134, episode reward: -16.918, mean reward: -0.070 [-11.130, 0.383], mean action: -0.502 [-1.097, 1.100], mean observation: -0.188 [-7.958, 2.912], loss: 0.712793, mean_absolute_error: 0.772677, mean_q: 12.898710
 624989/1000000: episode: 2008, duration: 1.849s, episode steps: 247, steps per second: 134, episode reward: 3.129, mean reward: 0.013 [-11.010, 0.490], mean action: -0.500 [-1.086, 1.075], mean observation: -0.186 [-7.943, 3.767], loss: 0.853101, mean_absolute_error: 0.801797, mean_q: 12.999711
 625268/1000000: episode: 2009, duration: 2.090s, episode steps: 279, steps per second: 133, episode reward: -46.541, mean reward: -0.167 [-10.585, 0.293], mean action: -0.514 [-1.143, 1.049], mean observation: -0.154 [-7.887, 5.282], loss: 0.765834, mean_absolute_error: 0.793951, mean_q: 12.991593
 625483/1000000: episode: 2010, duration: 1.612s, episode steps: 215, steps per second: 133, episode reward: -37.989, mean reward: -0.177 [-11.297, 0.198], mean action: -0.500 [-1.146, 1.063], mean observation: -0.157 [-7.927, 1.000], loss: 0.800851, mean_absolute_error: 0.790819, mean_q: 12.955643
 625749/1000000: episode: 2011, duration: 1.991s, episode steps: 266, steps per second: 134, episode reward: 27.666, mean reward: 0.104 [-10.773, 0.703], mean action: -0.511 [-1.108, 1.075], mean observation: -0.179 [-7.988, 5.134], loss: 0.859995, mean_absolute_error: 0.804951, mean_q: 12.911699
 625991/1000000: episode: 2012, duration: 1.821s, episode steps: 242, steps per second: 133, episode reward: -33.699, mean reward: -0.139 [-11.296, 0.381], mean action: -0.472 [-1.066, 1.127], mean observation: -0.189 [-7.949, 3.141], loss: 0.858231, mean_absolute_error: 0.792955, mean_q: 12.976068
 626243/1000000: episode: 2013, duration: 1.893s, episode steps: 252, steps per second: 133, episode reward: -40.340, mean reward: -0.160 [-11.094, 0.282], mean action: -0.482 [-1.090, 1.061], mean observation: -0.196 [-7.840, 3.814], loss: 0.856556, mean_absolute_error: 0.786937, mean_q: 12.975853
 626462/1000000: episode: 2014, duration: 1.631s, episode steps: 219, steps per second: 134, episode reward: -25.648, mean reward: -0.117 [-10.952, 0.172], mean action: -0.488 [-1.092, 1.090], mean observation: -0.123 [-8.011, 1.000], loss: 0.903458, mean_absolute_error: 0.800320, mean_q: 12.918479
 626660/1000000: episode: 2015, duration: 1.491s, episode steps: 198, steps per second: 133, episode reward: 24.052, mean reward: 0.121 [-10.746, 0.383], mean action: -0.479 [-1.156, 1.206], mean observation: -0.119 [-7.395, 1.000], loss: 0.804187, mean_absolute_error: 0.791200, mean_q: 13.043154
 626885/1000000: episode: 2016, duration: 1.682s, episode steps: 225, steps per second: 134, episode reward: 18.010, mean reward: 0.080 [-10.993, 0.445], mean action: -0.496 [-1.114, 1.139], mean observation: -0.155 [-7.928, 1.000], loss: 0.908470, mean_absolute_error: 0.799287, mean_q: 13.010007
 627135/1000000: episode: 2017, duration: 1.872s, episode steps: 250, steps per second: 134, episode reward: 16.727, mean reward: 0.067 [-10.719, 0.457], mean action: -0.507 [-1.160, 1.127], mean observation: -0.167 [-8.043, 4.280], loss: 0.920602, mean_absolute_error: 0.807003, mean_q: 12.975049
 627397/1000000: episode: 2018, duration: 1.971s, episode steps: 262, steps per second: 133, episode reward: 18.535, mean reward: 0.071 [-10.699, 0.522], mean action: -0.488 [-1.100, 1.076], mean observation: -0.163 [-7.927, 4.983], loss: 0.776746, mean_absolute_error: 0.796221, mean_q: 13.030402
 627589/1000000: episode: 2019, duration: 1.442s, episode steps: 192, steps per second: 133, episode reward: -31.914, mean reward: -0.166 [-11.027, 0.102], mean action: -0.491 [-1.091, 1.101], mean observation: -0.148 [-6.850, 1.000], loss: 0.843687, mean_absolute_error: 0.802262, mean_q: 12.954528
 627833/1000000: episode: 2020, duration: 1.820s, episode steps: 244, steps per second: 134, episode reward: -12.035, mean reward: -0.049 [-11.186, 0.479], mean action: -0.474 [-1.079, 1.116], mean observation: -0.186 [-7.977, 3.463], loss: 0.832576, mean_absolute_error: 0.802439, mean_q: 12.956857
 628085/1000000: episode: 2021, duration: 1.893s, episode steps: 252, steps per second: 133, episode reward: 30.565, mean reward: 0.121 [-10.866, 0.603], mean action: -0.520 [-1.154, 1.082], mean observation: -0.182 [-7.952, 4.282], loss: 0.847971, mean_absolute_error: 0.796994, mean_q: 13.042439
 628314/1000000: episode: 2022, duration: 1.710s, episode steps: 229, steps per second: 134, episode reward: -49.925, mean reward: -0.218 [-11.412, 0.221], mean action: -0.518 [-1.106, 1.016], mean observation: -0.191 [-7.886, 1.000], loss: 0.815458, mean_absolute_error: 0.791728, mean_q: 13.065542
 628558/1000000: episode: 2023, duration: 1.822s, episode steps: 244, steps per second: 134, episode reward: -32.251, mean reward: -0.132 [-11.264, 0.408], mean action: -0.529 [-1.156, 1.062], mean observation: -0.197 [-7.967, 3.573], loss: 0.806646, mean_absolute_error: 0.797296, mean_q: 13.009314
 628810/1000000: episode: 2024, duration: 1.884s, episode steps: 252, steps per second: 134, episode reward: -17.288, mean reward: -0.069 [-10.711, 0.302], mean action: -0.491 [-1.103, 1.119], mean observation: -0.149 [-8.021, 4.482], loss: 0.850798, mean_absolute_error: 0.794830, mean_q: 13.061477
 628982/1000000: episode: 2025, duration: 1.290s, episode steps: 172, steps per second: 133, episode reward: 12.150, mean reward: 0.071 [-10.443, 0.247], mean action: -0.488 [-1.057, 1.088], mean observation: -0.079 [-5.990, 1.000], loss: 0.831576, mean_absolute_error: 0.793595, mean_q: 13.041083
 629244/1000000: episode: 2026, duration: 1.963s, episode steps: 262, steps per second: 133, episode reward: -11.307, mean reward: -0.043 [-11.013, 0.566], mean action: -0.509 [-1.128, 1.107], mean observation: -0.188 [-7.930, 4.972], loss: 0.840440, mean_absolute_error: 0.786160, mean_q: 13.003215
 629504/1000000: episode: 2027, duration: 1.958s, episode steps: 260, steps per second: 133, episode reward: -26.869, mean reward: -0.103 [-11.107, 0.492], mean action: -0.488 [-1.073, 1.075], mean observation: -0.182 [-7.891, 4.818], loss: 0.724735, mean_absolute_error: 0.787260, mean_q: 13.025445
 629774/1000000: episode: 2028, duration: 2.019s, episode steps: 270, steps per second: 134, episode reward: -64.789, mean reward: -0.240 [-10.608, 0.136], mean action: -0.506 [-1.099, 1.042], mean observation: -0.181 [-7.901, 5.241], loss: 0.778748, mean_absolute_error: 0.779836, mean_q: 12.998097
 630025/1000000: episode: 2029, duration: 1.884s, episode steps: 251, steps per second: 133, episode reward: -14.342, mean reward: -0.057 [-10.696, 0.291], mean action: -0.489 [-1.142, 1.125], mean observation: -0.168 [-7.960, 4.288], loss: 0.869592, mean_absolute_error: 0.792024, mean_q: 13.024717
 630280/1000000: episode: 2030, duration: 1.911s, episode steps: 255, steps per second: 133, episode reward: -30.659, mean reward: -0.120 [-11.137, 0.432], mean action: -0.509 [-1.148, 1.090], mean observation: -0.178 [-7.987, 4.456], loss: 0.816686, mean_absolute_error: 0.795232, mean_q: 13.019195
 630427/1000000: episode: 2031, duration: 1.105s, episode steps: 147, steps per second: 133, episode reward: 22.077, mean reward: 0.150 [-10.012, 0.258], mean action: -0.491 [-1.063, 1.080], mean observation: -0.016 [-5.858, 1.000], loss: 0.958982, mean_absolute_error: 0.800124, mean_q: 13.077936
 630698/1000000: episode: 2032, duration: 2.027s, episode steps: 271, steps per second: 134, episode reward: -4.867, mean reward: -0.018 [-10.779, 0.512], mean action: -0.493 [-1.115, 1.135], mean observation: -0.165 [-7.906, 5.226], loss: 0.766178, mean_absolute_error: 0.786772, mean_q: 13.067763
 630959/1000000: episode: 2033, duration: 1.952s, episode steps: 261, steps per second: 134, episode reward: -57.780, mean reward: -0.221 [-11.018, 0.302], mean action: -0.503 [-1.117, 1.147], mean observation: -0.200 [-7.955, 4.969], loss: 0.793118, mean_absolute_error: 0.776522, mean_q: 12.992139
 631197/1000000: episode: 2034, duration: 1.783s, episode steps: 238, steps per second: 134, episode reward: -10.586, mean reward: -0.044 [-11.006, 0.340], mean action: -0.497 [-1.113, 1.040], mean observation: -0.175 [-7.916, 2.463], loss: 0.989976, mean_absolute_error: 0.806910, mean_q: 13.015180
 631374/1000000: episode: 2035, duration: 1.332s, episode steps: 177, steps per second: 133, episode reward: 28.428, mean reward: 0.161 [-10.369, 0.331], mean action: -0.508 [-1.159, 1.065], mean observation: -0.078 [-6.024, 1.000], loss: 0.819189, mean_absolute_error: 0.797406, mean_q: 13.120549
 631562/1000000: episode: 2036, duration: 1.408s, episode steps: 188, steps per second: 134, episode reward: -3.888, mean reward: -0.021 [-10.646, 0.170], mean action: -0.481 [-1.071, 1.144], mean observation: -0.110 [-6.353, 1.000], loss: 0.811088, mean_absolute_error: 0.784119, mean_q: 13.008470
 631819/1000000: episode: 2037, duration: 1.917s, episode steps: 257, steps per second: 134, episode reward: -49.137, mean reward: -0.191 [-10.765, 0.206], mean action: -0.500 [-1.147, 1.131], mean observation: -0.188 [-8.017, 4.825], loss: 0.763989, mean_absolute_error: 0.776651, mean_q: 13.148808
 632032/1000000: episode: 2038, duration: 1.598s, episode steps: 213, steps per second: 133, episode reward: 16.328, mean reward: 0.077 [-10.991, 0.419], mean action: -0.519 [-1.123, 1.064], mean observation: -0.144 [-7.766, 1.000], loss: 0.790972, mean_absolute_error: 0.786680, mean_q: 13.097583
 632241/1000000: episode: 2039, duration: 1.571s, episode steps: 209, steps per second: 133, episode reward: -18.461, mean reward: -0.088 [-10.936, 0.181], mean action: -0.498 [-1.120, 1.140], mean observation: -0.109 [-7.847, 1.000], loss: 0.922061, mean_absolute_error: 0.785626, mean_q: 13.080955
 632425/1000000: episode: 2040, duration: 1.378s, episode steps: 184, steps per second: 134, episode reward: 7.381, mean reward: 0.040 [-10.524, 0.215], mean action: -0.486 [-1.115, 1.067], mean observation: -0.093 [-5.980, 1.000], loss: 0.830101, mean_absolute_error: 0.780566, mean_q: 13.032097
 632660/1000000: episode: 2041, duration: 1.757s, episode steps: 235, steps per second: 134, episode reward: 25.249, mean reward: 0.107 [-11.037, 0.525], mean action: -0.495 [-1.111, 1.074], mean observation: -0.169 [-7.868, 1.614], loss: 0.791702, mean_absolute_error: 0.786334, mean_q: 13.088005
 632903/1000000: episode: 2042, duration: 1.816s, episode steps: 243, steps per second: 134, episode reward: 12.487, mean reward: 0.051 [-11.105, 0.558], mean action: -0.486 [-1.084, 1.082], mean observation: -0.180 [-7.917, 3.148], loss: 0.840496, mean_absolute_error: 0.787069, mean_q: 13.052300
 633087/1000000: episode: 2043, duration: 1.378s, episode steps: 184, steps per second: 134, episode reward: 20.748, mean reward: 0.113 [-10.554, 0.318], mean action: -0.492 [-1.089, 1.086], mean observation: -0.086 [-6.060, 1.000], loss: 0.858150, mean_absolute_error: 0.784513, mean_q: 13.155725
 633350/1000000: episode: 2044, duration: 1.968s, episode steps: 263, steps per second: 134, episode reward: -35.009, mean reward: -0.133 [-10.788, 0.290], mean action: -0.486 [-1.102, 1.107], mean observation: -0.145 [-7.868, 4.965], loss: 0.921473, mean_absolute_error: 0.783641, mean_q: 13.053260
 633619/1000000: episode: 2045, duration: 2.017s, episode steps: 269, steps per second: 133, episode reward: -44.101, mean reward: -0.164 [-10.736, 0.311], mean action: -0.502 [-1.100, 1.091], mean observation: -0.198 [-7.991, 5.180], loss: 0.755712, mean_absolute_error: 0.779667, mean_q: 13.211159
 633845/1000000: episode: 2046, duration: 1.683s, episode steps: 226, steps per second: 134, episode reward: -15.190, mean reward: -0.067 [-11.195, 0.340], mean action: -0.500 [-1.091, 1.050], mean observation: -0.153 [-7.932, 1.000], loss: 0.821461, mean_absolute_error: 0.785148, mean_q: 13.119385
 634067/1000000: episode: 2047, duration: 1.681s, episode steps: 222, steps per second: 132, episode reward: -46.323, mean reward: -0.209 [-11.359, 0.185], mean action: -0.492 [-1.079, 1.040], mean observation: -0.184 [-7.895, 1.000], loss: 0.912526, mean_absolute_error: 0.799031, mean_q: 13.091619
 634296/1000000: episode: 2048, duration: 1.722s, episode steps: 229, steps per second: 133, episode reward: -11.529, mean reward: -0.050 [-11.105, 0.338], mean action: -0.512 [-1.139, 1.084], mean observation: -0.154 [-7.982, 1.269], loss: 0.755997, mean_absolute_error: 0.776946, mean_q: 13.145107
 634494/1000000: episode: 2049, duration: 1.479s, episode steps: 198, steps per second: 134, episode reward: 2.866, mean reward: 0.014 [-10.630, 0.207], mean action: -0.495 [-1.140, 1.084], mean observation: -0.082 [-6.835, 1.000], loss: 0.745217, mean_absolute_error: 0.777017, mean_q: 13.221566
 634694/1000000: episode: 2050, duration: 1.498s, episode steps: 200, steps per second: 134, episode reward: 3.893, mean reward: 0.019 [-10.930, 0.312], mean action: -0.503 [-1.125, 1.087], mean observation: -0.139 [-7.246, 1.000], loss: 0.737885, mean_absolute_error: 0.768701, mean_q: 13.106270
 634878/1000000: episode: 2051, duration: 1.387s, episode steps: 184, steps per second: 133, episode reward: 30.214, mean reward: 0.164 [-10.491, 0.365], mean action: -0.502 [-1.085, 1.077], mean observation: -0.093 [-6.023, 1.000], loss: 0.806046, mean_absolute_error: 0.784253, mean_q: 13.061781
 635064/1000000: episode: 2052, duration: 1.424s, episode steps: 186, steps per second: 131, episode reward: 5.863, mean reward: 0.032 [-10.665, 0.247], mean action: -0.525 [-1.171, 1.036], mean observation: -0.110 [-6.033, 1.000], loss: 0.823699, mean_absolute_error: 0.778381, mean_q: 13.118547
 635273/1000000: episode: 2053, duration: 1.570s, episode steps: 209, steps per second: 133, episode reward: -10.653, mean reward: -0.051 [-10.965, 0.228], mean action: -0.504 [-1.095, 1.080], mean observation: -0.113 [-7.568, 1.000], loss: 0.786318, mean_absolute_error: 0.768110, mean_q: 13.180139
 635540/1000000: episode: 2054, duration: 2.002s, episode steps: 267, steps per second: 133, episode reward: 8.435, mean reward: 0.032 [-10.816, 0.607], mean action: -0.476 [-1.105, 1.089], mean observation: -0.175 [-7.926, 5.176], loss: 0.851013, mean_absolute_error: 0.788625, mean_q: 13.140591
 635754/1000000: episode: 2055, duration: 1.603s, episode steps: 214, steps per second: 134, episode reward: -34.242, mean reward: -0.160 [-11.210, 0.182], mean action: -0.497 [-1.127, 1.061], mean observation: -0.178 [-7.836, 1.000], loss: 0.833224, mean_absolute_error: 0.775311, mean_q: 13.149268
 636022/1000000: episode: 2056, duration: 1.999s, episode steps: 268, steps per second: 134, episode reward: 33.957, mean reward: 0.127 [-10.669, 0.687], mean action: -0.495 [-1.082, 1.088], mean observation: -0.171 [-7.966, 5.204], loss: 0.822583, mean_absolute_error: 0.789657, mean_q: 13.084599
 636274/1000000: episode: 2057, duration: 1.888s, episode steps: 252, steps per second: 133, episode reward: -49.133, mean reward: -0.195 [-11.213, 0.381], mean action: -0.507 [-1.135, 1.097], mean observation: -0.193 [-7.991, 4.472], loss: 0.816257, mean_absolute_error: 0.786592, mean_q: 13.133549
 636465/1000000: episode: 2058, duration: 1.428s, episode steps: 191, steps per second: 134, episode reward: -19.101, mean reward: -0.100 [-10.937, 0.159], mean action: -0.498 [-1.112, 1.127], mean observation: -0.131 [-6.703, 1.000], loss: 0.713221, mean_absolute_error: 0.760541, mean_q: 13.272085
 636652/1000000: episode: 2059, duration: 1.453s, episode steps: 187, steps per second: 129, episode reward: -6.414, mean reward: -0.034 [-10.744, 0.183], mean action: -0.494 [-1.130, 1.146], mean observation: -0.090 [-6.603, 1.000], loss: 0.760490, mean_absolute_error: 0.766260, mean_q: 13.169919
 636904/1000000: episode: 2060, duration: 1.887s, episode steps: 252, steps per second: 134, episode reward: -42.995, mean reward: -0.171 [-10.921, 0.240], mean action: -0.512 [-1.111, 1.061], mean observation: -0.194 [-7.980, 4.300], loss: 0.762104, mean_absolute_error: 0.772492, mean_q: 13.199407
 637142/1000000: episode: 2061, duration: 1.785s, episode steps: 238, steps per second: 133, episode reward: -35.512, mean reward: -0.149 [-11.328, 0.357], mean action: -0.509 [-1.114, 1.062], mean observation: -0.198 [-7.969, 2.687], loss: 0.793088, mean_absolute_error: 0.778258, mean_q: 13.076543
 637406/1000000: episode: 2062, duration: 1.980s, episode steps: 264, steps per second: 133, episode reward: -1.899, mean reward: -0.007 [-10.596, 0.415], mean action: -0.495 [-1.122, 1.091], mean observation: -0.179 [-7.953, 5.149], loss: 0.783160, mean_absolute_error: 0.775301, mean_q: 13.096426
 637649/1000000: episode: 2063, duration: 1.815s, episode steps: 243, steps per second: 134, episode reward: -63.405, mean reward: -0.261 [-11.380, 0.263], mean action: -0.493 [-1.081, 1.099], mean observation: -0.201 [-7.972, 3.387], loss: 0.724320, mean_absolute_error: 0.766823, mean_q: 13.216105
 637801/1000000: episode: 2064, duration: 1.140s, episode steps: 152, steps per second: 133, episode reward: 1.660, mean reward: 0.011 [-10.202, 0.126], mean action: -0.474 [-1.113, 1.104], mean observation: -0.003 [-5.965, 1.000], loss: 0.928439, mean_absolute_error: 0.780358, mean_q: 13.138736
 638042/1000000: episode: 2065, duration: 1.801s, episode steps: 241, steps per second: 134, episode reward: -53.645, mean reward: -0.223 [-11.363, 0.279], mean action: -0.506 [-1.115, 1.007], mean observation: -0.180 [-7.927, 2.935], loss: 0.732157, mean_absolute_error: 0.770447, mean_q: 13.148587
 638282/1000000: episode: 2066, duration: 1.792s, episode steps: 240, steps per second: 134, episode reward: -19.555, mean reward: -0.081 [-11.277, 0.427], mean action: -0.504 [-1.088, 1.054], mean observation: -0.191 [-7.880, 2.657], loss: 0.870795, mean_absolute_error: 0.793378, mean_q: 13.093217
 638478/1000000: episode: 2067, duration: 1.469s, episode steps: 196, steps per second: 133, episode reward: 11.730, mean reward: 0.060 [-10.657, 0.275], mean action: -0.509 [-1.100, 1.052], mean observation: -0.096 [-7.045, 1.000], loss: 0.923481, mean_absolute_error: 0.781916, mean_q: 13.110392
 638659/1000000: episode: 2068, duration: 1.361s, episode steps: 181, steps per second: 133, episode reward: 20.250, mean reward: 0.112 [-10.384, 0.270], mean action: -0.521 [-1.123, 1.035], mean observation: -0.067 [-6.019, 1.000], loss: 0.818523, mean_absolute_error: 0.765603, mean_q: 13.184196
 638930/1000000: episode: 2069, duration: 2.023s, episode steps: 271, steps per second: 134, episode reward: -84.839, mean reward: -0.313 [-11.023, 0.253], mean action: -0.509 [-1.097, 1.055], mean observation: -0.168 [-7.936, 5.241], loss: 0.800059, mean_absolute_error: 0.772139, mean_q: 13.245044
 639183/1000000: episode: 2070, duration: 1.903s, episode steps: 253, steps per second: 133, episode reward: -17.529, mean reward: -0.069 [-10.691, 0.276], mean action: -0.515 [-1.093, 1.101], mean observation: -0.168 [-7.930, 4.365], loss: 0.798853, mean_absolute_error: 0.780137, mean_q: 13.209436
 639340/1000000: episode: 2071, duration: 1.185s, episode steps: 157, steps per second: 132, episode reward: 3.066, mean reward: 0.020 [-10.145, 0.119], mean action: -0.490 [-1.111, 1.093], mean observation: -0.012 [-5.933, 1.000], loss: 0.921498, mean_absolute_error: 0.786190, mean_q: 13.188977
 639563/1000000: episode: 2072, duration: 1.673s, episode steps: 223, steps per second: 133, episode reward: -60.003, mean reward: -0.269 [-11.405, 0.130], mean action: -0.487 [-1.063, 1.130], mean observation: -0.190 [-7.923, 1.000], loss: 0.914773, mean_absolute_error: 0.786642, mean_q: 13.132783
 639718/1000000: episode: 2073, duration: 1.168s, episode steps: 155, steps per second: 133, episode reward: 8.457, mean reward: 0.055 [-10.158, 0.165], mean action: -0.495 [-1.134, 1.121], mean observation: -0.004 [-6.029, 1.000], loss: 0.817105, mean_absolute_error: 0.777524, mean_q: 13.112716
 639942/1000000: episode: 2074, duration: 1.664s, episode steps: 224, steps per second: 135, episode reward: 0.873, mean reward: 0.004 [-11.054, 0.364], mean action: -0.497 [-1.117, 1.092], mean observation: -0.172 [-7.954, 1.000], loss: 0.884112, mean_absolute_error: 0.788375, mean_q: 13.130315
 640110/1000000: episode: 2075, duration: 1.263s, episode steps: 168, steps per second: 133, episode reward: 6.979, mean reward: 0.042 [-10.391, 0.198], mean action: -0.494 [-1.110, 1.051], mean observation: -0.068 [-6.004, 1.000], loss: 0.748151, mean_absolute_error: 0.772413, mean_q: 13.236586
 640189/1000000: episode: 2076, duration: 0.602s, episode steps: 79, steps per second: 131, episode reward: 3.735, mean reward: 0.047 [-9.836, 0.176], mean action: -0.491 [-1.082, 1.063], mean observation: 0.084 [-2.334, 1.000], loss: 0.779960, mean_absolute_error: 0.770372, mean_q: 13.128571
 640396/1000000: episode: 2077, duration: 1.555s, episode steps: 207, steps per second: 133, episode reward: 14.247, mean reward: 0.069 [-10.934, 0.379], mean action: -0.513 [-1.163, 1.033], mean observation: -0.136 [-7.593, 1.000], loss: 0.735455, mean_absolute_error: 0.780657, mean_q: 13.210699
 640641/1000000: episode: 2078, duration: 1.831s, episode steps: 245, steps per second: 134, episode reward: -11.483, mean reward: -0.047 [-11.056, 0.406], mean action: -0.497 [-1.097, 1.075], mean observation: -0.169 [-7.916, 3.433], loss: 0.862118, mean_absolute_error: 0.782936, mean_q: 13.204574
 640918/1000000: episode: 2079, duration: 2.070s, episode steps: 277, steps per second: 134, episode reward: -45.153, mean reward: -0.163 [-10.441, 0.235], mean action: -0.507 [-1.176, 1.128], mean observation: -0.174 [-7.947, 5.238], loss: 0.792593, mean_absolute_error: 0.774221, mean_q: 13.194107
 641152/1000000: episode: 2080, duration: 1.750s, episode steps: 234, steps per second: 134, episode reward: -22.916, mean reward: -0.098 [-10.878, 0.209], mean action: -0.501 [-1.083, 1.076], mean observation: -0.163 [-7.960, 1.922], loss: 0.763452, mean_absolute_error: 0.777711, mean_q: 13.215934
 641331/1000000: episode: 2081, duration: 1.350s, episode steps: 179, steps per second: 133, episode reward: -32.556, mean reward: -0.182 [-10.785, 0.014], mean action: -0.514 [-1.098, 1.048], mean observation: -0.072 [-5.995, 1.000], loss: 0.783766, mean_absolute_error: 0.775550, mean_q: 13.271338
 641574/1000000: episode: 2082, duration: 1.816s, episode steps: 243, steps per second: 134, episode reward: -65.217, mean reward: -0.268 [-11.360, 0.247], mean action: -0.535 [-1.158, 1.104], mean observation: -0.194 [-8.029, 3.406], loss: 0.802829, mean_absolute_error: 0.772349, mean_q: 13.113550
 641809/1000000: episode: 2083, duration: 1.760s, episode steps: 235, steps per second: 134, episode reward: -46.912, mean reward: -0.200 [-11.391, 0.291], mean action: -0.497 [-1.055, 1.024], mean observation: -0.190 [-7.902, 2.050], loss: 0.900236, mean_absolute_error: 0.794759, mean_q: 13.187114
 641973/1000000: episode: 2084, duration: 1.230s, episode steps: 164, steps per second: 133, episode reward: -22.267, mean reward: -0.136 [-10.521, 0.014], mean action: -0.497 [-1.097, 1.069], mean observation: -0.043 [-6.000, 1.000], loss: 0.892437, mean_absolute_error: 0.772373, mean_q: 13.232761
 642210/1000000: episode: 2085, duration: 1.778s, episode steps: 237, steps per second: 133, episode reward: -21.121, mean reward: -0.089 [-10.846, 0.224], mean action: -0.506 [-1.073, 1.066], mean observation: -0.162 [-7.946, 2.444], loss: 0.783897, mean_absolute_error: 0.771620, mean_q: 13.235005
 642384/1000000: episode: 2086, duration: 1.310s, episode steps: 174, steps per second: 133, episode reward: -0.189, mean reward: -0.001 [-10.519, 0.173], mean action: -0.500 [-1.078, 1.085], mean observation: -0.057 [-6.018, 1.000], loss: 0.829120, mean_absolute_error: 0.775202, mean_q: 13.191487
 642624/1000000: episode: 2087, duration: 1.790s, episode steps: 240, steps per second: 134, episode reward: 18.567, mean reward: 0.077 [-10.839, 0.470], mean action: -0.493 [-1.122, 1.153], mean observation: -0.175 [-8.024, 3.307], loss: 0.813510, mean_absolute_error: 0.762321, mean_q: 13.287628
 642879/1000000: episode: 2088, duration: 1.906s, episode steps: 255, steps per second: 134, episode reward: -15.797, mean reward: -0.062 [-10.691, 0.299], mean action: -0.504 [-1.093, 1.097], mean observation: -0.152 [-7.951, 4.548], loss: 0.794938, mean_absolute_error: 0.766156, mean_q: 13.323520
 643100/1000000: episode: 2089, duration: 1.655s, episode steps: 221, steps per second: 134, episode reward: -20.662, mean reward: -0.093 [-11.253, 0.314], mean action: -0.496 [-1.106, 1.124], mean observation: -0.163 [-7.925, 1.000], loss: 0.854843, mean_absolute_error: 0.786715, mean_q: 13.224051
 643340/1000000: episode: 2090, duration: 1.798s, episode steps: 240, steps per second: 133, episode reward: 13.832, mean reward: 0.058 [-10.997, 0.483], mean action: -0.484 [-1.072, 1.105], mean observation: -0.174 [-7.921, 2.749], loss: 0.967117, mean_absolute_error: 0.792243, mean_q: 13.204879
 643593/1000000: episode: 2091, duration: 1.888s, episode steps: 253, steps per second: 134, episode reward: 10.028, mean reward: 0.040 [-10.817, 0.462], mean action: -0.516 [-1.147, 1.070], mean observation: -0.171 [-7.898, 4.315], loss: 0.854932, mean_absolute_error: 0.778098, mean_q: 13.271686
 643811/1000000: episode: 2092, duration: 1.631s, episode steps: 218, steps per second: 134, episode reward: -38.149, mean reward: -0.175 [-11.273, 0.199], mean action: -0.482 [-1.100, 1.057], mean observation: -0.179 [-7.931, 1.000], loss: 0.791936, mean_absolute_error: 0.768732, mean_q: 13.224616
 643974/1000000: episode: 2093, duration: 1.222s, episode steps: 163, steps per second: 133, episode reward: 27.570, mean reward: 0.169 [-10.198, 0.310], mean action: -0.477 [-1.088, 1.084], mean observation: -0.048 [-6.054, 1.000], loss: 0.790360, mean_absolute_error: 0.756598, mean_q: 13.155756
 644188/1000000: episode: 2094, duration: 1.600s, episode steps: 214, steps per second: 134, episode reward: -8.041, mean reward: -0.038 [-10.846, 0.223], mean action: -0.486 [-1.093, 1.060], mean observation: -0.142 [-7.881, 1.000], loss: 0.944086, mean_absolute_error: 0.781723, mean_q: 13.185905
 644434/1000000: episode: 2095, duration: 1.838s, episode steps: 246, steps per second: 134, episode reward: -84.213, mean reward: -0.342 [-11.394, 0.166], mean action: -0.506 [-1.089, 1.079], mean observation: -0.169 [-7.913, 3.535], loss: 0.769756, mean_absolute_error: 0.770382, mean_q: 13.202633
 644679/1000000: episode: 2096, duration: 1.841s, episode steps: 245, steps per second: 133, episode reward: 26.616, mean reward: 0.109 [-10.978, 0.595], mean action: -0.503 [-1.115, 1.072], mean observation: -0.177 [-7.928, 3.554], loss: 0.790277, mean_absolute_error: 0.766940, mean_q: 13.243336
 644922/1000000: episode: 2097, duration: 1.824s, episode steps: 243, steps per second: 133, episode reward: -49.511, mean reward: -0.204 [-11.288, 0.278], mean action: -0.481 [-1.082, 1.119], mean observation: -0.200 [-7.929, 3.136], loss: 0.807683, mean_absolute_error: 0.766028, mean_q: 13.282145
 645158/1000000: episode: 2098, duration: 1.756s, episode steps: 236, steps per second: 134, episode reward: 22.743, mean reward: 0.096 [-10.975, 0.511], mean action: -0.512 [-1.120, 1.092], mean observation: -0.181 [-7.929, 2.347], loss: 0.878531, mean_absolute_error: 0.773509, mean_q: 13.152485
 645380/1000000: episode: 2099, duration: 1.665s, episode steps: 222, steps per second: 133, episode reward: -2.623, mean reward: -0.012 [-10.880, 0.284], mean action: -0.527 [-1.188, 1.081], mean observation: -0.161 [-8.043, 1.000], loss: 0.918122, mean_absolute_error: 0.775424, mean_q: 13.121038
 645638/1000000: episode: 2100, duration: 1.931s, episode steps: 258, steps per second: 134, episode reward: -4.980, mean reward: -0.019 [-11.007, 0.505], mean action: -0.501 [-1.082, 1.111], mean observation: -0.172 [-7.880, 4.509], loss: 0.764263, mean_absolute_error: 0.764711, mean_q: 13.245085
 645808/1000000: episode: 2101, duration: 1.275s, episode steps: 170, steps per second: 133, episode reward: 6.125, mean reward: 0.036 [-10.433, 0.199], mean action: -0.501 [-1.161, 1.084], mean observation: -0.086 [-6.032, 1.000], loss: 0.815592, mean_absolute_error: 0.765564, mean_q: 13.216013
 646059/1000000: episode: 2102, duration: 1.878s, episode steps: 251, steps per second: 134, episode reward: 18.024, mean reward: 0.072 [-10.988, 0.577], mean action: -0.508 [-1.109, 1.073], mean observation: -0.188 [-7.884, 4.011], loss: 0.796432, mean_absolute_error: 0.759579, mean_q: 13.179041
 646332/1000000: episode: 2103, duration: 2.080s, episode steps: 273, steps per second: 131, episode reward: 20.580, mean reward: 0.075 [-10.588, 0.600], mean action: -0.529 [-1.192, 1.093], mean observation: -0.164 [-7.979, 5.207], loss: 0.803838, mean_absolute_error: 0.748026, mean_q: 13.175411
 646539/1000000: episode: 2104, duration: 1.550s, episode steps: 207, steps per second: 134, episode reward: 15.316, mean reward: 0.074 [-10.886, 0.367], mean action: -0.483 [-1.094, 1.087], mean observation: -0.144 [-7.599, 1.000], loss: 0.736032, mean_absolute_error: 0.757868, mean_q: 13.273183
 646791/1000000: episode: 2105, duration: 1.889s, episode steps: 252, steps per second: 133, episode reward: -19.620, mean reward: -0.078 [-11.131, 0.497], mean action: -0.531 [-1.116, 1.091], mean observation: -0.198 [-7.918, 4.407], loss: 0.771088, mean_absolute_error: 0.740780, mean_q: 13.284848
 646950/1000000: episode: 2106, duration: 1.190s, episode steps: 159, steps per second: 134, episode reward: 14.421, mean reward: 0.091 [-10.116, 0.197], mean action: -0.493 [-1.102, 1.093], mean observation: -0.019 [-6.002, 1.000], loss: 0.795358, mean_absolute_error: 0.749248, mean_q: 13.198789
 647189/1000000: episode: 2107, duration: 1.795s, episode steps: 239, steps per second: 133, episode reward: -12.296, mean reward: -0.051 [-11.174, 0.451], mean action: -0.490 [-1.102, 1.105], mean observation: -0.188 [-8.056, 3.108], loss: 0.761879, mean_absolute_error: 0.751954, mean_q: 13.296477
 647464/1000000: episode: 2108, duration: 2.076s, episode steps: 275, steps per second: 132, episode reward: -47.186, mean reward: -0.172 [-10.453, 0.204], mean action: -0.473 [-1.073, 1.175], mean observation: -0.140 [-7.949, 5.239], loss: 0.838122, mean_absolute_error: 0.760069, mean_q: 13.259945
 647669/1000000: episode: 2109, duration: 1.536s, episode steps: 205, steps per second: 133, episode reward: -19.013, mean reward: -0.093 [-11.004, 0.191], mean action: -0.480 [-1.113, 1.188], mean observation: -0.112 [-7.626, 1.000], loss: 0.762435, mean_absolute_error: 0.735521, mean_q: 13.262676
 647946/1000000: episode: 2110, duration: 2.062s, episode steps: 277, steps per second: 134, episode reward: -25.658, mean reward: -0.093 [-10.797, 0.549], mean action: -0.505 [-1.102, 1.066], mean observation: -0.192 [-7.945, 5.239], loss: 0.742106, mean_absolute_error: 0.741409, mean_q: 13.281520
 648145/1000000: episode: 2111, duration: 1.492s, episode steps: 199, steps per second: 133, episode reward: -41.535, mean reward: -0.209 [-11.152, 0.086], mean action: -0.492 [-1.092, 1.119], mean observation: -0.124 [-7.174, 1.000], loss: 0.904076, mean_absolute_error: 0.757203, mean_q: 13.196354
 648392/1000000: episode: 2112, duration: 1.840s, episode steps: 247, steps per second: 134, episode reward: -74.726, mean reward: -0.303 [-11.430, 0.213], mean action: -0.514 [-1.133, 1.097], mean observation: -0.206 [-7.898, 3.353], loss: 0.737433, mean_absolute_error: 0.733022, mean_q: 13.284052
 648533/1000000: episode: 2113, duration: 1.062s, episode steps: 141, steps per second: 133, episode reward: -7.344, mean reward: -0.052 [-10.108, 0.038], mean action: -0.480 [-1.054, 1.055], mean observation: 0.043 [-5.602, 1.000], loss: 0.711461, mean_absolute_error: 0.731244, mean_q: 13.358443
 648704/1000000: episode: 2114, duration: 1.289s, episode steps: 171, steps per second: 133, episode reward: 28.851, mean reward: 0.169 [-10.294, 0.328], mean action: -0.472 [-1.057, 1.085], mean observation: -0.054 [-6.009, 1.000], loss: 0.719839, mean_absolute_error: 0.737098, mean_q: 13.306872
 648948/1000000: episode: 2115, duration: 1.827s, episode steps: 244, steps per second: 134, episode reward: -68.026, mean reward: -0.279 [-11.349, 0.223], mean action: -0.493 [-1.110, 1.044], mean observation: -0.176 [-7.922, 3.428], loss: 0.830214, mean_absolute_error: 0.743254, mean_q: 13.207305
 649149/1000000: episode: 2116, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: 7.775, mean reward: 0.039 [-10.795, 0.289], mean action: -0.500 [-1.145, 1.117], mean observation: -0.107 [-7.373, 1.000], loss: 0.677418, mean_absolute_error: 0.718739, mean_q: 13.290691
 649416/1000000: episode: 2117, duration: 2.004s, episode steps: 267, steps per second: 133, episode reward: -8.061, mean reward: -0.030 [-10.923, 0.585], mean action: -0.482 [-1.082, 1.094], mean observation: -0.184 [-7.906, 5.181], loss: 0.835221, mean_absolute_error: 0.744678, mean_q: 13.257404
 649680/1000000: episode: 2118, duration: 1.977s, episode steps: 264, steps per second: 134, episode reward: -34.809, mean reward: -0.132 [-11.007, 0.441], mean action: -0.530 [-1.114, 1.054], mean observation: -0.202 [-7.942, 5.049], loss: 0.719302, mean_absolute_error: 0.734765, mean_q: 13.306071
 650254/1000000: episode: 2119, duration: 4.268s, episode steps: 574, steps per second: 134, episode reward: 57.588, mean reward: 0.100 [-10.311, 0.246], mean action: -0.991 [-1.146, 0.665], mean observation: 0.045 [-1.501, 1.000], loss: 0.761449, mean_absolute_error: 0.742965, mean_q: 13.320253
 650912/1000000: episode: 2120, duration: 4.894s, episode steps: 658, steps per second: 134, episode reward: 56.737, mean reward: 0.086 [-10.420, 0.236], mean action: -1.003 [-1.113, -0.857], mean observation: 0.218 [-0.234, 1.531], loss: 0.807655, mean_absolute_error: 0.747115, mean_q: 13.323549
 651434/1000000: episode: 2121, duration: 3.891s, episode steps: 522, steps per second: 134, episode reward: 82.051, mean reward: 0.157 [-10.351, 0.305], mean action: -1.009 [-1.147, -0.856], mean observation: 0.050 [-1.982, 1.177], loss: 0.763803, mean_absolute_error: 0.734866, mean_q: 13.396621
 652201/1000000: episode: 2122, duration: 5.690s, episode steps: 767, steps per second: 135, episode reward: 100.514, mean reward: 0.131 [-10.450, 0.252], mean action: -1.004 [-1.117, -0.895], mean observation: 0.062 [-2.140, 1.348], loss: 0.780069, mean_absolute_error: 0.749343, mean_q: 13.373462
 652898/1000000: episode: 2123, duration: 5.176s, episode steps: 697, steps per second: 135, episode reward: 113.017, mean reward: 0.162 [-10.327, 0.291], mean action: -1.007 [-1.119, -0.888], mean observation: 0.050 [-1.610, 1.000], loss: 0.735597, mean_absolute_error: 0.747986, mean_q: 13.486880
 653593/1000000: episode: 2124, duration: 5.176s, episode steps: 695, steps per second: 134, episode reward: 245.837, mean reward: 0.354 [-10.349, 0.563], mean action: -0.997 [-1.146, -0.839], mean observation: 0.034 [-1.678, 1.000], loss: 0.772747, mean_absolute_error: 0.753941, mean_q: 13.505415
 654495/1000000: episode: 2125, duration: 6.695s, episode steps: 902, steps per second: 135, episode reward: 212.208, mean reward: 0.235 [-10.278, 0.429], mean action: -0.998 [-1.186, -0.847], mean observation: 0.073 [-1.079, 1.000], loss: 0.792885, mean_absolute_error: 0.757941, mean_q: 13.573946
 655068/1000000: episode: 2126, duration: 4.257s, episode steps: 573, steps per second: 135, episode reward: 68.927, mean reward: 0.120 [-10.590, 0.297], mean action: -1.010 [-1.131, -0.865], mean observation: 0.244 [-0.787, 2.169], loss: 0.800753, mean_absolute_error: 0.766494, mean_q: 13.684397
 655806/1000000: episode: 2127, duration: 5.503s, episode steps: 738, steps per second: 134, episode reward: 272.650, mean reward: 0.369 [-10.329, 0.536], mean action: -0.990 [-1.116, -0.838], mean observation: 0.055 [-2.192, 1.000], loss: 0.819504, mean_absolute_error: 0.769752, mean_q: 13.682430
 656795/1000000: episode: 2128, duration: 7.354s, episode steps: 989, steps per second: 134, episode reward: 390.231, mean reward: 0.395 [-10.251, 0.654], mean action: -1.002 [-1.173, -0.767], mean observation: 0.187 [-0.465, 1.522], loss: 0.807539, mean_absolute_error: 0.772997, mean_q: 13.755104
 657650/1000000: episode: 2129, duration: 6.370s, episode steps: 855, steps per second: 134, episode reward: -32.915, mean reward: -0.038 [-11.023, 0.106], mean action: -0.992 [-1.146, -0.861], mean observation: 0.003 [-2.963, 1.000], loss: 0.764870, mean_absolute_error: 0.762785, mean_q: 13.847713
 658345/1000000: episode: 2130, duration: 5.167s, episode steps: 695, steps per second: 135, episode reward: 147.147, mean reward: 0.212 [-10.287, 0.345], mean action: -0.997 [-1.131, -0.848], mean observation: 0.179 [-0.335, 1.872], loss: 0.843680, mean_absolute_error: 0.777802, mean_q: 13.806418
 659178/1000000: episode: 2131, duration: 6.177s, episode steps: 833, steps per second: 135, episode reward: 124.374, mean reward: 0.149 [-10.242, 0.237], mean action: -0.988 [-1.154, -0.861], mean observation: 0.192 [-0.439, 1.429], loss: 0.838719, mean_absolute_error: 0.776134, mean_q: 13.858374
 659799/1000000: episode: 2132, duration: 4.622s, episode steps: 621, steps per second: 134, episode reward: -5.258, mean reward: -0.008 [-10.363, 0.128], mean action: -0.981 [-1.150, -0.837], mean observation: 0.045 [-1.182, 1.000], loss: 0.823612, mean_absolute_error: 0.777723, mean_q: 13.952563
 660494/1000000: episode: 2133, duration: 5.389s, episode steps: 695, steps per second: 129, episode reward: 154.374, mean reward: 0.222 [-10.734, 0.450], mean action: -1.002 [-1.148, -0.852], mean observation: 0.122 [-1.778, 2.045], loss: 0.761932, mean_absolute_error: 0.769992, mean_q: 13.931573
 661316/1000000: episode: 2134, duration: 6.102s, episode steps: 822, steps per second: 135, episode reward: 242.498, mean reward: 0.295 [-10.267, 0.519], mean action: -1.003 [-1.149, -0.828], mean observation: 0.200 [-0.564, 2.016], loss: 0.820094, mean_absolute_error: 0.776264, mean_q: 13.987929
 662236/1000000: episode: 2135, duration: 6.951s, episode steps: 920, steps per second: 132, episode reward: 250.024, mean reward: 0.272 [-10.311, 0.415], mean action: -1.002 [-1.165, -0.859], mean observation: 0.179 [-0.648, 1.880], loss: 0.795110, mean_absolute_error: 0.778597, mean_q: 13.969396
 662871/1000000: episode: 2136, duration: 5.325s, episode steps: 635, steps per second: 119, episode reward: 50.500, mean reward: 0.080 [-10.612, 0.235], mean action: -1.001 [-1.169, -0.833], mean observation: 0.232 [-0.583, 2.384], loss: 0.777875, mean_absolute_error: 0.773299, mean_q: 13.990531
 663844/1000000: episode: 2137, duration: 7.709s, episode steps: 973, steps per second: 126, episode reward: -7.749, mean reward: -0.008 [-10.644, 0.323], mean action: -1.006 [-1.148, -0.844], mean observation: 0.136 [-0.993, 1.021], loss: 0.806279, mean_absolute_error: 0.782002, mean_q: 14.015313
 664476/1000000: episode: 2138, duration: 4.840s, episode steps: 632, steps per second: 131, episode reward: 128.702, mean reward: 0.204 [-10.560, 0.363], mean action: -0.991 [-1.133, -0.829], mean observation: 0.028 [-2.778, 1.000], loss: 0.809710, mean_absolute_error: 0.777488, mean_q: 14.102706
 665415/1000000: episode: 2139, duration: 8.000s, episode steps: 939, steps per second: 117, episode reward: 353.047, mean reward: 0.376 [-10.237, 0.543], mean action: -1.014 [-1.143, -0.830], mean observation: 0.095 [-1.241, 1.000], loss: 0.810263, mean_absolute_error: 0.773924, mean_q: 14.102804
 666049/1000000: episode: 2140, duration: 5.738s, episode steps: 634, steps per second: 110, episode reward: 28.350, mean reward: 0.045 [-10.379, 0.159], mean action: -1.005 [-1.153, -0.870], mean observation: 0.095 [-1.342, 1.487], loss: 0.798663, mean_absolute_error: 0.775494, mean_q: 14.076888
 666855/1000000: episode: 2141, duration: 6.701s, episode steps: 806, steps per second: 120, episode reward: 151.372, mean reward: 0.188 [-10.301, 0.318], mean action: -1.010 [-1.138, -0.860], mean observation: 0.071 [-1.577, 1.000], loss: 0.813275, mean_absolute_error: 0.777679, mean_q: 14.058552
 667563/1000000: episode: 2142, duration: 5.574s, episode steps: 708, steps per second: 127, episode reward: 147.883, mean reward: 0.209 [-10.950, 0.465], mean action: -1.005 [-1.159, -0.863], mean observation: 0.261 [-0.800, 2.606], loss: 0.784458, mean_absolute_error: 0.772191, mean_q: 14.128711
 668201/1000000: episode: 2143, duration: 5.008s, episode steps: 638, steps per second: 127, episode reward: 51.838, mean reward: 0.081 [-10.426, 0.221], mean action: -1.002 [-1.142, -0.873], mean observation: 0.108 [-1.601, 1.000], loss: 0.776718, mean_absolute_error: 0.775733, mean_q: 14.113388
 669011/1000000: episode: 2144, duration: 6.188s, episode steps: 810, steps per second: 131, episode reward: 94.647, mean reward: 0.117 [-10.476, 0.305], mean action: -1.001 [-1.113, -0.885], mean observation: 0.047 [-1.071, 1.000], loss: 0.723580, mean_absolute_error: 0.765138, mean_q: 14.162675
 669866/1000000: episode: 2145, duration: 6.549s, episode steps: 855, steps per second: 131, episode reward: 214.340, mean reward: 0.251 [-10.645, 0.505], mean action: -0.997 [-1.141, -0.866], mean observation: 0.035 [-1.858, 1.000], loss: 0.779475, mean_absolute_error: 0.772924, mean_q: 14.144405
 670489/1000000: episode: 2146, duration: 4.935s, episode steps: 623, steps per second: 126, episode reward: 61.023, mean reward: 0.098 [-10.284, 0.207], mean action: -0.983 [-1.089, -0.854], mean observation: 0.214 [-0.270, 1.502], loss: 0.809625, mean_absolute_error: 0.776318, mean_q: 14.151121
 670749/1000000: episode: 2147, duration: 2.001s, episode steps: 260, steps per second: 130, episode reward: 57.009, mean reward: 0.219 [-9.745, 0.259], mean action: -1.027 [-1.157, -0.887], mean observation: 0.113 [-0.173, 1.000], loss: 0.890773, mean_absolute_error: 0.794571, mean_q: 14.102535
 671329/1000000: episode: 2148, duration: 4.517s, episode steps: 580, steps per second: 128, episode reward: 122.150, mean reward: 0.211 [-9.716, 0.283], mean action: -0.997 [-1.138, -0.863], mean observation: 0.124 [-0.795, 1.000], loss: 0.754875, mean_absolute_error: 0.772466, mean_q: 14.162235
 672173/1000000: episode: 2149, duration: 6.451s, episode steps: 844, steps per second: 131, episode reward: 205.986, mean reward: 0.244 [-10.861, 0.544], mean action: -0.999 [-1.169, -0.819], mean observation: 0.126 [-2.351, 2.109], loss: 0.820838, mean_absolute_error: 0.780918, mean_q: 14.132167
 673141/1000000: episode: 2150, duration: 7.371s, episode steps: 968, steps per second: 131, episode reward: 167.124, mean reward: 0.173 [-10.611, 0.334], mean action: -1.011 [-1.145, -0.864], mean observation: 0.204 [-0.613, 2.156], loss: 0.781195, mean_absolute_error: 0.781037, mean_q: 14.122291
 673938/1000000: episode: 2151, duration: 6.072s, episode steps: 797, steps per second: 131, episode reward: 232.002, mean reward: 0.291 [-10.291, 0.434], mean action: -0.989 [-1.120, -0.838], mean observation: 0.205 [-0.359, 2.033], loss: 0.744134, mean_absolute_error: 0.768377, mean_q: 14.184759
 674637/1000000: episode: 2152, duration: 5.415s, episode steps: 699, steps per second: 129, episode reward: 30.085, mean reward: 0.043 [-10.004, 0.086], mean action: -0.999 [-1.142, -0.896], mean observation: 0.168 [-0.465, 1.809], loss: 0.814395, mean_absolute_error: 0.782371, mean_q: 14.195964
 675406/1000000: episode: 2153, duration: 5.900s, episode steps: 769, steps per second: 130, episode reward: 216.978, mean reward: 0.282 [-10.410, 0.443], mean action: -1.000 [-1.150, -0.874], mean observation: 0.214 [-0.447, 2.335], loss: 0.826910, mean_absolute_error: 0.781262, mean_q: 14.198422
 676117/1000000: episode: 2154, duration: 5.453s, episode steps: 711, steps per second: 130, episode reward: 188.290, mean reward: 0.265 [-10.535, 0.411], mean action: -1.003 [-1.129, -0.832], mean observation: 0.173 [-1.016, 2.515], loss: 0.839649, mean_absolute_error: 0.784970, mean_q: 14.172484
 676672/1000000: episode: 2155, duration: 4.255s, episode steps: 555, steps per second: 130, episode reward: 38.922, mean reward: 0.070 [-10.135, 0.125], mean action: -0.985 [-1.114, -0.829], mean observation: 0.084 [-1.658, 1.000], loss: 0.759400, mean_absolute_error: 0.773227, mean_q: 14.239366
 677170/1000000: episode: 2156, duration: 3.752s, episode steps: 498, steps per second: 133, episode reward: 8.614, mean reward: 0.017 [-10.251, 0.102], mean action: -1.010 [-1.143, -0.866], mean observation: 0.110 [-0.941, 1.693], loss: 0.764624, mean_absolute_error: 0.772274, mean_q: 14.165243
 677803/1000000: episode: 2157, duration: 4.867s, episode steps: 633, steps per second: 130, episode reward: 27.852, mean reward: 0.044 [-10.595, 0.254], mean action: -0.994 [-1.110, -0.887], mean observation: 0.126 [-1.378, 1.300], loss: 0.754887, mean_absolute_error: 0.769662, mean_q: 14.183361
 678599/1000000: episode: 2158, duration: 5.970s, episode steps: 796, steps per second: 133, episode reward: 157.482, mean reward: 0.198 [-10.400, 0.316], mean action: -0.996 [-1.157, -0.870], mean observation: 0.041 [-1.824, 1.000], loss: 0.791959, mean_absolute_error: 0.776570, mean_q: 14.183392
 679264/1000000: episode: 2159, duration: 4.977s, episode steps: 665, steps per second: 134, episode reward: 151.447, mean reward: 0.228 [-10.469, 0.360], mean action: -0.998 [-1.173, -0.848], mean observation: 0.229 [-0.603, 2.086], loss: 0.816883, mean_absolute_error: 0.784469, mean_q: 14.174712
 679447/1000000: episode: 2160, duration: 1.439s, episode steps: 183, steps per second: 127, episode reward: 25.198, mean reward: 0.138 [-9.811, 0.194], mean action: -0.986 [-1.094, -0.872], mean observation: 0.122 [-0.291, 1.000], loss: 0.638817, mean_absolute_error: 0.762148, mean_q: 14.135337
 680409/1000000: episode: 2161, duration: 7.371s, episode steps: 962, steps per second: 131, episode reward: 189.083, mean reward: 0.197 [-10.650, 0.474], mean action: -0.993 [-1.122, -0.863], mean observation: 0.118 [-1.696, 1.000], loss: 0.815572, mean_absolute_error: 0.788221, mean_q: 14.216206
 680983/1000000: episode: 2162, duration: 4.508s, episode steps: 574, steps per second: 127, episode reward: 102.245, mean reward: 0.178 [-10.346, 0.340], mean action: -0.986 [-1.101, -0.854], mean observation: 0.026 [-1.602, 1.000], loss: 0.781954, mean_absolute_error: 0.777506, mean_q: 14.162717
 681555/1000000: episode: 2163, duration: 4.339s, episode steps: 572, steps per second: 132, episode reward: 68.293, mean reward: 0.119 [-10.490, 0.282], mean action: -1.000 [-1.123, -0.866], mean observation: 0.245 [-0.489, 1.852], loss: 0.730227, mean_absolute_error: 0.774792, mean_q: 14.219193
 682286/1000000: episode: 2164, duration: 6.071s, episode steps: 731, steps per second: 120, episode reward: 61.041, mean reward: 0.084 [-10.967, 0.310], mean action: -0.991 [-1.137, -0.835], mean observation: 0.251 [-0.611, 2.633], loss: 0.774990, mean_absolute_error: 0.782424, mean_q: 14.180975
 682998/1000000: episode: 2165, duration: 6.003s, episode steps: 712, steps per second: 119, episode reward: 280.177, mean reward: 0.394 [-10.288, 0.592], mean action: -0.988 [-1.121, -0.868], mean observation: 0.051 [-2.136, 1.000], loss: 0.794476, mean_absolute_error: 0.780651, mean_q: 14.210283
 684195/1000000: episode: 2166, duration: 9.515s, episode steps: 1197, steps per second: 126, episode reward: 170.448, mean reward: 0.142 [-10.698, 0.243], mean action: -0.994 [-1.133, -0.839], mean observation: 0.056 [-2.231, 1.000], loss: 0.771201, mean_absolute_error: 0.776520, mean_q: 14.189671
 684917/1000000: episode: 2167, duration: 5.560s, episode steps: 722, steps per second: 130, episode reward: 106.003, mean reward: 0.147 [-10.283, 0.276], mean action: -1.006 [-1.176, -0.874], mean observation: 0.107 [-1.112, 1.013], loss: 0.768613, mean_absolute_error: 0.780534, mean_q: 14.172571
 685265/1000000: episode: 2168, duration: 2.803s, episode steps: 348, steps per second: 124, episode reward: 33.122, mean reward: 0.095 [-9.827, 0.172], mean action: -0.989 [-1.111, -0.835], mean observation: 0.142 [-0.449, 1.145], loss: 0.781988, mean_absolute_error: 0.787219, mean_q: 14.259784
 686216/1000000: episode: 2169, duration: 8.277s, episode steps: 951, steps per second: 115, episode reward: 216.014, mean reward: 0.227 [-10.556, 0.670], mean action: -1.007 [-1.204, -0.874], mean observation: 0.048 [-2.015, 1.000], loss: 0.732015, mean_absolute_error: 0.776327, mean_q: 14.236607
 686553/1000000: episode: 2170, duration: 2.976s, episode steps: 337, steps per second: 113, episode reward: 72.127, mean reward: 0.214 [-9.734, 0.266], mean action: -1.007 [-1.134, -0.892], mean observation: 0.123 [-0.901, 1.000], loss: 0.805439, mean_absolute_error: 0.785712, mean_q: 14.252678
 687289/1000000: episode: 2171, duration: 5.974s, episode steps: 736, steps per second: 123, episode reward: 177.550, mean reward: 0.241 [-10.647, 0.405], mean action: -0.994 [-1.115, -0.830], mean observation: 0.119 [-1.946, 2.022], loss: 0.750434, mean_absolute_error: 0.778093, mean_q: 14.204033
 687815/1000000: episode: 2172, duration: 4.084s, episode steps: 526, steps per second: 129, episode reward: 10.877, mean reward: 0.021 [-10.212, 0.097], mean action: -0.997 [-1.121, -0.817], mean observation: 0.194 [-0.301, 1.370], loss: 0.773402, mean_absolute_error: 0.773083, mean_q: 14.245000
 688576/1000000: episode: 2173, duration: 5.896s, episode steps: 761, steps per second: 129, episode reward: 112.964, mean reward: 0.148 [-10.659, 0.328], mean action: -0.987 [-1.139, -0.840], mean observation: 0.032 [-2.854, 1.203], loss: 0.758945, mean_absolute_error: 0.781684, mean_q: 14.202568
 689288/1000000: episode: 2174, duration: 5.322s, episode steps: 712, steps per second: 134, episode reward: 161.757, mean reward: 0.227 [-10.612, 0.484], mean action: -1.000 [-1.185, -0.847], mean observation: 0.121 [-1.243, 2.002], loss: 0.748478, mean_absolute_error: 0.778306, mean_q: 14.265858
 690105/1000000: episode: 2175, duration: 6.108s, episode steps: 817, steps per second: 134, episode reward: 187.215, mean reward: 0.229 [-10.375, 0.401], mean action: -1.004 [-1.175, -0.846], mean observation: 0.076 [-2.271, 1.000], loss: 0.737234, mean_absolute_error: 0.774404, mean_q: 14.246319
 691527/1000000: episode: 2176, duration: 10.806s, episode steps: 1422, steps per second: 132, episode reward: 42.185, mean reward: 0.030 [-10.380, 0.302], mean action: -0.997 [-1.177, -0.861], mean observation: 0.121 [-1.890, 1.000], loss: 0.769055, mean_absolute_error: 0.784539, mean_q: 14.273998
 692223/1000000: episode: 2177, duration: 5.237s, episode steps: 696, steps per second: 133, episode reward: 176.400, mean reward: 0.253 [-10.550, 0.466], mean action: -1.001 [-1.167, -0.871], mean observation: 0.014 [-1.892, 1.000], loss: 0.803271, mean_absolute_error: 0.784390, mean_q: 14.301008
 693070/1000000: episode: 2178, duration: 6.389s, episode steps: 847, steps per second: 133, episode reward: 163.370, mean reward: 0.193 [-10.829, 0.472], mean action: -0.988 [-1.102, -0.867], mean observation: 0.099 [-2.737, 1.578], loss: 0.787009, mean_absolute_error: 0.788278, mean_q: 14.275364
 693863/1000000: episode: 2179, duration: 5.958s, episode steps: 793, steps per second: 133, episode reward: 333.799, mean reward: 0.421 [-10.367, 0.720], mean action: -0.999 [-1.173, -0.886], mean observation: 0.223 [-0.339, 1.978], loss: 0.792304, mean_absolute_error: 0.795874, mean_q: 14.295485
 694624/1000000: episode: 2180, duration: 5.787s, episode steps: 761, steps per second: 131, episode reward: 211.434, mean reward: 0.278 [-10.375, 0.404], mean action: -1.009 [-1.151, -0.872], mean observation: 0.042 [-2.018, 1.000], loss: 0.782698, mean_absolute_error: 0.784607, mean_q: 14.286034
 695463/1000000: episode: 2181, duration: 6.392s, episode steps: 839, steps per second: 131, episode reward: 156.914, mean reward: 0.187 [-10.429, 0.345], mean action: -0.991 [-1.152, -0.845], mean observation: 0.042 [-1.360, 1.000], loss: 0.786042, mean_absolute_error: 0.796213, mean_q: 14.286449
 696155/1000000: episode: 2182, duration: 5.380s, episode steps: 692, steps per second: 129, episode reward: 37.653, mean reward: 0.054 [-10.741, 0.193], mean action: -0.994 [-1.186, -0.842], mean observation: 0.237 [-0.924, 2.005], loss: 0.774202, mean_absolute_error: 0.798466, mean_q: 14.291907
 696793/1000000: episode: 2183, duration: 4.823s, episode steps: 638, steps per second: 132, episode reward: 110.450, mean reward: 0.173 [-10.278, 0.316], mean action: -0.996 [-1.160, -0.847], mean observation: 0.215 [-0.205, 1.502], loss: 0.782946, mean_absolute_error: 0.807862, mean_q: 14.265488
 697518/1000000: episode: 2184, duration: 5.477s, episode steps: 725, steps per second: 132, episode reward: -9.321, mean reward: -0.013 [-10.689, 0.119], mean action: -1.001 [-1.143, -0.864], mean observation: 0.002 [-2.426, 1.000], loss: 0.811941, mean_absolute_error: 0.804945, mean_q: 14.317037
 698100/1000000: episode: 2185, duration: 4.465s, episode steps: 582, steps per second: 130, episode reward: 104.236, mean reward: 0.179 [-10.479, 0.364], mean action: -0.993 [-1.136, -0.831], mean observation: 0.077 [-2.017, 1.600], loss: 0.738215, mean_absolute_error: 0.802803, mean_q: 14.307220
 698765/1000000: episode: 2186, duration: 5.345s, episode steps: 665, steps per second: 124, episode reward: 206.852, mean reward: 0.311 [-10.289, 0.511], mean action: -0.997 [-1.130, -0.847], mean observation: 0.050 [-1.650, 1.000], loss: 0.785408, mean_absolute_error: 0.808751, mean_q: 14.278780
 699372/1000000: episode: 2187, duration: 4.683s, episode steps: 607, steps per second: 130, episode reward: 137.103, mean reward: 0.226 [-10.155, 0.318], mean action: -0.999 [-1.197, -0.884], mean observation: 0.018 [-1.822, 1.000], loss: 0.765812, mean_absolute_error: 0.804469, mean_q: 14.336472
 700057/1000000: episode: 2188, duration: 5.224s, episode steps: 685, steps per second: 131, episode reward: 271.783, mean reward: 0.397 [-10.429, 0.610], mean action: -1.008 [-1.133, -0.850], mean observation: 0.093 [-1.957, 1.531], loss: 0.792523, mean_absolute_error: 0.809239, mean_q: 14.348497
 701049/1000000: episode: 2189, duration: 7.731s, episode steps: 992, steps per second: 128, episode reward: 188.794, mean reward: 0.190 [-10.613, 0.344], mean action: -0.999 [-1.178, -0.868], mean observation: 0.114 [-1.736, 1.798], loss: 0.800878, mean_absolute_error: 0.811903, mean_q: 14.351890
 702105/1000000: episode: 2190, duration: 8.187s, episode steps: 1056, steps per second: 129, episode reward: 193.868, mean reward: 0.184 [-10.623, 0.354], mean action: -1.001 [-1.187, -0.873], mean observation: 0.045 [-1.593, 1.000], loss: 0.774031, mean_absolute_error: 0.816409, mean_q: 14.387374
 703122/1000000: episode: 2191, duration: 7.796s, episode steps: 1017, steps per second: 130, episode reward: 145.112, mean reward: 0.143 [-10.615, 0.334], mean action: -0.993 [-1.167, -0.865], mean observation: 0.203 [-0.426, 2.004], loss: 0.748125, mean_absolute_error: 0.809983, mean_q: 14.430003
 703899/1000000: episode: 2192, duration: 5.950s, episode steps: 777, steps per second: 131, episode reward: 24.643, mean reward: 0.032 [-10.406, 0.114], mean action: -1.002 [-1.168, -0.868], mean observation: 0.083 [-1.613, 1.614], loss: 0.793561, mean_absolute_error: 0.820692, mean_q: 14.374188
 704726/1000000: episode: 2193, duration: 6.355s, episode steps: 827, steps per second: 130, episode reward: 314.788, mean reward: 0.381 [-10.357, 0.566], mean action: -0.998 [-1.112, -0.818], mean observation: 0.047 [-1.857, 1.000], loss: 0.800173, mean_absolute_error: 0.822885, mean_q: 14.419618
 705519/1000000: episode: 2194, duration: 5.977s, episode steps: 793, steps per second: 133, episode reward: 265.831, mean reward: 0.335 [-10.618, 0.592], mean action: -0.998 [-1.166, -0.849], mean observation: 0.019 [-2.332, 1.000], loss: 0.743033, mean_absolute_error: 0.815934, mean_q: 14.382110
 706213/1000000: episode: 2195, duration: 5.273s, episode steps: 694, steps per second: 132, episode reward: 173.407, mean reward: 0.250 [-10.374, 0.387], mean action: -1.016 [-1.179, -0.899], mean observation: 0.222 [-0.432, 2.261], loss: 0.815025, mean_absolute_error: 0.827013, mean_q: 14.380217
 706861/1000000: episode: 2196, duration: 4.907s, episode steps: 648, steps per second: 132, episode reward: 149.794, mean reward: 0.231 [-10.728, 0.420], mean action: -0.996 [-1.131, -0.812], mean observation: 0.100 [-2.121, 2.227], loss: 0.770086, mean_absolute_error: 0.825207, mean_q: 14.348817
 707738/1000000: episode: 2197, duration: 6.699s, episode steps: 877, steps per second: 131, episode reward: 153.240, mean reward: 0.175 [-10.515, 0.365], mean action: -0.994 [-1.174, -0.819], mean observation: 0.040 [-1.909, 1.000], loss: 0.825607, mean_absolute_error: 0.832937, mean_q: 14.390260
 708434/1000000: episode: 2198, duration: 5.278s, episode steps: 696, steps per second: 132, episode reward: 179.321, mean reward: 0.258 [-10.772, 0.527], mean action: -0.983 [-1.150, -0.847], mean observation: 0.122 [-2.254, 1.606], loss: 0.807185, mean_absolute_error: 0.830677, mean_q: 14.421527
 709063/1000000: episode: 2199, duration: 4.696s, episode steps: 629, steps per second: 134, episode reward: 33.294, mean reward: 0.053 [-10.556, 0.210], mean action: -0.991 [-1.100, -0.849], mean observation: 0.100 [-1.993, 1.000], loss: 0.763408, mean_absolute_error: 0.829541, mean_q: 14.348088
 709626/1000000: episode: 2200, duration: 4.255s, episode steps: 563, steps per second: 132, episode reward: -7.281, mean reward: -0.013 [-10.495, 0.099], mean action: -0.994 [-1.144, -0.848], mean observation: 0.029 [-2.162, 1.000], loss: 0.785167, mean_absolute_error: 0.835399, mean_q: 14.371780
 710213/1000000: episode: 2201, duration: 4.402s, episode steps: 587, steps per second: 133, episode reward: 76.397, mean reward: 0.130 [-10.274, 0.267], mean action: -1.006 [-1.154, -0.866], mean observation: 0.060 [-1.486, 1.000], loss: 0.856618, mean_absolute_error: 0.839026, mean_q: 14.375829
 710861/1000000: episode: 2202, duration: 4.893s, episode steps: 648, steps per second: 132, episode reward: 112.814, mean reward: 0.174 [-10.368, 0.292], mean action: -1.003 [-1.129, -0.836], mean observation: 0.076 [-1.961, 1.017], loss: 0.857888, mean_absolute_error: 0.844916, mean_q: 14.380704
 711683/1000000: episode: 2203, duration: 6.234s, episode steps: 822, steps per second: 132, episode reward: 203.355, mean reward: 0.247 [-10.492, 0.404], mean action: -1.006 [-1.169, -0.851], mean observation: 0.046 [-2.337, 1.000], loss: 0.806772, mean_absolute_error: 0.838936, mean_q: 14.390432
 712433/1000000: episode: 2204, duration: 5.719s, episode steps: 750, steps per second: 131, episode reward: 175.113, mean reward: 0.233 [-10.776, 0.468], mean action: -0.991 [-1.173, -0.843], mean observation: 0.244 [-0.517, 2.609], loss: 0.779505, mean_absolute_error: 0.838843, mean_q: 14.436441
 713274/1000000: episode: 2205, duration: 6.423s, episode steps: 841, steps per second: 131, episode reward: 180.476, mean reward: 0.215 [-10.566, 0.377], mean action: -1.012 [-1.158, -0.844], mean observation: 0.037 [-1.824, 1.000], loss: 0.820493, mean_absolute_error: 0.835826, mean_q: 14.474530
 713895/1000000: episode: 2206, duration: 4.800s, episode steps: 621, steps per second: 129, episode reward: 165.623, mean reward: 0.267 [-10.732, 0.496], mean action: -0.995 [-1.118, -0.849], mean observation: 0.145 [-1.609, 2.486], loss: 0.759352, mean_absolute_error: 0.828842, mean_q: 14.520250
 714667/1000000: episode: 2207, duration: 5.940s, episode steps: 772, steps per second: 130, episode reward: 81.328, mean reward: 0.105 [-10.560, 0.266], mean action: -0.988 [-1.140, -0.833], mean observation: 0.023 [-2.134, 1.000], loss: 0.787832, mean_absolute_error: 0.837282, mean_q: 14.450287
 715463/1000000: episode: 2208, duration: 6.130s, episode steps: 796, steps per second: 130, episode reward: 219.036, mean reward: 0.275 [-10.463, 0.429], mean action: -0.995 [-1.158, -0.865], mean observation: 0.208 [-0.333, 2.141], loss: 0.844885, mean_absolute_error: 0.846795, mean_q: 14.426581
 716092/1000000: episode: 2209, duration: 4.818s, episode steps: 629, steps per second: 131, episode reward: 175.666, mean reward: 0.279 [-10.527, 0.427], mean action: -1.003 [-1.136, -0.851], mean observation: 0.101 [-2.281, 1.768], loss: 0.836197, mean_absolute_error: 0.851076, mean_q: 14.378308
 717075/1000000: episode: 2210, duration: 7.411s, episode steps: 983, steps per second: 133, episode reward: 329.108, mean reward: 0.335 [-10.645, 0.557], mean action: -0.999 [-1.232, -0.852], mean observation: 0.046 [-2.027, 1.000], loss: 0.829693, mean_absolute_error: 0.846568, mean_q: 14.452207
 717718/1000000: episode: 2211, duration: 4.913s, episode steps: 643, steps per second: 131, episode reward: 8.808, mean reward: 0.014 [-10.442, 0.149], mean action: -0.991 [-1.146, -0.878], mean observation: 0.168 [-0.629, 1.627], loss: 0.837320, mean_absolute_error: 0.851772, mean_q: 14.401606
 718475/1000000: episode: 2212, duration: 5.780s, episode steps: 757, steps per second: 131, episode reward: 273.390, mean reward: 0.361 [-10.565, 0.629], mean action: -0.999 [-1.149, -0.871], mean observation: 0.245 [-0.372, 2.297], loss: 0.828057, mean_absolute_error: 0.851680, mean_q: 14.475312
 719253/1000000: episode: 2213, duration: 5.912s, episode steps: 778, steps per second: 132, episode reward: 192.376, mean reward: 0.247 [-10.733, 0.494], mean action: -1.007 [-1.138, -0.855], mean observation: 0.009 [-1.961, 1.000], loss: 0.868003, mean_absolute_error: 0.854896, mean_q: 14.475724
 720003/1000000: episode: 2214, duration: 5.674s, episode steps: 750, steps per second: 132, episode reward: -8.983, mean reward: -0.012 [-10.510, 0.105], mean action: -0.998 [-1.136, -0.865], mean observation: 0.146 [-1.227, 1.415], loss: 0.827085, mean_absolute_error: 0.851153, mean_q: 14.475494
 720748/1000000: episode: 2215, duration: 5.673s, episode steps: 745, steps per second: 131, episode reward: 156.713, mean reward: 0.210 [-10.432, 0.376], mean action: -1.010 [-1.211, -0.877], mean observation: 0.219 [-0.706, 2.082], loss: 0.837670, mean_absolute_error: 0.855118, mean_q: 14.510556
 721389/1000000: episode: 2216, duration: 5.003s, episode steps: 641, steps per second: 128, episode reward: 100.166, mean reward: 0.156 [-10.430, 0.343], mean action: -1.004 [-1.119, -0.851], mean observation: 0.017 [-1.589, 1.000], loss: 0.841913, mean_absolute_error: 0.854134, mean_q: 14.444621
 722499/1000000: episode: 2217, duration: 8.313s, episode steps: 1110, steps per second: 134, episode reward: 185.546, mean reward: 0.167 [-10.333, 0.316], mean action: -0.986 [-1.122, -0.849], mean observation: 0.163 [-0.571, 1.467], loss: 0.850668, mean_absolute_error: 0.863589, mean_q: 14.436636
 723488/1000000: episode: 2218, duration: 7.647s, episode steps: 989, steps per second: 129, episode reward: 207.758, mean reward: 0.210 [-10.621, 0.499], mean action: -0.997 [-1.120, -0.853], mean observation: 0.114 [-1.817, 1.000], loss: 0.842866, mean_absolute_error: 0.866837, mean_q: 14.455607
 724716/1000000: episode: 2219, duration: 9.388s, episode steps: 1228, steps per second: 131, episode reward: 494.334, mean reward: 0.403 [-10.471, 0.573], mean action: -0.995 [-1.157, -0.847], mean observation: 0.118 [-1.453, 1.068], loss: 0.852713, mean_absolute_error: 0.868425, mean_q: 14.525178
 725326/1000000: episode: 2220, duration: 4.589s, episode steps: 610, steps per second: 133, episode reward: 150.612, mean reward: 0.247 [-10.477, 0.417], mean action: -0.985 [-1.118, -0.857], mean observation: 0.005 [-1.980, 1.000], loss: 0.833373, mean_absolute_error: 0.859424, mean_q: 14.519319
 726097/1000000: episode: 2221, duration: 5.855s, episode steps: 771, steps per second: 132, episode reward: 148.255, mean reward: 0.192 [-10.463, 0.360], mean action: -0.996 [-1.135, -0.856], mean observation: 0.210 [-0.393, 2.325], loss: 0.838389, mean_absolute_error: 0.864734, mean_q: 14.580362
 726619/1000000: episode: 2222, duration: 3.980s, episode steps: 522, steps per second: 131, episode reward: -9.601, mean reward: -0.018 [-10.030, 0.008], mean action: -1.002 [-1.143, -0.867], mean observation: 0.165 [-0.392, 1.000], loss: 0.825239, mean_absolute_error: 0.865351, mean_q: 14.540889
 727502/1000000: episode: 2223, duration: 6.937s, episode steps: 883, steps per second: 127, episode reward: 297.458, mean reward: 0.337 [-10.265, 0.509], mean action: -1.002 [-1.147, -0.891], mean observation: 0.169 [-1.038, 1.344], loss: 0.885065, mean_absolute_error: 0.870851, mean_q: 14.574759
 728022/1000000: episode: 2224, duration: 4.045s, episode steps: 520, steps per second: 129, episode reward: -5.212, mean reward: -0.010 [-10.663, 0.182], mean action: -1.001 [-1.135, -0.865], mean observation: 0.260 [-0.555, 1.894], loss: 0.873728, mean_absolute_error: 0.872297, mean_q: 14.560231
 728737/1000000: episode: 2225, duration: 5.472s, episode steps: 715, steps per second: 131, episode reward: 164.633, mean reward: 0.230 [-10.554, 0.390], mean action: -0.997 [-1.127, -0.842], mean observation: 0.033 [-2.462, 1.000], loss: 0.878624, mean_absolute_error: 0.867702, mean_q: 14.584395
 729485/1000000: episode: 2226, duration: 5.687s, episode steps: 748, steps per second: 132, episode reward: 105.546, mean reward: 0.141 [-10.443, 0.324], mean action: -1.000 [-1.176, -0.841], mean observation: 0.130 [-1.096, 1.451], loss: 0.843539, mean_absolute_error: 0.869273, mean_q: 14.655481
 730448/1000000: episode: 2227, duration: 7.641s, episode steps: 963, steps per second: 126, episode reward: 283.031, mean reward: 0.294 [-10.601, 0.536], mean action: -1.004 [-1.131, -0.857], mean observation: 0.125 [-1.514, 1.221], loss: 0.886227, mean_absolute_error: 0.871722, mean_q: 14.607562
 731694/1000000: episode: 2228, duration: 9.423s, episode steps: 1246, steps per second: 132, episode reward: 122.080, mean reward: 0.098 [-10.105, 0.225], mean action: -0.998 [-1.138, -0.847], mean observation: 0.167 [-0.580, 1.000], loss: 0.840622, mean_absolute_error: 0.866365, mean_q: 14.645259
 732471/1000000: episode: 2229, duration: 5.923s, episode steps: 777, steps per second: 131, episode reward: -4.921, mean reward: -0.006 [-10.336, 0.133], mean action: -1.004 [-1.134, -0.877], mean observation: 0.159 [-1.098, 1.000], loss: 0.852250, mean_absolute_error: 0.870927, mean_q: 14.602676
 733258/1000000: episode: 2230, duration: 6.018s, episode steps: 787, steps per second: 131, episode reward: -16.355, mean reward: -0.021 [-10.512, 0.168], mean action: -0.992 [-1.113, -0.735], mean observation: 0.216 [-0.353, 1.048], loss: 0.822891, mean_absolute_error: 0.867397, mean_q: 14.600282
 733892/1000000: episode: 2231, duration: 4.853s, episode steps: 634, steps per second: 131, episode reward: 129.319, mean reward: 0.204 [-10.835, 0.426], mean action: -0.995 [-1.145, -0.812], mean observation: 0.229 [-1.149, 3.119], loss: 0.922442, mean_absolute_error: 0.879318, mean_q: 14.608431
 734537/1000000: episode: 2232, duration: 4.927s, episode steps: 645, steps per second: 131, episode reward: 135.336, mean reward: 0.210 [-10.396, 0.359], mean action: -1.011 [-1.165, -0.885], mean observation: 0.093 [-1.795, 1.427], loss: 0.861346, mean_absolute_error: 0.875338, mean_q: 14.602843
 735392/1000000: episode: 2233, duration: 6.490s, episode steps: 855, steps per second: 132, episode reward: 298.482, mean reward: 0.349 [-10.316, 0.478], mean action: -0.990 [-1.105, -0.842], mean observation: 0.068 [-2.096, 1.000], loss: 0.859183, mean_absolute_error: 0.873821, mean_q: 14.622265
 736310/1000000: episode: 2234, duration: 6.999s, episode steps: 918, steps per second: 131, episode reward: 363.284, mean reward: 0.396 [-10.374, 0.661], mean action: -0.982 [-1.143, -0.835], mean observation: 0.091 [-2.349, 1.000], loss: 0.868648, mean_absolute_error: 0.871281, mean_q: 14.681361
 737170/1000000: episode: 2235, duration: 6.592s, episode steps: 860, steps per second: 130, episode reward: 182.651, mean reward: 0.212 [-10.399, 0.353], mean action: -1.008 [-1.154, -0.880], mean observation: 0.155 [-0.898, 1.660], loss: 0.867252, mean_absolute_error: 0.867152, mean_q: 14.661865
 737903/1000000: episode: 2236, duration: 5.681s, episode steps: 733, steps per second: 129, episode reward: 194.954, mean reward: 0.266 [-10.482, 0.434], mean action: -0.993 [-1.130, -0.831], mean observation: 0.242 [-0.308, 1.733], loss: 0.848156, mean_absolute_error: 0.875869, mean_q: 14.711732
 739201/1000000: episode: 2237, duration: 9.840s, episode steps: 1298, steps per second: 132, episode reward: 126.093, mean reward: 0.097 [-10.155, 0.307], mean action: -1.005 [-1.213, -0.865], mean observation: 0.160 [-1.049, 2.139], loss: 0.824589, mean_absolute_error: 0.871165, mean_q: 14.679404
 739766/1000000: episode: 2238, duration: 4.326s, episode steps: 565, steps per second: 131, episode reward: 188.680, mean reward: 0.334 [-10.449, 0.564], mean action: -1.010 [-1.195, -0.869], mean observation: 0.253 [-0.535, 2.243], loss: 0.877368, mean_absolute_error: 0.873779, mean_q: 14.726165
 740456/1000000: episode: 2239, duration: 5.287s, episode steps: 690, steps per second: 131, episode reward: 156.161, mean reward: 0.226 [-10.357, 0.351], mean action: -1.003 [-1.141, -0.855], mean observation: 0.206 [-0.394, 2.216], loss: 0.930731, mean_absolute_error: 0.877447, mean_q: 14.663448
 741406/1000000: episode: 2240, duration: 7.330s, episode steps: 950, steps per second: 130, episode reward: 274.578, mean reward: 0.289 [-10.559, 0.484], mean action: -1.010 [-1.147, -0.883], mean observation: 0.148 [-1.434, 2.031], loss: 0.866983, mean_absolute_error: 0.873385, mean_q: 14.804232
 741937/1000000: episode: 2241, duration: 4.143s, episode steps: 531, steps per second: 128, episode reward: 97.311, mean reward: 0.183 [-9.924, 0.233], mean action: -0.986 [-1.133, -0.799], mean observation: 0.142 [-0.335, 1.000], loss: 0.893476, mean_absolute_error: 0.874728, mean_q: 14.752481
 742417/1000000: episode: 2242, duration: 3.629s, episode steps: 480, steps per second: 132, episode reward: 83.303, mean reward: 0.174 [-9.835, 0.199], mean action: -1.006 [-1.130, -0.882], mean observation: 0.096 [-0.801, 1.000], loss: 0.950397, mean_absolute_error: 0.889278, mean_q: 14.804783
 742886/1000000: episode: 2243, duration: 3.586s, episode steps: 469, steps per second: 131, episode reward: 54.200, mean reward: 0.116 [-9.937, 0.149], mean action: -0.997 [-1.164, -0.876], mean observation: 0.160 [-0.392, 1.451], loss: 0.793216, mean_absolute_error: 0.859513, mean_q: 14.758210
 743669/1000000: episode: 2244, duration: 6.120s, episode steps: 783, steps per second: 128, episode reward: 185.606, mean reward: 0.237 [-10.422, 0.389], mean action: -0.996 [-1.122, -0.854], mean observation: 0.112 [-1.393, 1.444], loss: 0.888022, mean_absolute_error: 0.874502, mean_q: 14.762303
 744191/1000000: episode: 2245, duration: 4.053s, episode steps: 522, steps per second: 129, episode reward: 44.468, mean reward: 0.085 [-9.856, 0.144], mean action: -1.006 [-1.145, -0.878], mean observation: 0.136 [-1.364, 1.000], loss: 0.795982, mean_absolute_error: 0.859085, mean_q: 14.788505
 744810/1000000: episode: 2246, duration: 4.979s, episode steps: 619, steps per second: 124, episode reward: 41.079, mean reward: 0.066 [-10.374, 0.213], mean action: -0.996 [-1.124, -0.831], mean observation: 0.063 [-1.646, 1.000], loss: 0.907468, mean_absolute_error: 0.878220, mean_q: 14.668938
 745541/1000000: episode: 2247, duration: 6.316s, episode steps: 731, steps per second: 116, episode reward: 130.481, mean reward: 0.178 [-10.486, 0.312], mean action: -1.004 [-1.140, -0.838], mean observation: 0.220 [-0.494, 1.919], loss: 0.842689, mean_absolute_error: 0.870440, mean_q: 14.808674
 746357/1000000: episode: 2248, duration: 6.344s, episode steps: 816, steps per second: 129, episode reward: 387.845, mean reward: 0.475 [-10.491, 0.728], mean action: -1.014 [-1.165, -0.857], mean observation: 0.115 [-2.017, 1.127], loss: 0.871179, mean_absolute_error: 0.868606, mean_q: 14.805597
 746993/1000000: episode: 2249, duration: 4.906s, episode steps: 636, steps per second: 130, episode reward: 93.749, mean reward: 0.147 [-9.871, 0.168], mean action: -0.990 [-1.123, -0.870], mean observation: 0.153 [-1.001, 1.000], loss: 0.899538, mean_absolute_error: 0.875349, mean_q: 14.801647
 747738/1000000: episode: 2250, duration: 5.764s, episode steps: 745, steps per second: 129, episode reward: 138.854, mean reward: 0.186 [-10.701, 0.393], mean action: -1.008 [-1.183, -0.861], mean observation: 0.158 [-0.861, 2.675], loss: 0.846336, mean_absolute_error: 0.867114, mean_q: 14.819178
 748768/1000000: episode: 2251, duration: 7.740s, episode steps: 1030, steps per second: 133, episode reward: 173.369, mean reward: 0.168 [-10.567, 0.356], mean action: -1.002 [-1.152, -0.836], mean observation: 0.122 [-2.290, 1.251], loss: 0.843180, mean_absolute_error: 0.867062, mean_q: 14.785458
 749630/1000000: episode: 2252, duration: 6.615s, episode steps: 862, steps per second: 130, episode reward: 52.197, mean reward: 0.061 [-10.333, 0.153], mean action: -1.024 [-1.182, -0.905], mean observation: 0.065 [-1.492, 1.000], loss: 0.903723, mean_absolute_error: 0.874614, mean_q: 14.811571
 750341/1000000: episode: 2253, duration: 5.588s, episode steps: 711, steps per second: 127, episode reward: 82.052, mean reward: 0.115 [-10.454, 0.303], mean action: -1.008 [-1.153, -0.889], mean observation: 0.094 [-1.861, 1.000], loss: 0.902717, mean_absolute_error: 0.874470, mean_q: 14.775110
 751051/1000000: episode: 2254, duration: 5.407s, episode steps: 710, steps per second: 131, episode reward: 167.726, mean reward: 0.236 [-10.397, 0.404], mean action: -1.033 [-1.164, -0.912], mean observation: 0.150 [-1.537, 1.360], loss: 0.841873, mean_absolute_error: 0.863754, mean_q: 14.868629
 751966/1000000: episode: 2255, duration: 7.120s, episode steps: 915, steps per second: 129, episode reward: 6.925, mean reward: 0.008 [-10.439, 0.158], mean action: -0.998 [-1.108, -0.857], mean observation: 0.047 [-1.083, 1.000], loss: 0.852059, mean_absolute_error: 0.864731, mean_q: 14.849759
 752603/1000000: episode: 2256, duration: 5.151s, episode steps: 637, steps per second: 124, episode reward: -4.627, mean reward: -0.007 [-10.332, 0.097], mean action: -0.985 [-1.147, -0.855], mean observation: 0.199 [-0.370, 1.221], loss: 0.911314, mean_absolute_error: 0.866459, mean_q: 14.860810
 753437/1000000: episode: 2257, duration: 6.632s, episode steps: 834, steps per second: 126, episode reward: 227.491, mean reward: 0.273 [-10.319, 0.400], mean action: -0.999 [-1.188, -0.824], mean observation: 0.062 [-1.711, 1.000], loss: 0.907539, mean_absolute_error: 0.871472, mean_q: 14.842235
 754058/1000000: episode: 2258, duration: 5.088s, episode steps: 621, steps per second: 122, episode reward: 158.233, mean reward: 0.255 [-10.532, 0.414], mean action: -1.003 [-1.143, -0.852], mean observation: -0.003 [-2.073, 1.000], loss: 0.873909, mean_absolute_error: 0.871161, mean_q: 14.912515
 755000/1000000: episode: 2259, duration: 7.294s, episode steps: 942, steps per second: 129, episode reward: 213.888, mean reward: 0.227 [-10.562, 0.417], mean action: -1.013 [-1.169, -0.869], mean observation: 0.204 [-0.971, 2.182], loss: 0.851528, mean_absolute_error: 0.864865, mean_q: 14.918255
 755626/1000000: episode: 2260, duration: 5.135s, episode steps: 626, steps per second: 122, episode reward: 64.692, mean reward: 0.103 [-10.229, 0.176], mean action: -1.006 [-1.142, -0.863], mean observation: 0.037 [-2.032, 1.000], loss: 0.760222, mean_absolute_error: 0.850498, mean_q: 14.941001
 756378/1000000: episode: 2261, duration: 5.702s, episode steps: 752, steps per second: 132, episode reward: 104.412, mean reward: 0.139 [-10.314, 0.231], mean action: -1.006 [-1.167, -0.881], mean observation: 0.046 [-1.523, 1.000], loss: 0.942646, mean_absolute_error: 0.877663, mean_q: 14.865112
 756659/1000000: episode: 2262, duration: 2.297s, episode steps: 281, steps per second: 122, episode reward: -19.210, mean reward: -0.068 [-10.070, -0.024], mean action: -0.989 [-1.094, -0.866], mean observation: 0.111 [-0.928, 1.000], loss: 0.886855, mean_absolute_error: 0.865255, mean_q: 15.064929
 757313/1000000: episode: 2263, duration: 4.923s, episode steps: 654, steps per second: 133, episode reward: 50.904, mean reward: 0.078 [-11.001, 0.302], mean action: -1.018 [-1.154, -0.843], mean observation: 0.267 [-1.048, 2.394], loss: 0.869308, mean_absolute_error: 0.867004, mean_q: 14.911011
 758098/1000000: episode: 2264, duration: 6.074s, episode steps: 785, steps per second: 129, episode reward: 131.625, mean reward: 0.168 [-10.535, 0.383], mean action: -0.995 [-1.152, -0.837], mean observation: 0.031 [-1.937, 1.000], loss: 0.874878, mean_absolute_error: 0.866695, mean_q: 14.922037
 758752/1000000: episode: 2265, duration: 4.906s, episode steps: 654, steps per second: 133, episode reward: 70.921, mean reward: 0.108 [-10.401, 0.225], mean action: -0.987 [-1.113, -0.858], mean observation: 0.107 [-1.499, 1.479], loss: 0.889015, mean_absolute_error: 0.871829, mean_q: 14.870819
 759492/1000000: episode: 2266, duration: 5.501s, episode steps: 740, steps per second: 135, episode reward: 97.547, mean reward: 0.132 [-10.288, 0.276], mean action: -0.987 [-1.119, -0.823], mean observation: 0.069 [-1.106, 1.000], loss: 0.832537, mean_absolute_error: 0.868721, mean_q: 14.884198
 760371/1000000: episode: 2267, duration: 6.781s, episode steps: 879, steps per second: 130, episode reward: 299.190, mean reward: 0.340 [-10.286, 0.557], mean action: -0.995 [-1.130, -0.851], mean observation: 0.098 [-1.886, 1.000], loss: 0.870397, mean_absolute_error: 0.867378, mean_q: 14.911363
 761189/1000000: episode: 2268, duration: 6.377s, episode steps: 818, steps per second: 128, episode reward: 132.622, mean reward: 0.162 [-10.469, 0.364], mean action: -0.997 [-1.123, -0.866], mean observation: 0.200 [-0.455, 2.422], loss: 0.871526, mean_absolute_error: 0.862113, mean_q: 14.961219
 761957/1000000: episode: 2269, duration: 5.908s, episode steps: 768, steps per second: 130, episode reward: 212.684, mean reward: 0.277 [-10.326, 0.404], mean action: -0.999 [-1.146, -0.863], mean observation: 0.044 [-1.700, 1.000], loss: 0.869576, mean_absolute_error: 0.866131, mean_q: 14.967525
 762587/1000000: episode: 2270, duration: 5.060s, episode steps: 630, steps per second: 125, episode reward: 109.446, mean reward: 0.174 [-10.276, 0.270], mean action: -1.001 [-1.132, -0.834], mean observation: 0.141 [-1.459, 1.525], loss: 0.871533, mean_absolute_error: 0.869986, mean_q: 14.915904
 763272/1000000: episode: 2271, duration: 5.156s, episode steps: 685, steps per second: 133, episode reward: 173.269, mean reward: 0.253 [-10.365, 0.433], mean action: -1.012 [-1.164, -0.852], mean observation: 0.156 [-1.442, 1.429], loss: 0.809500, mean_absolute_error: 0.858483, mean_q: 14.979291
 763746/1000000: episode: 2272, duration: 3.751s, episode steps: 474, steps per second: 126, episode reward: 87.867, mean reward: 0.185 [-9.853, 0.216], mean action: -0.980 [-1.119, -0.863], mean observation: 0.133 [-0.962, 1.000], loss: 0.862637, mean_absolute_error: 0.860891, mean_q: 14.928575
 764653/1000000: episode: 2273, duration: 7.058s, episode steps: 907, steps per second: 129, episode reward: 260.467, mean reward: 0.287 [-10.350, 0.472], mean action: -1.000 [-1.164, -0.856], mean observation: 0.178 [-0.458, 2.102], loss: 0.849816, mean_absolute_error: 0.859071, mean_q: 15.039343
 765458/1000000: episode: 2274, duration: 6.140s, episode steps: 805, steps per second: 131, episode reward: 215.842, mean reward: 0.268 [-10.446, 0.556], mean action: -1.011 [-1.191, -0.887], mean observation: 0.047 [-1.634, 1.000], loss: 0.859776, mean_absolute_error: 0.859856, mean_q: 15.096414
 766010/1000000: episode: 2275, duration: 4.098s, episode steps: 552, steps per second: 135, episode reward: 100.220, mean reward: 0.182 [-10.269, 0.326], mean action: -0.991 [-1.140, -0.823], mean observation: 0.069 [-1.642, 1.000], loss: 0.870910, mean_absolute_error: 0.867869, mean_q: 14.965212
 766950/1000000: episode: 2276, duration: 6.998s, episode steps: 940, steps per second: 134, episode reward: 306.304, mean reward: 0.326 [-10.701, 0.584], mean action: -0.997 [-1.211, -0.845], mean observation: 0.139 [-1.278, 1.902], loss: 0.881182, mean_absolute_error: 0.864170, mean_q: 15.043829
 767733/1000000: episode: 2277, duration: 6.036s, episode steps: 783, steps per second: 130, episode reward: 293.925, mean reward: 0.375 [-10.973, 0.747], mean action: -0.999 [-1.155, -0.844], mean observation: -0.006 [-2.325, 1.000], loss: 0.824636, mean_absolute_error: 0.858965, mean_q: 15.128101
 768501/1000000: episode: 2278, duration: 5.885s, episode steps: 768, steps per second: 131, episode reward: 266.031, mean reward: 0.346 [-10.237, 0.544], mean action: -0.987 [-1.143, -0.849], mean observation: 0.072 [-1.401, 1.000], loss: 0.894982, mean_absolute_error: 0.863686, mean_q: 15.053440
 769433/1000000: episode: 2279, duration: 7.101s, episode steps: 932, steps per second: 131, episode reward: 263.630, mean reward: 0.283 [-10.286, 0.446], mean action: -1.015 [-1.177, -0.845], mean observation: 0.186 [-1.436, 1.470], loss: 0.865092, mean_absolute_error: 0.859972, mean_q: 15.131746
 770114/1000000: episode: 2280, duration: 5.271s, episode steps: 681, steps per second: 129, episode reward: 97.921, mean reward: 0.144 [-10.459, 0.311], mean action: -1.006 [-1.151, -0.893], mean observation: 0.129 [-1.113, 1.694], loss: 0.860568, mean_absolute_error: 0.856856, mean_q: 15.119135
 770724/1000000: episode: 2281, duration: 4.607s, episode steps: 610, steps per second: 132, episode reward: 65.152, mean reward: 0.107 [-10.690, 0.362], mean action: -1.001 [-1.154, -0.863], mean observation: 0.120 [-2.113, 1.231], loss: 0.855072, mean_absolute_error: 0.856152, mean_q: 15.196569
 771960/1000000: episode: 2282, duration: 9.168s, episode steps: 1236, steps per second: 135, episode reward: 32.888, mean reward: 0.027 [-10.091, 0.379], mean action: -1.001 [-1.162, -0.854], mean observation: 0.113 [-0.885, 2.018], loss: 0.860447, mean_absolute_error: 0.850500, mean_q: 15.220681
 772669/1000000: episode: 2283, duration: 5.260s, episode steps: 709, steps per second: 135, episode reward: 80.422, mean reward: 0.113 [-10.505, 0.239], mean action: -0.993 [-1.172, -0.859], mean observation: 0.088 [-2.025, 1.595], loss: 0.847098, mean_absolute_error: 0.850763, mean_q: 15.232874
 773144/1000000: episode: 2284, duration: 3.531s, episode steps: 475, steps per second: 135, episode reward: 130.553, mean reward: 0.275 [-9.796, 0.316], mean action: -0.991 [-1.124, -0.868], mean observation: 0.077 [-1.375, 1.000], loss: 0.861539, mean_absolute_error: 0.858682, mean_q: 15.190333
 774028/1000000: episode: 2285, duration: 6.762s, episode steps: 884, steps per second: 131, episode reward: 174.423, mean reward: 0.197 [-10.922, 0.528], mean action: -1.001 [-1.152, -0.839], mean observation: 0.133 [-1.933, 2.518], loss: 0.897901, mean_absolute_error: 0.853864, mean_q: 15.238308
 774264/1000000: episode: 2286, duration: 1.896s, episode steps: 236, steps per second: 124, episode reward: 49.930, mean reward: 0.212 [-9.751, 0.256], mean action: -1.016 [-1.141, -0.852], mean observation: 0.136 [-0.241, 1.000], loss: 0.948361, mean_absolute_error: 0.855560, mean_q: 15.243235
 774988/1000000: episode: 2287, duration: 5.728s, episode steps: 724, steps per second: 126, episode reward: 277.772, mean reward: 0.384 [-10.617, 0.673], mean action: -1.003 [-1.133, -0.807], mean observation: 0.122 [-1.940, 1.443], loss: 0.858923, mean_absolute_error: 0.852465, mean_q: 15.300957
 776112/1000000: episode: 2288, duration: 8.670s, episode steps: 1124, steps per second: 130, episode reward: 303.987, mean reward: 0.270 [-10.471, 0.503], mean action: -1.006 [-1.147, -0.836], mean observation: 0.193 [-0.460, 2.227], loss: 0.810331, mean_absolute_error: 0.843294, mean_q: 15.279552
 776750/1000000: episode: 2289, duration: 4.929s, episode steps: 638, steps per second: 129, episode reward: 127.515, mean reward: 0.200 [-10.990, 0.474], mean action: -0.992 [-1.191, -0.859], mean observation: 0.157 [-1.491, 2.992], loss: 0.841370, mean_absolute_error: 0.843531, mean_q: 15.339522
 777254/1000000: episode: 2290, duration: 3.834s, episode steps: 504, steps per second: 131, episode reward: 67.443, mean reward: 0.134 [-9.855, 0.161], mean action: -0.978 [-1.123, -0.793], mean observation: 0.158 [-0.159, 1.054], loss: 0.838244, mean_absolute_error: 0.843295, mean_q: 15.250132
 778107/1000000: episode: 2291, duration: 6.502s, episode steps: 853, steps per second: 131, episode reward: 322.545, mean reward: 0.378 [-10.245, 0.563], mean action: -1.011 [-1.133, -0.869], mean observation: 0.097 [-1.426, 1.000], loss: 0.864160, mean_absolute_error: 0.849076, mean_q: 15.286480
 778869/1000000: episode: 2292, duration: 5.717s, episode steps: 762, steps per second: 133, episode reward: 330.990, mean reward: 0.434 [-10.320, 0.605], mean action: -0.993 [-1.160, -0.833], mean observation: 0.065 [-2.132, 1.000], loss: 0.857000, mean_absolute_error: 0.845228, mean_q: 15.368258
 779561/1000000: episode: 2293, duration: 5.184s, episode steps: 692, steps per second: 133, episode reward: 71.326, mean reward: 0.103 [-10.685, 0.303], mean action: -0.988 [-1.153, -0.767], mean observation: 0.129 [-1.575, 1.860], loss: 0.838369, mean_absolute_error: 0.844998, mean_q: 15.345320
 780251/1000000: episode: 2294, duration: 5.552s, episode steps: 690, steps per second: 124, episode reward: 40.047, mean reward: 0.058 [-10.769, 0.259], mean action: -1.003 [-1.128, -0.838], mean observation: 0.128 [-1.901, 1.529], loss: 0.780582, mean_absolute_error: 0.843134, mean_q: 15.316654
 780873/1000000: episode: 2295, duration: 5.049s, episode steps: 622, steps per second: 123, episode reward: 107.024, mean reward: 0.172 [-10.281, 0.322], mean action: -1.008 [-1.122, -0.904], mean observation: 0.052 [-1.451, 1.000], loss: 0.775833, mean_absolute_error: 0.835854, mean_q: 15.377575
 781436/1000000: episode: 2296, duration: 4.375s, episode steps: 563, steps per second: 129, episode reward: 96.711, mean reward: 0.172 [-10.341, 0.329], mean action: -0.987 [-1.124, -0.852], mean observation: 0.202 [-0.343, 1.893], loss: 0.807126, mean_absolute_error: 0.838733, mean_q: 15.308164
 782098/1000000: episode: 2297, duration: 5.255s, episode steps: 662, steps per second: 126, episode reward: 202.660, mean reward: 0.306 [-10.586, 0.539], mean action: -1.003 [-1.125, -0.871], mean observation: 0.090 [-2.230, 1.336], loss: 0.867405, mean_absolute_error: 0.844619, mean_q: 15.410933
 782960/1000000: episode: 2298, duration: 6.758s, episode steps: 862, steps per second: 128, episode reward: 233.525, mean reward: 0.271 [-10.369, 0.415], mean action: -1.011 [-1.159, -0.875], mean observation: 0.057 [-1.761, 1.000], loss: 0.854702, mean_absolute_error: 0.839460, mean_q: 15.412138
 783721/1000000: episode: 2299, duration: 5.809s, episode steps: 761, steps per second: 131, episode reward: 313.604, mean reward: 0.412 [-10.442, 0.600], mean action: -1.005 [-1.144, -0.844], mean observation: 0.087 [-2.420, 1.000], loss: 0.905535, mean_absolute_error: 0.843658, mean_q: 15.400501
 784415/1000000: episode: 2300, duration: 5.398s, episode steps: 694, steps per second: 129, episode reward: 190.623, mean reward: 0.275 [-10.478, 0.438], mean action: -1.003 [-1.107, -0.851], mean observation: 0.203 [-0.643, 2.368], loss: 0.872758, mean_absolute_error: 0.834435, mean_q: 15.431846
 785273/1000000: episode: 2301, duration: 6.559s, episode steps: 858, steps per second: 131, episode reward: 216.351, mean reward: 0.252 [-10.564, 0.521], mean action: -1.000 [-1.141, -0.870], mean observation: 0.143 [-1.034, 1.498], loss: 0.875069, mean_absolute_error: 0.838148, mean_q: 15.466711
 785992/1000000: episode: 2302, duration: 5.598s, episode steps: 719, steps per second: 128, episode reward: 133.896, mean reward: 0.186 [-10.842, 0.453], mean action: -1.008 [-1.193, -0.857], mean observation: -0.018 [-2.651, 1.000], loss: 0.856529, mean_absolute_error: 0.837996, mean_q: 15.484438
 786765/1000000: episode: 2303, duration: 5.873s, episode steps: 773, steps per second: 132, episode reward: 56.748, mean reward: 0.073 [-10.709, 0.293], mean action: -1.002 [-1.127, -0.846], mean observation: 0.119 [-1.734, 1.302], loss: 0.823020, mean_absolute_error: 0.835961, mean_q: 15.467687
 787325/1000000: episode: 2304, duration: 4.222s, episode steps: 560, steps per second: 133, episode reward: 4.493, mean reward: 0.008 [-10.512, 0.164], mean action: -1.004 [-1.132, -0.876], mean observation: -0.005 [-1.677, 1.000], loss: 0.813241, mean_absolute_error: 0.828220, mean_q: 15.512647
 787746/1000000: episode: 2305, duration: 3.144s, episode steps: 421, steps per second: 134, episode reward: 2.286, mean reward: 0.005 [-9.920, 0.079], mean action: -0.996 [-1.153, -0.860], mean observation: 0.132 [-1.334, 1.000], loss: 0.813711, mean_absolute_error: 0.828226, mean_q: 15.418614
 788478/1000000: episode: 2306, duration: 5.504s, episode steps: 732, steps per second: 133, episode reward: 214.940, mean reward: 0.294 [-10.527, 0.535], mean action: -0.974 [-1.142, -0.805], mean observation: 0.113 [-1.543, 1.503], loss: 0.800107, mean_absolute_error: 0.827165, mean_q: 15.485753
 789406/1000000: episode: 2307, duration: 6.975s, episode steps: 928, steps per second: 133, episode reward: 352.502, mean reward: 0.380 [-10.353, 0.580], mean action: -1.002 [-1.135, -0.861], mean observation: 0.119 [-1.369, 1.819], loss: 0.858177, mean_absolute_error: 0.829145, mean_q: 15.475905
 789976/1000000: episode: 2308, duration: 4.304s, episode steps: 570, steps per second: 132, episode reward: 60.375, mean reward: 0.106 [-10.419, 0.275], mean action: -0.988 [-1.140, -0.833], mean observation: 0.109 [-1.617, 1.000], loss: 0.870556, mean_absolute_error: 0.829152, mean_q: 15.543997
 790749/1000000: episode: 2309, duration: 6.022s, episode steps: 773, steps per second: 128, episode reward: 217.407, mean reward: 0.281 [-10.391, 0.412], mean action: -0.996 [-1.120, -0.852], mean observation: 0.190 [-0.850, 2.366], loss: 0.809016, mean_absolute_error: 0.829660, mean_q: 15.506049
 791308/1000000: episode: 2310, duration: 4.241s, episode steps: 559, steps per second: 132, episode reward: 109.373, mean reward: 0.196 [-10.296, 0.349], mean action: -0.974 [-1.097, -0.848], mean observation: 0.092 [-1.602, 1.176], loss: 0.811831, mean_absolute_error: 0.831289, mean_q: 15.540348
 792006/1000000: episode: 2311, duration: 5.509s, episode steps: 698, steps per second: 127, episode reward: 215.219, mean reward: 0.308 [-10.295, 0.456], mean action: -0.981 [-1.135, -0.858], mean observation: 0.066 [-1.916, 1.000], loss: 0.847251, mean_absolute_error: 0.833292, mean_q: 15.589984
 792694/1000000: episode: 2312, duration: 5.471s, episode steps: 688, steps per second: 126, episode reward: 107.205, mean reward: 0.156 [-9.844, 0.186], mean action: -0.999 [-1.163, -0.842], mean observation: 0.103 [-1.456, 1.000], loss: 0.842568, mean_absolute_error: 0.831211, mean_q: 15.561291
 793346/1000000: episode: 2313, duration: 4.922s, episode steps: 652, steps per second: 132, episode reward: 82.392, mean reward: 0.126 [-10.327, 0.290], mean action: -1.012 [-1.172, -0.844], mean observation: 0.206 [-1.009, 1.617], loss: 0.849048, mean_absolute_error: 0.830091, mean_q: 15.567230
 794146/1000000: episode: 2314, duration: 6.255s, episode steps: 800, steps per second: 128, episode reward: 120.862, mean reward: 0.151 [-10.514, 0.303], mean action: -1.005 [-1.156, -0.856], mean observation: 0.214 [-0.346, 1.926], loss: 0.831132, mean_absolute_error: 0.828061, mean_q: 15.646709
 794776/1000000: episode: 2315, duration: 4.742s, episode steps: 630, steps per second: 133, episode reward: 53.782, mean reward: 0.085 [-10.327, 0.221], mean action: -1.000 [-1.134, -0.879], mean observation: 0.098 [-1.184, 1.517], loss: 0.838385, mean_absolute_error: 0.833909, mean_q: 15.694046
 795441/1000000: episode: 2316, duration: 5.151s, episode steps: 665, steps per second: 129, episode reward: 84.368, mean reward: 0.127 [-10.366, 0.290], mean action: -1.013 [-1.127, -0.898], mean observation: 0.235 [-0.179, 1.352], loss: 0.838795, mean_absolute_error: 0.830171, mean_q: 15.705466
 796236/1000000: episode: 2317, duration: 5.891s, episode steps: 795, steps per second: 135, episode reward: 342.996, mean reward: 0.431 [-10.359, 0.598], mean action: -0.995 [-1.121, -0.859], mean observation: 0.189 [-0.392, 2.257], loss: 0.860723, mean_absolute_error: 0.830707, mean_q: 15.636308
 796953/1000000: episode: 2318, duration: 5.424s, episode steps: 717, steps per second: 132, episode reward: 194.039, mean reward: 0.271 [-10.664, 0.483], mean action: -0.981 [-1.130, -0.814], mean observation: 0.241 [-1.000, 2.225], loss: 0.846690, mean_absolute_error: 0.833726, mean_q: 15.677448
 797364/1000000: episode: 2319, duration: 3.098s, episode steps: 411, steps per second: 133, episode reward: 105.639, mean reward: 0.257 [-9.740, 0.287], mean action: -0.998 [-1.099, -0.909], mean observation: 0.137 [-0.404, 1.000], loss: 0.797099, mean_absolute_error: 0.828040, mean_q: 15.727723
 798239/1000000: episode: 2320, duration: 6.796s, episode steps: 875, steps per second: 129, episode reward: 264.151, mean reward: 0.302 [-10.761, 0.620], mean action: -0.991 [-1.152, -0.835], mean observation: 0.132 [-1.883, 2.107], loss: 0.816248, mean_absolute_error: 0.832479, mean_q: 15.784460
 799030/1000000: episode: 2321, duration: 6.082s, episode steps: 791, steps per second: 130, episode reward: 239.140, mean reward: 0.302 [-10.611, 0.587], mean action: -1.005 [-1.154, -0.871], mean observation: 0.236 [-0.549, 1.948], loss: 0.807663, mean_absolute_error: 0.825440, mean_q: 15.770788
 799676/1000000: episode: 2322, duration: 5.000s, episode steps: 646, steps per second: 129, episode reward: 161.080, mean reward: 0.249 [-10.721, 0.429], mean action: -0.997 [-1.126, -0.861], mean observation: 0.240 [-1.110, 2.276], loss: 0.819786, mean_absolute_error: 0.827520, mean_q: 15.790807
 800148/1000000: episode: 2323, duration: 3.607s, episode steps: 472, steps per second: 131, episode reward: 4.912, mean reward: 0.010 [-10.208, 0.090], mean action: -1.013 [-1.162, -0.870], mean observation: 0.028 [-1.118, 1.000], loss: 0.840111, mean_absolute_error: 0.822948, mean_q: 15.837810
 800885/1000000: episode: 2324, duration: 5.645s, episode steps: 737, steps per second: 131, episode reward: 161.108, mean reward: 0.219 [-10.401, 0.384], mean action: -1.001 [-1.115, -0.860], mean observation: 0.217 [-0.469, 2.262], loss: 0.801410, mean_absolute_error: 0.826112, mean_q: 15.856428
 801647/1000000: episode: 2325, duration: 5.890s, episode steps: 762, steps per second: 129, episode reward: 229.211, mean reward: 0.301 [-10.488, 0.442], mean action: -0.986 [-1.137, -0.855], mean observation: 0.076 [-2.373, 1.000], loss: 0.882828, mean_absolute_error: 0.835026, mean_q: 15.787035
 802547/1000000: episode: 2326, duration: 6.727s, episode steps: 900, steps per second: 134, episode reward: 182.584, mean reward: 0.203 [-10.469, 0.355], mean action: -1.013 [-1.165, -0.883], mean observation: 0.073 [-2.446, 1.000], loss: 0.806826, mean_absolute_error: 0.826057, mean_q: 15.922850
 803118/1000000: episode: 2327, duration: 4.260s, episode steps: 571, steps per second: 134, episode reward: 70.044, mean reward: 0.123 [-10.697, 0.302], mean action: -1.006 [-1.178, -0.843], mean observation: 0.263 [-1.122, 2.164], loss: 0.826579, mean_absolute_error: 0.829479, mean_q: 15.962528
 803709/1000000: episode: 2328, duration: 4.414s, episode steps: 591, steps per second: 134, episode reward: 93.157, mean reward: 0.158 [-9.923, 0.187], mean action: -0.992 [-1.099, -0.883], mean observation: 0.070 [-1.247, 1.000], loss: 0.830210, mean_absolute_error: 0.830195, mean_q: 15.971400
 804441/1000000: episode: 2329, duration: 5.454s, episode steps: 732, steps per second: 134, episode reward: 267.875, mean reward: 0.366 [-10.342, 0.556], mean action: -1.009 [-1.136, -0.862], mean observation: 0.209 [-0.341, 2.187], loss: 0.902956, mean_absolute_error: 0.839440, mean_q: 16.022680
 805087/1000000: episode: 2330, duration: 4.885s, episode steps: 646, steps per second: 132, episode reward: 40.221, mean reward: 0.062 [-10.492, 0.223], mean action: -0.989 [-1.118, -0.847], mean observation: 0.231 [-0.274, 1.687], loss: 0.819050, mean_absolute_error: 0.826544, mean_q: 16.022604
 805902/1000000: episode: 2331, duration: 6.478s, episode steps: 815, steps per second: 126, episode reward: 142.609, mean reward: 0.175 [-10.273, 0.316], mean action: -0.991 [-1.105, -0.856], mean observation: 0.065 [-1.594, 1.000], loss: 0.847181, mean_absolute_error: 0.835193, mean_q: 16.064209
 806598/1000000: episode: 2332, duration: 5.935s, episode steps: 696, steps per second: 117, episode reward: 168.749, mean reward: 0.242 [-10.568, 0.480], mean action: -1.007 [-1.140, -0.887], mean observation: 0.140 [-1.669, 1.609], loss: 0.830867, mean_absolute_error: 0.835148, mean_q: 16.089180
 807555/1000000: episode: 2333, duration: 7.860s, episode steps: 957, steps per second: 122, episode reward: 107.294, mean reward: 0.112 [-10.662, 0.269], mean action: -0.995 [-1.123, -0.860], mean observation: 0.140 [-1.613, 1.786], loss: 0.850615, mean_absolute_error: 0.835599, mean_q: 16.075214
 808265/1000000: episode: 2334, duration: 5.678s, episode steps: 710, steps per second: 125, episode reward: 129.248, mean reward: 0.182 [-10.267, 0.316], mean action: -1.001 [-1.119, -0.881], mean observation: 0.030 [-2.017, 1.000], loss: 0.891571, mean_absolute_error: 0.842169, mean_q: 16.133890
 808755/1000000: episode: 2335, duration: 4.114s, episode steps: 490, steps per second: 119, episode reward: 139.507, mean reward: 0.285 [-9.715, 0.312], mean action: -1.004 [-1.140, -0.869], mean observation: 0.134 [-0.673, 1.000], loss: 0.822163, mean_absolute_error: 0.832844, mean_q: 16.168736
 809563/1000000: episode: 2336, duration: 6.776s, episode steps: 808, steps per second: 119, episode reward: 95.004, mean reward: 0.118 [-10.640, 0.312], mean action: -0.993 [-1.114, -0.863], mean observation: 0.223 [-0.313, 2.129], loss: 0.828817, mean_absolute_error: 0.836833, mean_q: 16.148201
 810174/1000000: episode: 2337, duration: 4.951s, episode steps: 611, steps per second: 123, episode reward: 138.097, mean reward: 0.226 [-10.597, 0.483], mean action: -0.998 [-1.146, -0.818], mean observation: -0.004 [-1.783, 1.000], loss: 0.778806, mean_absolute_error: 0.824537, mean_q: 16.262081
 810918/1000000: episode: 2338, duration: 6.296s, episode steps: 744, steps per second: 118, episode reward: 218.464, mean reward: 0.294 [-10.321, 0.409], mean action: -0.999 [-1.134, -0.824], mean observation: 0.073 [-2.165, 1.000], loss: 0.827947, mean_absolute_error: 0.838241, mean_q: 16.163988
 811846/1000000: episode: 2339, duration: 7.877s, episode steps: 928, steps per second: 118, episode reward: 147.580, mean reward: 0.159 [-10.769, 0.354], mean action: -0.994 [-1.146, -0.838], mean observation: 0.136 [-1.418, 2.001], loss: 0.798972, mean_absolute_error: 0.835449, mean_q: 16.339472
 813043/1000000: episode: 2340, duration: 9.647s, episode steps: 1197, steps per second: 124, episode reward: 389.220, mean reward: 0.325 [-10.559, 0.653], mean action: -1.002 [-1.207, -0.874], mean observation: 0.076 [-1.402, 1.000], loss: 0.845230, mean_absolute_error: 0.839947, mean_q: 16.294250
 813960/1000000: episode: 2341, duration: 7.360s, episode steps: 917, steps per second: 125, episode reward: 317.654, mean reward: 0.346 [-10.252, 0.519], mean action: -1.008 [-1.176, -0.894], mean observation: 0.087 [-1.581, 1.000], loss: 0.817913, mean_absolute_error: 0.840224, mean_q: 16.302525
 814684/1000000: episode: 2342, duration: 5.847s, episode steps: 724, steps per second: 124, episode reward: 103.690, mean reward: 0.143 [-10.695, 0.322], mean action: -0.994 [-1.108, -0.864], mean observation: -0.000 [-2.471, 1.000], loss: 0.850487, mean_absolute_error: 0.840974, mean_q: 16.340219
 815536/1000000: episode: 2343, duration: 6.743s, episode steps: 852, steps per second: 126, episode reward: 195.992, mean reward: 0.230 [-10.116, 0.345], mean action: -1.013 [-1.155, -0.887], mean observation: 0.082 [-1.242, 1.550], loss: 0.788943, mean_absolute_error: 0.831676, mean_q: 16.397255
 816019/1000000: episode: 2344, duration: 4.117s, episode steps: 483, steps per second: 117, episode reward: 2.448, mean reward: 0.005 [-10.114, 0.056], mean action: -0.994 [-1.121, -0.870], mean observation: 0.142 [-0.636, 1.040], loss: 0.862754, mean_absolute_error: 0.841897, mean_q: 16.427328
 817249/1000000: episode: 2345, duration: 9.471s, episode steps: 1230, steps per second: 130, episode reward: 303.483, mean reward: 0.247 [-10.499, 0.646], mean action: -0.991 [-1.166, -0.847], mean observation: 0.144 [-1.424, 2.084], loss: 0.831695, mean_absolute_error: 0.842031, mean_q: 16.420589
 818019/1000000: episode: 2346, duration: 5.861s, episode steps: 770, steps per second: 131, episode reward: 136.407, mean reward: 0.177 [-10.293, 0.276], mean action: -1.002 [-1.143, -0.873], mean observation: 0.086 [-1.750, 1.000], loss: 0.823851, mean_absolute_error: 0.842596, mean_q: 16.439775
 818747/1000000: episode: 2347, duration: 5.507s, episode steps: 728, steps per second: 132, episode reward: 84.484, mean reward: 0.116 [-10.642, 0.321], mean action: -1.006 [-1.155, -0.858], mean observation: 0.009 [-1.914, 1.000], loss: 0.815355, mean_absolute_error: 0.842202, mean_q: 16.455639
 819840/1000000: episode: 2348, duration: 8.716s, episode steps: 1093, steps per second: 125, episode reward: 225.728, mean reward: 0.207 [-10.525, 0.384], mean action: -1.000 [-1.135, -0.878], mean observation: 0.061 [-2.292, 1.000], loss: 0.833646, mean_absolute_error: 0.837315, mean_q: 16.434095
 820328/1000000: episode: 2349, duration: 3.771s, episode steps: 488, steps per second: 129, episode reward: 82.060, mean reward: 0.168 [-9.952, 0.222], mean action: -1.011 [-1.167, -0.899], mean observation: 0.164 [-0.460, 1.369], loss: 0.850210, mean_absolute_error: 0.840717, mean_q: 16.470072
 821219/1000000: episode: 2350, duration: 7.041s, episode steps: 891, steps per second: 127, episode reward: 309.531, mean reward: 0.347 [-10.290, 0.549], mean action: -0.997 [-1.134, -0.813], mean observation: 0.086 [-1.729, 1.000], loss: 0.839809, mean_absolute_error: 0.839225, mean_q: 16.455399
 821954/1000000: episode: 2351, duration: 5.749s, episode steps: 735, steps per second: 128, episode reward: 311.543, mean reward: 0.424 [-10.399, 0.627], mean action: -0.998 [-1.115, -0.855], mean observation: 0.035 [-2.233, 1.000], loss: 0.865975, mean_absolute_error: 0.844540, mean_q: 16.485409
 822577/1000000: episode: 2352, duration: 4.896s, episode steps: 623, steps per second: 127, episode reward: -15.524, mean reward: -0.025 [-10.650, 0.169], mean action: -0.979 [-1.123, -0.810], mean observation: -0.000 [-1.564, 1.000], loss: 0.807253, mean_absolute_error: 0.831473, mean_q: 16.514143
 823328/1000000: episode: 2353, duration: 5.627s, episode steps: 751, steps per second: 133, episode reward: 89.933, mean reward: 0.120 [-10.290, 0.196], mean action: -0.998 [-1.130, -0.866], mean observation: 0.080 [-2.121, 1.000], loss: 0.835914, mean_absolute_error: 0.836093, mean_q: 16.411594
 823762/1000000: episode: 2354, duration: 3.251s, episode steps: 434, steps per second: 134, episode reward: 114.443, mean reward: 0.264 [-9.761, 0.294], mean action: -1.008 [-1.141, -0.890], mean observation: 0.172 [-0.203, 1.394], loss: 0.849458, mean_absolute_error: 0.841782, mean_q: 16.433151
 824473/1000000: episode: 2355, duration: 5.404s, episode steps: 711, steps per second: 132, episode reward: 223.492, mean reward: 0.314 [-10.249, 0.500], mean action: -1.009 [-1.119, -0.899], mean observation: 0.190 [-0.463, 1.526], loss: 0.827551, mean_absolute_error: 0.832904, mean_q: 16.454796
 825203/1000000: episode: 2356, duration: 5.561s, episode steps: 730, steps per second: 131, episode reward: 273.958, mean reward: 0.375 [-10.556, 0.601], mean action: -1.006 [-1.172, -0.826], mean observation: 0.016 [-2.052, 1.000], loss: 0.835571, mean_absolute_error: 0.830649, mean_q: 16.412483
 826149/1000000: episode: 2357, duration: 7.179s, episode steps: 946, steps per second: 132, episode reward: 179.857, mean reward: 0.190 [-10.464, 0.370], mean action: -0.986 [-1.134, -0.855], mean observation: 0.082 [-2.404, 1.000], loss: 0.875499, mean_absolute_error: 0.834907, mean_q: 16.469830
 826765/1000000: episode: 2358, duration: 4.660s, episode steps: 616, steps per second: 132, episode reward: 141.825, mean reward: 0.230 [-10.325, 0.360], mean action: -0.993 [-1.150, -0.812], mean observation: 0.043 [-1.972, 1.000], loss: 0.783105, mean_absolute_error: 0.827120, mean_q: 16.433523
 828265/1000000: episode: 2359, duration: 11.502s, episode steps: 1500, steps per second: 130, episode reward: 370.275, mean reward: 0.247 [-0.062, 0.388], mean action: -0.989 [-1.186, -0.832], mean observation: 0.100 [-2.472, 1.000], loss: 0.814355, mean_absolute_error: 0.832623, mean_q: 16.468742
 828569/1000000: episode: 2360, duration: 2.326s, episode steps: 304, steps per second: 131, episode reward: 27.739, mean reward: 0.091 [-9.880, 0.127], mean action: -0.997 [-1.128, -0.877], mean observation: 0.121 [-0.534, 1.000], loss: 0.723718, mean_absolute_error: 0.817626, mean_q: 16.424107
 829266/1000000: episode: 2361, duration: 5.308s, episode steps: 697, steps per second: 131, episode reward: 165.514, mean reward: 0.237 [-10.344, 0.390], mean action: -1.014 [-1.121, -0.869], mean observation: 0.046 [-1.820, 1.000], loss: 0.853766, mean_absolute_error: 0.834718, mean_q: 16.436348
 830090/1000000: episode: 2362, duration: 6.345s, episode steps: 824, steps per second: 130, episode reward: 178.576, mean reward: 0.217 [-10.406, 0.437], mean action: -0.987 [-1.179, -0.850], mean observation: 0.154 [-1.278, 1.866], loss: 0.884408, mean_absolute_error: 0.837354, mean_q: 16.446285
 830770/1000000: episode: 2363, duration: 5.261s, episode steps: 680, steps per second: 129, episode reward: 64.955, mean reward: 0.096 [-10.478, 0.247], mean action: -0.991 [-1.125, -0.864], mean observation: 0.139 [-1.498, 1.364], loss: 0.844260, mean_absolute_error: 0.838092, mean_q: 16.377604
 831714/1000000: episode: 2364, duration: 7.261s, episode steps: 944, steps per second: 130, episode reward: 169.557, mean reward: 0.180 [-10.411, 0.336], mean action: -1.007 [-1.145, -0.867], mean observation: 0.217 [-0.319, 1.222], loss: 0.821292, mean_absolute_error: 0.834117, mean_q: 16.396349
 832188/1000000: episode: 2365, duration: 3.699s, episode steps: 474, steps per second: 128, episode reward: 87.884, mean reward: 0.185 [-9.824, 0.211], mean action: -1.013 [-1.134, -0.877], mean observation: 0.086 [-1.094, 1.000], loss: 0.810606, mean_absolute_error: 0.828539, mean_q: 16.429697
 832827/1000000: episode: 2366, duration: 5.024s, episode steps: 639, steps per second: 127, episode reward: 137.074, mean reward: 0.215 [-10.521, 0.349], mean action: -1.015 [-1.178, -0.875], mean observation: 0.241 [-0.774, 2.149], loss: 0.807210, mean_absolute_error: 0.829236, mean_q: 16.371868
 833346/1000000: episode: 2367, duration: 4.049s, episode steps: 519, steps per second: 128, episode reward: -8.995, mean reward: -0.017 [-10.465, 0.095], mean action: -1.004 [-1.172, -0.844], mean observation: 0.017 [-2.102, 1.000], loss: 0.810391, mean_absolute_error: 0.833707, mean_q: 16.368299
 833899/1000000: episode: 2368, duration: 4.308s, episode steps: 553, steps per second: 128, episode reward: 125.991, mean reward: 0.228 [-10.336, 0.397], mean action: -1.001 [-1.138, -0.873], mean observation: 0.023 [-1.799, 1.000], loss: 0.835976, mean_absolute_error: 0.838910, mean_q: 16.389629
 834502/1000000: episode: 2369, duration: 4.745s, episode steps: 603, steps per second: 127, episode reward: 71.549, mean reward: 0.119 [-10.534, 0.264], mean action: -0.994 [-1.143, -0.836], mean observation: 0.191 [-0.507, 2.424], loss: 0.759542, mean_absolute_error: 0.819640, mean_q: 16.301332
 835333/1000000: episode: 2370, duration: 6.598s, episode steps: 831, steps per second: 126, episode reward: 300.843, mean reward: 0.362 [-10.579, 0.560], mean action: -0.995 [-1.137, -0.879], mean observation: 0.121 [-1.734, 1.883], loss: 0.836445, mean_absolute_error: 0.834169, mean_q: 16.358809
 835999/1000000: episode: 2371, duration: 5.156s, episode steps: 666, steps per second: 129, episode reward: 13.790, mean reward: 0.021 [-10.361, 0.109], mean action: -1.005 [-1.120, -0.844], mean observation: 0.085 [-1.607, 1.532], loss: 0.798644, mean_absolute_error: 0.836349, mean_q: 16.311623
 836647/1000000: episode: 2372, duration: 5.056s, episode steps: 648, steps per second: 128, episode reward: 93.253, mean reward: 0.144 [-10.482, 0.328], mean action: -1.010 [-1.137, -0.867], mean observation: 0.016 [-1.600, 1.000], loss: 0.818067, mean_absolute_error: 0.834263, mean_q: 16.305893
 837364/1000000: episode: 2373, duration: 5.509s, episode steps: 717, steps per second: 130, episode reward: 47.847, mean reward: 0.067 [-10.381, 0.195], mean action: -0.998 [-1.111, -0.854], mean observation: 0.156 [-1.143, 1.254], loss: 0.856674, mean_absolute_error: 0.831684, mean_q: 16.344494
 838109/1000000: episode: 2374, duration: 5.747s, episode steps: 745, steps per second: 130, episode reward: 232.461, mean reward: 0.312 [-10.284, 0.538], mean action: -1.014 [-1.161, -0.878], mean observation: 0.195 [-0.408, 2.133], loss: 0.781957, mean_absolute_error: 0.827747, mean_q: 16.330265
 839083/1000000: episode: 2375, duration: 7.741s, episode steps: 974, steps per second: 126, episode reward: 292.727, mean reward: 0.301 [-10.395, 0.528], mean action: -0.991 [-1.115, -0.861], mean observation: 0.109 [-1.698, 1.000], loss: 0.854814, mean_absolute_error: 0.838219, mean_q: 16.293697
 840071/1000000: episode: 2376, duration: 7.429s, episode steps: 988, steps per second: 133, episode reward: 413.126, mean reward: 0.418 [-10.282, 0.666], mean action: -1.005 [-1.129, -0.845], mean observation: 0.169 [-1.143, 1.236], loss: 0.802079, mean_absolute_error: 0.839473, mean_q: 16.242643
 840864/1000000: episode: 2377, duration: 6.218s, episode steps: 793, steps per second: 128, episode reward: 198.922, mean reward: 0.251 [-10.449, 0.453], mean action: -1.001 [-1.152, -0.858], mean observation: 0.224 [-0.338, 1.550], loss: 0.814318, mean_absolute_error: 0.837990, mean_q: 16.288012
 841665/1000000: episode: 2378, duration: 6.100s, episode steps: 801, steps per second: 131, episode reward: 136.409, mean reward: 0.170 [-10.825, 0.408], mean action: -1.001 [-1.172, -0.828], mean observation: 0.136 [-1.784, 2.378], loss: 0.824162, mean_absolute_error: 0.839724, mean_q: 16.274637
 842971/1000000: episode: 2379, duration: 10.055s, episode steps: 1306, steps per second: 130, episode reward: -119.085, mean reward: -0.091 [-10.351, 0.215], mean action: -0.987 [-1.146, -0.841], mean observation: 0.111 [-1.038, 1.000], loss: 0.813490, mean_absolute_error: 0.834819, mean_q: 16.264185
 843619/1000000: episode: 2380, duration: 5.021s, episode steps: 648, steps per second: 129, episode reward: 5.607, mean reward: 0.009 [-10.794, 0.225], mean action: -0.980 [-1.120, -0.852], mean observation: -0.015 [-1.941, 1.000], loss: 0.762444, mean_absolute_error: 0.824264, mean_q: 16.224827
 844176/1000000: episode: 2381, duration: 4.217s, episode steps: 557, steps per second: 132, episode reward: 48.986, mean reward: 0.088 [-10.044, 0.161], mean action: -1.004 [-1.149, -0.867], mean observation: 0.123 [-0.568, 1.000], loss: 0.788842, mean_absolute_error: 0.826175, mean_q: 16.299049
 844764/1000000: episode: 2382, duration: 4.476s, episode steps: 588, steps per second: 131, episode reward: 12.931, mean reward: 0.022 [-10.205, 0.105], mean action: -0.995 [-1.136, -0.838], mean observation: 0.191 [-0.324, 1.104], loss: 0.887504, mean_absolute_error: 0.838818, mean_q: 16.261738
 845550/1000000: episode: 2383, duration: 5.924s, episode steps: 786, steps per second: 133, episode reward: 176.913, mean reward: 0.225 [-10.406, 0.367], mean action: -1.009 [-1.184, -0.867], mean observation: 0.031 [-2.077, 1.000], loss: 0.816882, mean_absolute_error: 0.832611, mean_q: 16.218431
 845709/1000000: episode: 2384, duration: 1.215s, episode steps: 159, steps per second: 131, episode reward: 32.401, mean reward: 0.204 [-9.734, 0.267], mean action: -1.005 [-1.095, -0.913], mean observation: 0.117 [-0.149, 1.000], loss: 0.780106, mean_absolute_error: 0.828723, mean_q: 16.212154
 846552/1000000: episode: 2385, duration: 6.594s, episode steps: 843, steps per second: 128, episode reward: 96.836, mean reward: 0.115 [-10.623, 0.311], mean action: -1.013 [-1.150, -0.873], mean observation: 0.218 [-0.242, 1.961], loss: 0.866681, mean_absolute_error: 0.849174, mean_q: 16.234232
 847177/1000000: episode: 2386, duration: 4.688s, episode steps: 625, steps per second: 133, episode reward: 180.638, mean reward: 0.289 [-9.762, 0.316], mean action: -0.984 [-1.109, -0.842], mean observation: 0.100 [-1.048, 1.000], loss: 0.864160, mean_absolute_error: 0.844330, mean_q: 16.242485
 847893/1000000: episode: 2387, duration: 5.413s, episode steps: 716, steps per second: 132, episode reward: 110.313, mean reward: 0.154 [-10.656, 0.325], mean action: -0.993 [-1.111, -0.849], mean observation: 0.091 [-2.444, 1.034], loss: 0.836636, mean_absolute_error: 0.840670, mean_q: 16.197866
 848689/1000000: episode: 2388, duration: 6.122s, episode steps: 796, steps per second: 130, episode reward: 113.693, mean reward: 0.143 [-10.425, 0.345], mean action: -1.000 [-1.150, -0.840], mean observation: 0.228 [-0.169, 1.143], loss: 0.826498, mean_absolute_error: 0.839534, mean_q: 16.212137
 849318/1000000: episode: 2389, duration: 4.987s, episode steps: 629, steps per second: 126, episode reward: 128.223, mean reward: 0.204 [-10.469, 0.349], mean action: -1.009 [-1.161, -0.827], mean observation: 0.162 [-1.672, 1.925], loss: 0.807291, mean_absolute_error: 0.833179, mean_q: 16.220139
 850161/1000000: episode: 2390, duration: 6.683s, episode steps: 843, steps per second: 126, episode reward: 188.763, mean reward: 0.224 [-10.321, 0.340], mean action: -0.998 [-1.123, -0.878], mean observation: 0.161 [-1.378, 1.321], loss: 0.843538, mean_absolute_error: 0.841188, mean_q: 16.162548
 850840/1000000: episode: 2391, duration: 5.182s, episode steps: 679, steps per second: 131, episode reward: 29.050, mean reward: 0.043 [-10.320, 0.171], mean action: -1.012 [-1.135, -0.833], mean observation: 0.054 [-1.171, 1.000], loss: 0.842686, mean_absolute_error: 0.840182, mean_q: 16.292667
 851586/1000000: episode: 2392, duration: 6.497s, episode steps: 746, steps per second: 115, episode reward: 44.042, mean reward: 0.059 [-10.633, 0.291], mean action: -0.998 [-1.139, -0.877], mean observation: 0.121 [-1.249, 1.282], loss: 0.808957, mean_absolute_error: 0.846948, mean_q: 16.228159
 852237/1000000: episode: 2393, duration: 5.126s, episode steps: 651, steps per second: 127, episode reward: -22.414, mean reward: -0.034 [-10.527, 0.100], mean action: -0.995 [-1.125, -0.867], mean observation: 0.152 [-0.872, 1.614], loss: 0.870002, mean_absolute_error: 0.845398, mean_q: 16.236568
 852869/1000000: episode: 2394, duration: 5.660s, episode steps: 632, steps per second: 112, episode reward: 115.945, mean reward: 0.183 [-10.355, 0.373], mean action: -0.995 [-1.139, -0.828], mean observation: 0.100 [-1.312, 1.473], loss: 0.777496, mean_absolute_error: 0.839368, mean_q: 16.208582
 853460/1000000: episode: 2395, duration: 5.122s, episode steps: 591, steps per second: 115, episode reward: 66.029, mean reward: 0.112 [-10.368, 0.265], mean action: -1.009 [-1.152, -0.888], mean observation: 0.179 [-0.480, 1.647], loss: 0.828734, mean_absolute_error: 0.846767, mean_q: 16.256233
 854098/1000000: episode: 2396, duration: 5.580s, episode steps: 638, steps per second: 114, episode reward: 99.692, mean reward: 0.156 [-10.361, 0.282], mean action: -1.003 [-1.143, -0.847], mean observation: 0.077 [-1.850, 1.198], loss: 0.819202, mean_absolute_error: 0.846353, mean_q: 16.187466
 854795/1000000: episode: 2397, duration: 6.117s, episode steps: 697, steps per second: 114, episode reward: 178.414, mean reward: 0.256 [-10.698, 0.522], mean action: -1.008 [-1.128, -0.871], mean observation: 0.259 [-0.443, 2.187], loss: 0.815742, mean_absolute_error: 0.843253, mean_q: 16.257853
 855411/1000000: episode: 2398, duration: 4.693s, episode steps: 616, steps per second: 131, episode reward: 127.312, mean reward: 0.207 [-10.577, 0.410], mean action: -1.011 [-1.168, -0.879], mean observation: 0.162 [-1.426, 2.210], loss: 0.846954, mean_absolute_error: 0.855352, mean_q: 16.197477
 856286/1000000: episode: 2399, duration: 6.602s, episode steps: 875, steps per second: 133, episode reward: 336.854, mean reward: 0.385 [-10.381, 0.599], mean action: -0.986 [-1.133, -0.833], mean observation: 0.174 [-0.688, 2.079], loss: 0.809200, mean_absolute_error: 0.842734, mean_q: 16.242399
 856978/1000000: episode: 2400, duration: 5.375s, episode steps: 692, steps per second: 129, episode reward: 89.011, mean reward: 0.129 [-10.572, 0.282], mean action: -1.003 [-1.158, -0.865], mean observation: 0.107 [-1.670, 1.740], loss: 0.812993, mean_absolute_error: 0.843603, mean_q: 16.247322
 857837/1000000: episode: 2401, duration: 6.683s, episode steps: 859, steps per second: 129, episode reward: 348.885, mean reward: 0.406 [-10.515, 0.634], mean action: -0.996 [-1.129, -0.885], mean observation: 0.226 [-0.281, 2.027], loss: 0.819906, mean_absolute_error: 0.845773, mean_q: 16.297327
 858889/1000000: episode: 2402, duration: 8.775s, episode steps: 1052, steps per second: 120, episode reward: 140.000, mean reward: 0.133 [-10.331, 0.382], mean action: -1.005 [-1.138, -0.855], mean observation: 0.165 [-0.536, 1.433], loss: 0.845366, mean_absolute_error: 0.845022, mean_q: 16.313210
 860060/1000000: episode: 2403, duration: 9.023s, episode steps: 1171, steps per second: 130, episode reward: 228.807, mean reward: 0.195 [-10.059, 0.301], mean action: -1.002 [-1.176, -0.832], mean observation: 0.093 [-1.660, 1.000], loss: 0.845406, mean_absolute_error: 0.844290, mean_q: 16.320583
 860854/1000000: episode: 2404, duration: 6.114s, episode steps: 794, steps per second: 130, episode reward: 172.324, mean reward: 0.217 [-10.602, 0.404], mean action: -0.996 [-1.119, -0.801], mean observation: 0.215 [-0.907, 2.413], loss: 0.796696, mean_absolute_error: 0.841748, mean_q: 16.328981
 861761/1000000: episode: 2405, duration: 6.865s, episode steps: 907, steps per second: 132, episode reward: 31.871, mean reward: 0.035 [-10.691, 0.395], mean action: -0.998 [-1.188, -0.833], mean observation: 0.040 [-1.133, 1.000], loss: 0.844678, mean_absolute_error: 0.846557, mean_q: 16.341976
 862474/1000000: episode: 2406, duration: 5.415s, episode steps: 713, steps per second: 132, episode reward: 171.666, mean reward: 0.241 [-10.589, 0.499], mean action: -1.006 [-1.152, -0.872], mean observation: 0.243 [-0.612, 2.460], loss: 0.822089, mean_absolute_error: 0.850076, mean_q: 16.281958
 863638/1000000: episode: 2407, duration: 8.662s, episode steps: 1164, steps per second: 134, episode reward: 343.417, mean reward: 0.295 [-10.468, 0.482], mean action: -1.002 [-1.146, -0.837], mean observation: 0.152 [-2.176, 1.556], loss: 0.811036, mean_absolute_error: 0.843692, mean_q: 16.287340
 864420/1000000: episode: 2408, duration: 5.799s, episode steps: 782, steps per second: 135, episode reward: 140.861, mean reward: 0.180 [-10.163, 0.315], mean action: -0.999 [-1.145, -0.851], mean observation: 0.089 [-0.846, 1.000], loss: 0.853097, mean_absolute_error: 0.847272, mean_q: 16.284182
 865237/1000000: episode: 2409, duration: 6.075s, episode steps: 817, steps per second: 134, episode reward: 188.662, mean reward: 0.231 [-10.401, 0.363], mean action: -0.998 [-1.169, -0.834], mean observation: 0.177 [-0.351, 2.119], loss: 0.823822, mean_absolute_error: 0.846003, mean_q: 16.294397
 865986/1000000: episode: 2410, duration: 5.567s, episode steps: 749, steps per second: 135, episode reward: 27.600, mean reward: 0.037 [-10.138, 0.078], mean action: -1.013 [-1.157, -0.861], mean observation: 0.092 [-0.876, 1.327], loss: 0.815593, mean_absolute_error: 0.841367, mean_q: 16.315979
 866345/1000000: episode: 2411, duration: 2.680s, episode steps: 359, steps per second: 134, episode reward: 82.940, mean reward: 0.231 [-9.794, 0.272], mean action: -1.005 [-1.137, -0.871], mean observation: 0.167 [-0.157, 1.289], loss: 0.823788, mean_absolute_error: 0.849050, mean_q: 16.321280
 867006/1000000: episode: 2412, duration: 4.945s, episode steps: 661, steps per second: 134, episode reward: 25.057, mean reward: 0.038 [-10.808, 0.249], mean action: -0.995 [-1.144, -0.851], mean observation: 0.099 [-2.239, 1.318], loss: 0.813504, mean_absolute_error: 0.842937, mean_q: 16.284193
 867587/1000000: episode: 2413, duration: 4.361s, episode steps: 581, steps per second: 133, episode reward: 133.928, mean reward: 0.231 [-10.157, 0.313], mean action: -1.004 [-1.151, -0.824], mean observation: 0.156 [-0.709, 2.436], loss: 0.821407, mean_absolute_error: 0.843002, mean_q: 16.321198
 868006/1000000: episode: 2414, duration: 3.147s, episode steps: 419, steps per second: 133, episode reward: 103.480, mean reward: 0.247 [-9.843, 0.297], mean action: -0.992 [-1.106, -0.874], mean observation: 0.059 [-1.115, 1.000], loss: 0.850145, mean_absolute_error: 0.853045, mean_q: 16.343830
 869320/1000000: episode: 2415, duration: 10.075s, episode steps: 1314, steps per second: 130, episode reward: 85.855, mean reward: 0.065 [-10.515, 0.353], mean action: -1.008 [-1.175, -0.842], mean observation: 0.132 [-0.795, 1.000], loss: 0.806795, mean_absolute_error: 0.837957, mean_q: 16.295767
 870175/1000000: episode: 2416, duration: 6.449s, episode steps: 855, steps per second: 133, episode reward: 119.229, mean reward: 0.139 [-10.320, 0.265], mean action: -0.995 [-1.120, -0.845], mean observation: 0.167 [-0.498, 1.278], loss: 0.812620, mean_absolute_error: 0.840594, mean_q: 16.353218
 870937/1000000: episode: 2417, duration: 5.823s, episode steps: 762, steps per second: 131, episode reward: 165.994, mean reward: 0.218 [-10.378, 0.353], mean action: -1.006 [-1.166, -0.822], mean observation: 0.041 [-1.857, 1.000], loss: 0.778943, mean_absolute_error: 0.837809, mean_q: 16.285412
 871640/1000000: episode: 2418, duration: 5.386s, episode steps: 703, steps per second: 131, episode reward: 120.908, mean reward: 0.172 [-9.913, 0.242], mean action: -1.016 [-1.144, -0.890], mean observation: 0.150 [-0.658, 1.598], loss: 0.832836, mean_absolute_error: 0.843286, mean_q: 16.285801
 872159/1000000: episode: 2419, duration: 3.934s, episode steps: 519, steps per second: 132, episode reward: 81.928, mean reward: 0.158 [-10.105, 0.252], mean action: -1.015 [-1.149, -0.878], mean observation: 0.208 [-0.206, 1.295], loss: 0.790204, mean_absolute_error: 0.835337, mean_q: 16.349850
 873057/1000000: episode: 2420, duration: 6.732s, episode steps: 898, steps per second: 133, episode reward: 224.475, mean reward: 0.250 [-10.324, 0.374], mean action: -1.006 [-1.146, -0.870], mean observation: 0.184 [-1.035, 1.704], loss: 0.875260, mean_absolute_error: 0.846550, mean_q: 16.295877
 873732/1000000: episode: 2421, duration: 5.058s, episode steps: 675, steps per second: 133, episode reward: 60.925, mean reward: 0.090 [-10.368, 0.183], mean action: -1.008 [-1.113, -0.877], mean observation: 0.112 [-1.276, 1.952], loss: 0.817765, mean_absolute_error: 0.844798, mean_q: 16.339815
 874718/1000000: episode: 2422, duration: 7.469s, episode steps: 986, steps per second: 132, episode reward: 270.684, mean reward: 0.275 [-10.437, 0.427], mean action: -1.002 [-1.148, -0.862], mean observation: 0.053 [-1.926, 1.000], loss: 0.815270, mean_absolute_error: 0.841847, mean_q: 16.400820
 875397/1000000: episode: 2423, duration: 5.167s, episode steps: 679, steps per second: 131, episode reward: 95.469, mean reward: 0.141 [-10.263, 0.235], mean action: -1.006 [-1.134, -0.875], mean observation: 0.140 [-1.791, 1.715], loss: 0.794693, mean_absolute_error: 0.840593, mean_q: 16.345411
 876004/1000000: episode: 2424, duration: 4.614s, episode steps: 607, steps per second: 132, episode reward: 137.830, mean reward: 0.227 [-10.268, 0.376], mean action: -0.998 [-1.139, -0.842], mean observation: 0.094 [-1.547, 1.000], loss: 0.816663, mean_absolute_error: 0.848711, mean_q: 16.298061
 876802/1000000: episode: 2425, duration: 6.072s, episode steps: 798, steps per second: 131, episode reward: 164.029, mean reward: 0.206 [-10.445, 0.398], mean action: -0.991 [-1.120, -0.847], mean observation: 0.046 [-1.771, 1.457], loss: 0.825770, mean_absolute_error: 0.847346, mean_q: 16.337967
 877561/1000000: episode: 2426, duration: 5.804s, episode steps: 759, steps per second: 131, episode reward: 109.682, mean reward: 0.145 [-10.275, 0.274], mean action: -0.998 [-1.112, -0.863], mean observation: 0.169 [-0.491, 1.144], loss: 0.839113, mean_absolute_error: 0.842049, mean_q: 16.362127
 878144/1000000: episode: 2427, duration: 4.393s, episode steps: 583, steps per second: 133, episode reward: 118.765, mean reward: 0.204 [-10.534, 0.401], mean action: -0.994 [-1.121, -0.859], mean observation: 0.169 [-1.140, 2.204], loss: 0.842840, mean_absolute_error: 0.848407, mean_q: 16.345663
 878897/1000000: episode: 2428, duration: 5.816s, episode steps: 753, steps per second: 129, episode reward: 160.793, mean reward: 0.214 [-10.537, 0.445], mean action: -1.000 [-1.149, -0.860], mean observation: 0.230 [-0.289, 1.715], loss: 0.830470, mean_absolute_error: 0.843546, mean_q: 16.328274
 879471/1000000: episode: 2429, duration: 4.407s, episode steps: 574, steps per second: 130, episode reward: 90.526, mean reward: 0.158 [-10.328, 0.302], mean action: -1.015 [-1.162, -0.893], mean observation: 0.083 [-1.635, 1.266], loss: 0.811975, mean_absolute_error: 0.841169, mean_q: 16.325342
 880064/1000000: episode: 2430, duration: 4.451s, episode steps: 593, steps per second: 133, episode reward: 64.276, mean reward: 0.108 [-10.116, 0.174], mean action: -0.986 [-1.103, -0.851], mean observation: 0.177 [-0.330, 1.352], loss: 0.854684, mean_absolute_error: 0.843370, mean_q: 16.321270
 880453/1000000: episode: 2431, duration: 3.132s, episode steps: 389, steps per second: 124, episode reward: 35.264, mean reward: 0.091 [-9.853, 0.147], mean action: -1.015 [-1.123, -0.910], mean observation: 0.116 [-0.490, 1.000], loss: 0.818832, mean_absolute_error: 0.844265, mean_q: 16.407824
 881089/1000000: episode: 2432, duration: 5.206s, episode steps: 636, steps per second: 122, episode reward: 81.588, mean reward: 0.128 [-10.304, 0.267], mean action: -0.988 [-1.094, -0.848], mean observation: 0.072 [-1.570, 1.091], loss: 0.892493, mean_absolute_error: 0.851466, mean_q: 16.396406
 881938/1000000: episode: 2433, duration: 6.554s, episode steps: 849, steps per second: 130, episode reward: 204.452, mean reward: 0.241 [-10.256, 0.439], mean action: -1.003 [-1.186, -0.845], mean observation: 0.082 [-1.280, 1.000], loss: 0.839445, mean_absolute_error: 0.849963, mean_q: 16.309462
 883087/1000000: episode: 2434, duration: 9.021s, episode steps: 1149, steps per second: 127, episode reward: 332.104, mean reward: 0.289 [-10.565, 0.533], mean action: -0.996 [-1.129, -0.869], mean observation: 0.132 [-1.614, 1.037], loss: 0.834353, mean_absolute_error: 0.847416, mean_q: 16.319929
 884189/1000000: episode: 2435, duration: 9.012s, episode steps: 1102, steps per second: 122, episode reward: 336.474, mean reward: 0.305 [-10.524, 0.530], mean action: -0.992 [-1.168, -0.835], mean observation: 0.062 [-1.837, 1.000], loss: 0.834215, mean_absolute_error: 0.847870, mean_q: 16.401690
 884611/1000000: episode: 2436, duration: 3.354s, episode steps: 422, steps per second: 126, episode reward: 80.348, mean reward: 0.190 [-9.727, 0.273], mean action: -1.013 [-1.129, -0.863], mean observation: 0.130 [-0.681, 1.069], loss: 0.858937, mean_absolute_error: 0.847308, mean_q: 16.467571
 885463/1000000: episode: 2437, duration: 6.465s, episode steps: 852, steps per second: 132, episode reward: 242.111, mean reward: 0.284 [-10.462, 0.488], mean action: -0.996 [-1.128, -0.855], mean observation: 0.226 [-0.383, 2.006], loss: 0.816791, mean_absolute_error: 0.848023, mean_q: 16.353649
 886299/1000000: episode: 2438, duration: 6.291s, episode steps: 836, steps per second: 133, episode reward: 206.700, mean reward: 0.247 [-10.486, 0.385], mean action: -0.996 [-1.182, -0.866], mean observation: 0.055 [-2.440, 1.000], loss: 0.882965, mean_absolute_error: 0.853698, mean_q: 16.426910
 887330/1000000: episode: 2439, duration: 8.110s, episode steps: 1031, steps per second: 127, episode reward: 126.612, mean reward: 0.123 [-10.574, 0.413], mean action: -0.998 [-1.192, -0.857], mean observation: 0.116 [-1.552, 1.281], loss: 0.822863, mean_absolute_error: 0.845257, mean_q: 16.443796
 888100/1000000: episode: 2440, duration: 6.188s, episode steps: 770, steps per second: 124, episode reward: 71.102, mean reward: 0.092 [-10.869, 0.289], mean action: -1.001 [-1.166, -0.858], mean observation: 0.101 [-2.732, 1.277], loss: 0.819745, mean_absolute_error: 0.841571, mean_q: 16.446426
 888814/1000000: episode: 2441, duration: 5.811s, episode steps: 714, steps per second: 123, episode reward: 120.827, mean reward: 0.169 [-10.521, 0.338], mean action: -0.991 [-1.131, -0.833], mean observation: 0.244 [-0.327, 1.598], loss: 0.805693, mean_absolute_error: 0.839721, mean_q: 16.469561
 889428/1000000: episode: 2442, duration: 4.768s, episode steps: 614, steps per second: 129, episode reward: 87.046, mean reward: 0.142 [-10.381, 0.291], mean action: -0.999 [-1.167, -0.837], mean observation: 0.079 [-1.737, 1.016], loss: 0.810392, mean_absolute_error: 0.837569, mean_q: 16.523458
 890252/1000000: episode: 2443, duration: 6.509s, episode steps: 824, steps per second: 127, episode reward: 139.384, mean reward: 0.169 [-10.511, 0.268], mean action: -0.994 [-1.128, -0.862], mean observation: 0.100 [-1.883, 1.973], loss: 0.886600, mean_absolute_error: 0.850320, mean_q: 16.468927
 890958/1000000: episode: 2444, duration: 6.137s, episode steps: 706, steps per second: 115, episode reward: 145.943, mean reward: 0.207 [-10.331, 0.352], mean action: -1.000 [-1.122, -0.857], mean observation: 0.215 [-0.326, 1.881], loss: 0.876077, mean_absolute_error: 0.853657, mean_q: 16.427818
 892223/1000000: episode: 2445, duration: 9.660s, episode steps: 1265, steps per second: 131, episode reward: 440.815, mean reward: 0.348 [-10.474, 0.579], mean action: -0.992 [-1.167, -0.812], mean observation: 0.151 [-1.815, 1.921], loss: 0.812764, mean_absolute_error: 0.836584, mean_q: 16.507885
 892922/1000000: episode: 2446, duration: 5.291s, episode steps: 699, steps per second: 132, episode reward: 205.154, mean reward: 0.293 [-10.420, 0.453], mean action: -1.010 [-1.154, -0.866], mean observation: 0.197 [-0.389, 2.389], loss: 0.810793, mean_absolute_error: 0.836373, mean_q: 16.454277
 893706/1000000: episode: 2447, duration: 6.098s, episode steps: 784, steps per second: 129, episode reward: 132.347, mean reward: 0.169 [-10.477, 0.331], mean action: -1.002 [-1.152, -0.885], mean observation: 0.221 [-0.437, 2.058], loss: 0.870477, mean_absolute_error: 0.844458, mean_q: 16.491930
 894450/1000000: episode: 2448, duration: 5.775s, episode steps: 744, steps per second: 129, episode reward: 98.805, mean reward: 0.133 [-10.638, 0.383], mean action: -0.988 [-1.126, -0.879], mean observation: 0.143 [-1.179, 1.555], loss: 0.837838, mean_absolute_error: 0.838320, mean_q: 16.495897
 894952/1000000: episode: 2449, duration: 4.003s, episode steps: 502, steps per second: 125, episode reward: 32.281, mean reward: 0.064 [-10.015, 0.119], mean action: -1.016 [-1.120, -0.923], mean observation: 0.072 [-0.648, 1.000], loss: 0.836963, mean_absolute_error: 0.837877, mean_q: 16.480417
 895778/1000000: episode: 2450, duration: 6.564s, episode steps: 826, steps per second: 126, episode reward: 237.924, mean reward: 0.288 [-10.396, 0.462], mean action: -1.023 [-1.159, -0.855], mean observation: 0.159 [-0.802, 1.710], loss: 0.803122, mean_absolute_error: 0.832895, mean_q: 16.501898
 896418/1000000: episode: 2451, duration: 4.975s, episode steps: 640, steps per second: 129, episode reward: -22.099, mean reward: -0.035 [-10.462, 0.135], mean action: -0.990 [-1.143, -0.818], mean observation: 0.099 [-1.105, 1.000], loss: 0.783313, mean_absolute_error: 0.832539, mean_q: 16.550261
 896871/1000000: episode: 2452, duration: 3.464s, episode steps: 453, steps per second: 131, episode reward: 64.860, mean reward: 0.143 [-9.776, 0.224], mean action: -1.013 [-1.134, -0.916], mean observation: 0.160 [-0.417, 1.189], loss: 0.831008, mean_absolute_error: 0.832123, mean_q: 16.539968
 897812/1000000: episode: 2453, duration: 7.559s, episode steps: 941, steps per second: 124, episode reward: 284.254, mean reward: 0.302 [-10.390, 0.413], mean action: -0.996 [-1.167, -0.843], mean observation: 0.060 [-2.103, 1.000], loss: 0.835319, mean_absolute_error: 0.831325, mean_q: 16.545544
 898562/1000000: episode: 2454, duration: 6.330s, episode steps: 750, steps per second: 118, episode reward: 124.619, mean reward: 0.166 [-10.246, 0.349], mean action: -0.976 [-1.106, -0.810], mean observation: 0.179 [-0.473, 1.000], loss: 0.848683, mean_absolute_error: 0.836820, mean_q: 16.534922
 899358/1000000: episode: 2455, duration: 7.006s, episode steps: 796, steps per second: 114, episode reward: 190.032, mean reward: 0.239 [-10.556, 0.410], mean action: -0.991 [-1.143, -0.870], mean observation: 0.140 [-1.941, 1.496], loss: 0.852912, mean_absolute_error: 0.837760, mean_q: 16.548489
 900025/1000000: episode: 2456, duration: 5.487s, episode steps: 667, steps per second: 122, episode reward: 31.311, mean reward: 0.047 [-10.767, 0.262], mean action: -0.996 [-1.173, -0.884], mean observation: 0.119 [-2.101, 1.263], loss: 0.801656, mean_absolute_error: 0.826653, mean_q: 16.585594
 900828/1000000: episode: 2457, duration: 6.052s, episode steps: 803, steps per second: 133, episode reward: 243.972, mean reward: 0.304 [-10.588, 0.479], mean action: -0.993 [-1.139, -0.843], mean observation: 0.031 [-2.370, 1.000], loss: 0.819962, mean_absolute_error: 0.829186, mean_q: 16.567455
 901623/1000000: episode: 2458, duration: 6.009s, episode steps: 795, steps per second: 132, episode reward: 202.911, mean reward: 0.255 [-10.426, 0.403], mean action: -1.004 [-1.181, -0.836], mean observation: 0.055 [-2.131, 1.000], loss: 0.822205, mean_absolute_error: 0.831436, mean_q: 16.621786
 903072/1000000: episode: 2459, duration: 12.965s, episode steps: 1449, steps per second: 112, episode reward: 306.277, mean reward: 0.211 [-10.421, 0.714], mean action: -1.010 [-1.195, -0.826], mean observation: 0.103 [-2.072, 1.000], loss: 0.794618, mean_absolute_error: 0.824938, mean_q: 16.579943
 903705/1000000: episode: 2460, duration: 6.376s, episode steps: 633, steps per second: 99, episode reward: 71.479, mean reward: 0.113 [-10.294, 0.226], mean action: -1.002 [-1.136, -0.887], mean observation: 0.174 [-1.770, 1.351], loss: 0.756354, mean_absolute_error: 0.821746, mean_q: 16.577604
 904603/1000000: episode: 2461, duration: 8.958s, episode steps: 898, steps per second: 100, episode reward: 39.813, mean reward: 0.044 [-10.687, 0.264], mean action: -1.005 [-1.150, -0.871], mean observation: 0.036 [-1.635, 1.166], loss: 0.815375, mean_absolute_error: 0.828371, mean_q: 16.608627
 905548/1000000: episode: 2462, duration: 8.542s, episode steps: 945, steps per second: 111, episode reward: 323.604, mean reward: 0.342 [-10.341, 0.475], mean action: -1.006 [-1.138, -0.863], mean observation: 0.099 [-1.979, 1.000], loss: 0.788731, mean_absolute_error: 0.821482, mean_q: 16.622311
 906383/1000000: episode: 2463, duration: 6.570s, episode steps: 835, steps per second: 127, episode reward: 322.147, mean reward: 0.386 [-10.517, 0.609], mean action: -0.987 [-1.123, -0.816], mean observation: 0.039 [-2.370, 1.000], loss: 0.841082, mean_absolute_error: 0.828712, mean_q: 16.656343
 907679/1000000: episode: 2464, duration: 10.414s, episode steps: 1296, steps per second: 124, episode reward: 173.123, mean reward: 0.134 [-10.098, 0.365], mean action: -1.005 [-1.170, -0.837], mean observation: 0.102 [-2.262, 1.000], loss: 0.814031, mean_absolute_error: 0.820351, mean_q: 16.670170
 908729/1000000: episode: 2465, duration: 8.750s, episode steps: 1050, steps per second: 120, episode reward: 123.886, mean reward: 0.118 [-10.442, 0.237], mean action: -1.012 [-1.137, -0.878], mean observation: 0.108 [-1.526, 1.000], loss: 0.822718, mean_absolute_error: 0.817764, mean_q: 16.650471
 909422/1000000: episode: 2466, duration: 5.275s, episode steps: 693, steps per second: 131, episode reward: 234.016, mean reward: 0.338 [-10.672, 0.615], mean action: -0.993 [-1.110, -0.837], mean observation: -0.003 [-2.092, 1.000], loss: 0.824153, mean_absolute_error: 0.825339, mean_q: 16.578732
 910116/1000000: episode: 2467, duration: 5.381s, episode steps: 694, steps per second: 129, episode reward: 102.721, mean reward: 0.148 [-10.658, 0.392], mean action: -0.992 [-1.121, -0.838], mean observation: 0.137 [-1.528, 1.610], loss: 0.863977, mean_absolute_error: 0.821923, mean_q: 16.617460
 910777/1000000: episode: 2468, duration: 5.032s, episode steps: 661, steps per second: 131, episode reward: 228.134, mean reward: 0.345 [-10.678, 0.592], mean action: -1.001 [-1.184, -0.845], mean observation: 0.260 [-0.572, 2.090], loss: 0.882391, mean_absolute_error: 0.833566, mean_q: 16.540113
 911427/1000000: episode: 2469, duration: 5.048s, episode steps: 650, steps per second: 129, episode reward: 108.671, mean reward: 0.167 [-10.292, 0.366], mean action: -0.998 [-1.131, -0.874], mean observation: 0.088 [-1.164, 1.000], loss: 0.779112, mean_absolute_error: 0.818536, mean_q: 16.604946
 912055/1000000: episode: 2470, duration: 4.815s, episode steps: 628, steps per second: 130, episode reward: -17.677, mean reward: -0.028 [-10.813, 0.197], mean action: -1.007 [-1.140, -0.882], mean observation: 0.259 [-0.386, 2.188], loss: 0.808913, mean_absolute_error: 0.818190, mean_q: 16.666492
 912725/1000000: episode: 2471, duration: 5.137s, episode steps: 670, steps per second: 130, episode reward: 61.883, mean reward: 0.092 [-10.554, 0.240], mean action: -0.992 [-1.182, -0.874], mean observation: 0.131 [-1.945, 1.388], loss: 0.831151, mean_absolute_error: 0.817502, mean_q: 16.600929
 913478/1000000: episode: 2472, duration: 5.680s, episode steps: 753, steps per second: 133, episode reward: 48.946, mean reward: 0.065 [-10.870, 0.242], mean action: -1.012 [-1.160, -0.880], mean observation: -0.006 [-2.513, 1.000], loss: 0.806182, mean_absolute_error: 0.818519, mean_q: 16.635962
 914035/1000000: episode: 2473, duration: 4.209s, episode steps: 557, steps per second: 132, episode reward: 36.744, mean reward: 0.066 [-10.358, 0.214], mean action: -1.005 [-1.145, -0.855], mean observation: 0.055 [-1.616, 1.000], loss: 0.800169, mean_absolute_error: 0.818770, mean_q: 16.599483
 914775/1000000: episode: 2474, duration: 5.806s, episode steps: 740, steps per second: 127, episode reward: 176.185, mean reward: 0.238 [-10.266, 0.340], mean action: -0.998 [-1.121, -0.819], mean observation: 0.139 [-1.452, 1.773], loss: 0.827930, mean_absolute_error: 0.820080, mean_q: 16.662186
 915614/1000000: episode: 2475, duration: 6.767s, episode steps: 839, steps per second: 124, episode reward: 157.186, mean reward: 0.187 [-10.258, 0.335], mean action: -0.990 [-1.114, -0.868], mean observation: 0.076 [-1.179, 1.000], loss: 0.806269, mean_absolute_error: 0.823461, mean_q: 16.680792
 916285/1000000: episode: 2476, duration: 5.300s, episode steps: 671, steps per second: 127, episode reward: 156.052, mean reward: 0.233 [-10.368, 0.394], mean action: -0.992 [-1.139, -0.863], mean observation: 0.201 [-1.595, 2.069], loss: 0.822017, mean_absolute_error: 0.818352, mean_q: 16.622345
 916717/1000000: episode: 2477, duration: 3.818s, episode steps: 432, steps per second: 113, episode reward: 110.607, mean reward: 0.256 [-9.719, 0.288], mean action: -0.999 [-1.131, -0.836], mean observation: 0.103 [-1.457, 1.000], loss: 0.838096, mean_absolute_error: 0.822011, mean_q: 16.666719
 917433/1000000: episode: 2478, duration: 7.273s, episode steps: 716, steps per second: 98, episode reward: 260.134, mean reward: 0.363 [-10.390, 0.573], mean action: -1.010 [-1.136, -0.868], mean observation: 0.210 [-0.418, 2.552], loss: 0.795097, mean_absolute_error: 0.813894, mean_q: 16.691397
 918484/1000000: episode: 2479, duration: 10.150s, episode steps: 1051, steps per second: 104, episode reward: 378.882, mean reward: 0.360 [-10.592, 0.628], mean action: -1.004 [-1.155, -0.818], mean observation: 0.054 [-1.914, 1.000], loss: 0.814871, mean_absolute_error: 0.818389, mean_q: 16.669991
 919146/1000000: episode: 2480, duration: 5.860s, episode steps: 662, steps per second: 113, episode reward: 85.558, mean reward: 0.129 [-10.747, 0.327], mean action: -0.993 [-1.105, -0.839], mean observation: 0.231 [-0.961, 2.954], loss: 0.812022, mean_absolute_error: 0.811330, mean_q: 16.682461
 919726/1000000: episode: 2481, duration: 4.754s, episode steps: 580, steps per second: 122, episode reward: 125.855, mean reward: 0.217 [-9.897, 0.263], mean action: -0.998 [-1.113, -0.836], mean observation: 0.080 [-1.554, 1.000], loss: 0.774935, mean_absolute_error: 0.807449, mean_q: 16.735523
 920583/1000000: episode: 2482, duration: 6.769s, episode steps: 857, steps per second: 127, episode reward: 113.490, mean reward: 0.132 [-10.606, 0.320], mean action: -1.001 [-1.149, -0.843], mean observation: 0.123 [-1.221, 1.831], loss: 0.901690, mean_absolute_error: 0.825441, mean_q: 16.735659
 921233/1000000: episode: 2483, duration: 5.416s, episode steps: 650, steps per second: 120, episode reward: 137.363, mean reward: 0.211 [-10.362, 0.383], mean action: -0.990 [-1.149, -0.803], mean observation: 0.218 [-0.242, 1.806], loss: 0.779431, mean_absolute_error: 0.807223, mean_q: 16.748505
 921581/1000000: episode: 2484, duration: 2.725s, episode steps: 348, steps per second: 128, episode reward: 71.960, mean reward: 0.207 [-9.836, 0.251], mean action: -1.013 [-1.167, -0.867], mean observation: 0.183 [-0.202, 1.662], loss: 0.832303, mean_absolute_error: 0.814575, mean_q: 16.673428
 922656/1000000: episode: 2485, duration: 8.734s, episode steps: 1075, steps per second: 123, episode reward: 177.851, mean reward: 0.165 [-10.573, 0.307], mean action: -1.003 [-1.155, -0.847], mean observation: 0.195 [-0.407, 2.466], loss: 0.799288, mean_absolute_error: 0.814381, mean_q: 16.766436
 923483/1000000: episode: 2486, duration: 6.581s, episode steps: 827, steps per second: 126, episode reward: 249.135, mean reward: 0.301 [-10.637, 0.593], mean action: -0.987 [-1.155, -0.821], mean observation: 0.142 [-1.798, 1.622], loss: 0.809169, mean_absolute_error: 0.815520, mean_q: 16.779364
 923969/1000000: episode: 2487, duration: 3.704s, episode steps: 486, steps per second: 131, episode reward: 129.751, mean reward: 0.267 [-9.804, 0.309], mean action: -1.002 [-1.132, -0.865], mean observation: 0.090 [-1.262, 1.000], loss: 0.852787, mean_absolute_error: 0.820309, mean_q: 16.768911
 924438/1000000: episode: 2488, duration: 3.546s, episode steps: 469, steps per second: 132, episode reward: 75.016, mean reward: 0.160 [-10.033, 0.228], mean action: -0.999 [-1.136, -0.885], mean observation: 0.169 [-0.278, 1.631], loss: 0.832710, mean_absolute_error: 0.813325, mean_q: 16.730568
 925040/1000000: episode: 2489, duration: 4.547s, episode steps: 602, steps per second: 132, episode reward: 119.720, mean reward: 0.199 [-10.234, 0.350], mean action: -1.007 [-1.167, -0.863], mean observation: 0.061 [-1.448, 1.000], loss: 0.819425, mean_absolute_error: 0.815829, mean_q: 16.763422
 926030/1000000: episode: 2490, duration: 7.385s, episode steps: 990, steps per second: 134, episode reward: 255.416, mean reward: 0.258 [-9.927, 0.401], mean action: -0.997 [-1.162, -0.859], mean observation: 0.133 [-1.539, 1.000], loss: 0.813648, mean_absolute_error: 0.810864, mean_q: 16.744446
 926748/1000000: episode: 2491, duration: 5.341s, episode steps: 718, steps per second: 134, episode reward: 266.490, mean reward: 0.371 [-10.637, 0.626], mean action: -1.001 [-1.170, -0.834], mean observation: 0.252 [-0.383, 2.218], loss: 0.773809, mean_absolute_error: 0.802916, mean_q: 16.794020
 927366/1000000: episode: 2492, duration: 4.622s, episode steps: 618, steps per second: 134, episode reward: 99.196, mean reward: 0.161 [-10.362, 0.292], mean action: -1.004 [-1.151, -0.796], mean observation: 0.215 [-0.397, 2.014], loss: 0.790253, mean_absolute_error: 0.803008, mean_q: 16.757807
 928235/1000000: episode: 2493, duration: 6.762s, episode steps: 869, steps per second: 129, episode reward: 250.223, mean reward: 0.288 [-10.403, 0.464], mean action: -1.008 [-1.128, -0.850], mean observation: 0.054 [-1.615, 1.000], loss: 0.779684, mean_absolute_error: 0.799583, mean_q: 16.799696
 928676/1000000: episode: 2494, duration: 3.628s, episode steps: 441, steps per second: 122, episode reward: 88.855, mean reward: 0.201 [-9.755, 0.245], mean action: -0.987 [-1.115, -0.843], mean observation: 0.151 [-0.302, 1.000], loss: 0.813777, mean_absolute_error: 0.800715, mean_q: 16.830385
 929380/1000000: episode: 2495, duration: 5.972s, episode steps: 704, steps per second: 118, episode reward: 144.202, mean reward: 0.205 [-10.354, 0.374], mean action: -1.010 [-1.143, -0.858], mean observation: 0.082 [-1.658, 1.000], loss: 0.780978, mean_absolute_error: 0.793595, mean_q: 16.817455
 930225/1000000: episode: 2496, duration: 6.780s, episode steps: 845, steps per second: 125, episode reward: 129.971, mean reward: 0.154 [-10.362, 0.325], mean action: -1.001 [-1.140, -0.849], mean observation: 0.195 [-0.118, 1.199], loss: 0.827272, mean_absolute_error: 0.795530, mean_q: 16.846910
 930854/1000000: episode: 2497, duration: 4.862s, episode steps: 629, steps per second: 129, episode reward: 158.234, mean reward: 0.252 [-10.613, 0.457], mean action: -0.989 [-1.122, -0.853], mean observation: 0.114 [-1.818, 2.073], loss: 0.803927, mean_absolute_error: 0.793430, mean_q: 16.881052
 931512/1000000: episode: 2498, duration: 6.151s, episode steps: 658, steps per second: 107, episode reward: 161.205, mean reward: 0.245 [-10.659, 0.480], mean action: -1.003 [-1.155, -0.850], mean observation: 0.234 [-0.885, 2.826], loss: 0.850677, mean_absolute_error: 0.800683, mean_q: 16.818188
 932119/1000000: episode: 2499, duration: 6.105s, episode steps: 607, steps per second: 99, episode reward: -8.119, mean reward: -0.013 [-10.452, 0.128], mean action: -0.988 [-1.119, -0.861], mean observation: 0.167 [-0.526, 1.610], loss: 0.768400, mean_absolute_error: 0.792690, mean_q: 16.738253
 932823/1000000: episode: 2500, duration: 7.078s, episode steps: 704, steps per second: 99, episode reward: 212.297, mean reward: 0.302 [-10.320, 0.499], mean action: -1.005 [-1.139, -0.878], mean observation: 0.212 [-0.723, 1.942], loss: 0.812631, mean_absolute_error: 0.794524, mean_q: 16.808937
 933545/1000000: episode: 2501, duration: 6.278s, episode steps: 722, steps per second: 115, episode reward: 133.138, mean reward: 0.184 [-10.255, 0.320], mean action: -0.995 [-1.106, -0.880], mean observation: 0.189 [-0.331, 1.343], loss: 0.773248, mean_absolute_error: 0.792882, mean_q: 16.848074
 934259/1000000: episode: 2502, duration: 5.750s, episode steps: 714, steps per second: 124, episode reward: 183.144, mean reward: 0.257 [-10.636, 0.456], mean action: -0.997 [-1.147, -0.869], mean observation: 0.231 [-0.533, 2.369], loss: 0.774672, mean_absolute_error: 0.787738, mean_q: 16.886145
 935171/1000000: episode: 2503, duration: 7.430s, episode steps: 912, steps per second: 123, episode reward: 148.089, mean reward: 0.162 [-10.393, 0.320], mean action: -0.996 [-1.164, -0.833], mean observation: 0.189 [-1.716, 1.823], loss: 0.825513, mean_absolute_error: 0.791511, mean_q: 16.878832
 935902/1000000: episode: 2504, duration: 5.513s, episode steps: 731, steps per second: 133, episode reward: 109.593, mean reward: 0.150 [-10.574, 0.285], mean action: -0.995 [-1.131, -0.858], mean observation: 0.213 [-0.684, 2.515], loss: 0.770738, mean_absolute_error: 0.783562, mean_q: 16.891645
 936447/1000000: episode: 2505, duration: 4.135s, episode steps: 545, steps per second: 132, episode reward: 133.884, mean reward: 0.246 [-9.744, 0.274], mean action: -1.005 [-1.162, -0.845], mean observation: 0.133 [-0.200, 1.120], loss: 0.835261, mean_absolute_error: 0.786844, mean_q: 16.839527
 937280/1000000: episode: 2506, duration: 6.907s, episode steps: 833, steps per second: 121, episode reward: 136.732, mean reward: 0.164 [-10.634, 0.410], mean action: -0.994 [-1.138, -0.850], mean observation: 0.228 [-0.272, 1.318], loss: 0.788964, mean_absolute_error: 0.784204, mean_q: 16.945242
 938022/1000000: episode: 2507, duration: 5.979s, episode steps: 742, steps per second: 124, episode reward: 104.196, mean reward: 0.140 [-10.899, 0.364], mean action: -0.991 [-1.131, -0.871], mean observation: 0.247 [-0.686, 2.450], loss: 0.794654, mean_absolute_error: 0.781751, mean_q: 16.900579
 938788/1000000: episode: 2508, duration: 6.200s, episode steps: 766, steps per second: 124, episode reward: 161.678, mean reward: 0.211 [-10.289, 0.326], mean action: -1.012 [-1.148, -0.880], mean observation: 0.065 [-1.726, 1.000], loss: 0.787693, mean_absolute_error: 0.787326, mean_q: 16.911488
 939597/1000000: episode: 2509, duration: 6.479s, episode steps: 809, steps per second: 125, episode reward: 120.075, mean reward: 0.148 [-10.431, 0.257], mean action: -0.993 [-1.170, -0.867], mean observation: 0.067 [-2.378, 1.000], loss: 0.782367, mean_absolute_error: 0.781379, mean_q: 16.961239
 940344/1000000: episode: 2510, duration: 5.623s, episode steps: 747, steps per second: 133, episode reward: 109.724, mean reward: 0.147 [-10.283, 0.253], mean action: -0.991 [-1.127, -0.795], mean observation: 0.209 [-0.164, 1.300], loss: 0.792765, mean_absolute_error: 0.783971, mean_q: 16.982916
 940889/1000000: episode: 2511, duration: 4.239s, episode steps: 545, steps per second: 129, episode reward: -15.702, mean reward: -0.029 [-10.557, 0.122], mean action: -1.001 [-1.167, -0.879], mean observation: 0.145 [-0.887, 1.970], loss: 0.828435, mean_absolute_error: 0.787881, mean_q: 16.925833
 941485/1000000: episode: 2512, duration: 4.509s, episode steps: 596, steps per second: 132, episode reward: 143.497, mean reward: 0.241 [-10.100, 0.335], mean action: -0.998 [-1.117, -0.832], mean observation: 0.052 [-1.636, 1.000], loss: 0.790819, mean_absolute_error: 0.778977, mean_q: 16.910257
 942168/1000000: episode: 2513, duration: 5.148s, episode steps: 683, steps per second: 133, episode reward: 165.644, mean reward: 0.243 [-10.293, 0.433], mean action: -1.000 [-1.129, -0.844], mean observation: 0.092 [-1.340, 1.000], loss: 0.818781, mean_absolute_error: 0.784627, mean_q: 16.874374
 942912/1000000: episode: 2514, duration: 5.586s, episode steps: 744, steps per second: 133, episode reward: -34.048, mean reward: -0.046 [-10.552, 0.102], mean action: -1.006 [-1.131, -0.865], mean observation: 0.030 [-1.765, 1.000], loss: 0.782870, mean_absolute_error: 0.778005, mean_q: 16.992073
 943620/1000000: episode: 2515, duration: 5.401s, episode steps: 708, steps per second: 131, episode reward: 109.847, mean reward: 0.155 [-10.630, 0.350], mean action: -1.004 [-1.144, -0.826], mean observation: 0.127 [-2.352, 2.170], loss: 0.786164, mean_absolute_error: 0.779536, mean_q: 16.968588
 944351/1000000: episode: 2516, duration: 5.549s, episode steps: 731, steps per second: 132, episode reward: 84.411, mean reward: 0.115 [-10.344, 0.221], mean action: -1.019 [-1.175, -0.842], mean observation: 0.070 [-1.696, 1.029], loss: 0.758846, mean_absolute_error: 0.778105, mean_q: 16.981367
 945194/1000000: episode: 2517, duration: 6.931s, episode steps: 843, steps per second: 122, episode reward: 77.831, mean reward: 0.092 [-10.509, 0.266], mean action: -0.996 [-1.141, -0.836], mean observation: 0.221 [-0.400, 1.771], loss: 0.784072, mean_absolute_error: 0.778484, mean_q: 17.063101
 946020/1000000: episode: 2518, duration: 6.423s, episode steps: 826, steps per second: 129, episode reward: 177.918, mean reward: 0.215 [-10.509, 0.452], mean action: -0.984 [-1.126, -0.836], mean observation: 0.066 [-2.718, 1.000], loss: 0.856039, mean_absolute_error: 0.788084, mean_q: 17.028488
 946836/1000000: episode: 2519, duration: 6.383s, episode steps: 816, steps per second: 128, episode reward: 97.348, mean reward: 0.119 [-10.703, 0.295], mean action: -0.989 [-1.125, -0.811], mean observation: 0.232 [-0.322, 1.710], loss: 0.829254, mean_absolute_error: 0.783544, mean_q: 17.014952
 947694/1000000: episode: 2520, duration: 6.849s, episode steps: 858, steps per second: 125, episode reward: 169.526, mean reward: 0.198 [-10.573, 0.389], mean action: -1.007 [-1.140, -0.854], mean observation: 0.141 [-1.503, 1.624], loss: 0.790158, mean_absolute_error: 0.777604, mean_q: 17.032709
 948370/1000000: episode: 2521, duration: 5.360s, episode steps: 676, steps per second: 126, episode reward: 221.843, mean reward: 0.328 [-10.276, 0.537], mean action: -1.001 [-1.135, -0.806], mean observation: 0.055 [-1.585, 1.000], loss: 0.845426, mean_absolute_error: 0.782358, mean_q: 17.065607
 949231/1000000: episode: 2522, duration: 6.480s, episode steps: 861, steps per second: 133, episode reward: 144.328, mean reward: 0.168 [-10.855, 0.445], mean action: -0.995 [-1.187, -0.841], mean observation: 0.117 [-2.131, 2.170], loss: 0.785564, mean_absolute_error: 0.771551, mean_q: 17.107878
 950132/1000000: episode: 2523, duration: 6.650s, episode steps: 901, steps per second: 135, episode reward: 205.454, mean reward: 0.228 [-10.313, 0.406], mean action: -0.995 [-1.107, -0.890], mean observation: 0.087 [-1.711, 1.000], loss: 0.737463, mean_absolute_error: 0.770176, mean_q: 17.117956
 950845/1000000: episode: 2524, duration: 5.276s, episode steps: 713, steps per second: 135, episode reward: 181.416, mean reward: 0.254 [-10.641, 0.451], mean action: -0.991 [-1.148, -0.821], mean observation: 0.024 [-2.875, 1.000], loss: 0.835077, mean_absolute_error: 0.782689, mean_q: 17.081905
 951637/1000000: episode: 2525, duration: 5.846s, episode steps: 792, steps per second: 135, episode reward: 178.307, mean reward: 0.225 [-10.607, 0.462], mean action: -1.005 [-1.161, -0.834], mean observation: 0.246 [-0.176, 1.533], loss: 0.758771, mean_absolute_error: 0.773991, mean_q: 17.147520
 951720/1000000: episode: 2526, duration: 0.622s, episode steps: 83, steps per second: 134, episode reward: -2.216, mean reward: -0.027 [-9.906, 0.094], mean action: -0.990 [-1.132, -0.887], mean observation: 0.110 [-0.486, 1.000], loss: 0.734433, mean_absolute_error: 0.770905, mean_q: 17.163246
 952494/1000000: episode: 2527, duration: 5.731s, episode steps: 774, steps per second: 135, episode reward: -26.128, mean reward: -0.034 [-10.453, 0.137], mean action: -1.000 [-1.121, -0.838], mean observation: 0.049 [-1.099, 1.116], loss: 0.794513, mean_absolute_error: 0.772651, mean_q: 17.218336
 953079/1000000: episode: 2528, duration: 4.316s, episode steps: 585, steps per second: 136, episode reward: 62.039, mean reward: 0.106 [-10.376, 0.202], mean action: -0.994 [-1.147, -0.854], mean observation: 0.014 [-1.883, 1.000], loss: 0.882603, mean_absolute_error: 0.783035, mean_q: 17.149284
 953587/1000000: episode: 2529, duration: 3.750s, episode steps: 508, steps per second: 135, episode reward: -22.679, mean reward: -0.045 [-10.276, 0.050], mean action: -1.026 [-1.142, -0.904], mean observation: 0.197 [-0.356, 1.205], loss: 0.750541, mean_absolute_error: 0.771795, mean_q: 17.232052
 953785/1000000: episode: 2530, duration: 1.477s, episode steps: 198, steps per second: 134, episode reward: 16.791, mean reward: 0.085 [-9.866, 0.136], mean action: -1.000 [-1.176, -0.877], mean observation: 0.133 [-0.199, 1.000], loss: 0.748258, mean_absolute_error: 0.771685, mean_q: 17.231371
 954581/1000000: episode: 2531, duration: 5.887s, episode steps: 796, steps per second: 135, episode reward: 262.043, mean reward: 0.329 [-10.286, 0.543], mean action: -0.995 [-1.121, -0.873], mean observation: 0.201 [-0.219, 1.506], loss: 0.825516, mean_absolute_error: 0.777595, mean_q: 17.179346
 954797/1000000: episode: 2532, duration: 1.614s, episode steps: 216, steps per second: 134, episode reward: 26.245, mean reward: 0.122 [-9.832, 0.168], mean action: -0.999 [-1.084, -0.908], mean observation: 0.104 [-0.285, 1.000], loss: 0.790098, mean_absolute_error: 0.763015, mean_q: 17.236950
 955762/1000000: episode: 2533, duration: 7.143s, episode steps: 965, steps per second: 135, episode reward: 129.781, mean reward: 0.134 [-10.390, 0.250], mean action: -1.004 [-1.133, -0.863], mean observation: 0.198 [-0.772, 2.285], loss: 0.782691, mean_absolute_error: 0.765629, mean_q: 17.227304
 956252/1000000: episode: 2534, duration: 3.631s, episode steps: 490, steps per second: 135, episode reward: 133.807, mean reward: 0.273 [-9.786, 0.306], mean action: -1.005 [-1.135, -0.843], mean observation: 0.074 [-1.159, 1.000], loss: 0.793720, mean_absolute_error: 0.764669, mean_q: 17.164808
 956945/1000000: episode: 2535, duration: 5.125s, episode steps: 693, steps per second: 135, episode reward: 178.049, mean reward: 0.257 [-10.244, 0.404], mean action: -1.015 [-1.184, -0.856], mean observation: 0.201 [-0.323, 1.537], loss: 0.741027, mean_absolute_error: 0.754285, mean_q: 17.232424
 957109/1000000: episode: 2536, duration: 1.217s, episode steps: 164, steps per second: 135, episode reward: 12.272, mean reward: 0.075 [-9.867, 0.137], mean action: -0.999 [-1.127, -0.901], mean observation: 0.149 [-0.020, 1.000], loss: 0.865344, mean_absolute_error: 0.775459, mean_q: 17.305082
 957951/1000000: episode: 2537, duration: 6.235s, episode steps: 842, steps per second: 135, episode reward: 194.553, mean reward: 0.231 [-10.282, 0.456], mean action: -1.002 [-1.163, -0.875], mean observation: 0.177 [-0.415, 1.662], loss: 0.860531, mean_absolute_error: 0.776481, mean_q: 17.197927
 958456/1000000: episode: 2538, duration: 3.745s, episode steps: 505, steps per second: 135, episode reward: 86.123, mean reward: 0.171 [-9.988, 0.231], mean action: -1.013 [-1.133, -0.880], mean observation: 0.142 [-0.708, 1.409], loss: 0.827334, mean_absolute_error: 0.768351, mean_q: 17.278412
 959307/1000000: episode: 2539, duration: 6.617s, episode steps: 851, steps per second: 129, episode reward: 150.955, mean reward: 0.177 [-10.638, 0.404], mean action: -1.022 [-1.163, -0.882], mean observation: 0.133 [-1.735, 1.812], loss: 0.773053, mean_absolute_error: 0.766290, mean_q: 17.306063
 960312/1000000: episode: 2540, duration: 8.140s, episode steps: 1005, steps per second: 123, episode reward: 191.731, mean reward: 0.191 [-9.950, 0.334], mean action: -0.989 [-1.137, -0.851], mean observation: 0.164 [-0.470, 1.627], loss: 0.772193, mean_absolute_error: 0.764216, mean_q: 17.336584
 960957/1000000: episode: 2541, duration: 4.837s, episode steps: 645, steps per second: 133, episode reward: 37.118, mean reward: 0.058 [-10.535, 0.238], mean action: -1.009 [-1.127, -0.863], mean observation: 0.242 [-0.254, 1.633], loss: 0.769630, mean_absolute_error: 0.761633, mean_q: 17.359882
 961670/1000000: episode: 2542, duration: 5.326s, episode steps: 713, steps per second: 134, episode reward: 241.024, mean reward: 0.338 [-10.633, 0.565], mean action: -1.008 [-1.161, -0.851], mean observation: 0.251 [-0.518, 1.998], loss: 0.819903, mean_absolute_error: 0.776718, mean_q: 17.327957
 962491/1000000: episode: 2543, duration: 6.173s, episode steps: 821, steps per second: 133, episode reward: 275.741, mean reward: 0.336 [-10.720, 0.581], mean action: -0.994 [-1.126, -0.868], mean observation: 0.020 [-2.511, 1.000], loss: 0.771639, mean_absolute_error: 0.766255, mean_q: 17.368660
 963169/1000000: episode: 2544, duration: 5.093s, episode steps: 678, steps per second: 133, episode reward: 270.277, mean reward: 0.399 [-10.596, 0.631], mean action: -0.978 [-1.118, -0.817], mean observation: 0.214 [-1.214, 2.907], loss: 0.797491, mean_absolute_error: 0.769836, mean_q: 17.380802
 964095/1000000: episode: 2545, duration: 7.132s, episode steps: 926, steps per second: 130, episode reward: 7.588, mean reward: 0.008 [-10.434, 0.168], mean action: -0.996 [-1.130, -0.835], mean observation: 0.134 [-0.812, 1.463], loss: 0.779272, mean_absolute_error: 0.770818, mean_q: 17.317369
 964850/1000000: episode: 2546, duration: 5.606s, episode steps: 755, steps per second: 135, episode reward: 104.567, mean reward: 0.138 [-10.241, 0.238], mean action: -1.005 [-1.140, -0.873], mean observation: 0.184 [-0.721, 1.444], loss: 0.788597, mean_absolute_error: 0.772190, mean_q: 17.399136
 965245/1000000: episode: 2547, duration: 2.933s, episode steps: 395, steps per second: 135, episode reward: 51.453, mean reward: 0.130 [-9.908, 0.170], mean action: -1.003 [-1.147, -0.890], mean observation: 0.140 [-0.881, 1.000], loss: 0.828373, mean_absolute_error: 0.777731, mean_q: 17.423994
 966029/1000000: episode: 2548, duration: 5.828s, episode steps: 784, steps per second: 135, episode reward: 141.649, mean reward: 0.181 [-11.013, 0.387], mean action: -1.003 [-1.201, -0.819], mean observation: 0.134 [-1.563, 2.722], loss: 0.772514, mean_absolute_error: 0.770873, mean_q: 17.366465
 966384/1000000: episode: 2549, duration: 2.638s, episode steps: 355, steps per second: 135, episode reward: 57.961, mean reward: 0.163 [-9.769, 0.230], mean action: -1.004 [-1.190, -0.873], mean observation: 0.096 [-1.540, 1.000], loss: 0.761616, mean_absolute_error: 0.769284, mean_q: 17.372202
 966975/1000000: episode: 2550, duration: 4.391s, episode steps: 591, steps per second: 135, episode reward: 65.680, mean reward: 0.111 [-9.904, 0.198], mean action: -0.989 [-1.119, -0.871], mean observation: 0.066 [-1.917, 1.000], loss: 0.827675, mean_absolute_error: 0.772118, mean_q: 17.431587
 967902/1000000: episode: 2551, duration: 6.891s, episode steps: 927, steps per second: 135, episode reward: 327.178, mean reward: 0.353 [-10.285, 0.514], mean action: -1.000 [-1.187, -0.881], mean observation: 0.169 [-1.308, 1.458], loss: 0.802006, mean_absolute_error: 0.771607, mean_q: 17.389973
 968827/1000000: episode: 2552, duration: 6.875s, episode steps: 925, steps per second: 135, episode reward: 171.653, mean reward: 0.186 [-10.446, 0.372], mean action: -1.006 [-1.140, -0.838], mean observation: 0.153 [-0.825, 1.317], loss: 0.776939, mean_absolute_error: 0.770069, mean_q: 17.462179
 969485/1000000: episode: 2553, duration: 4.894s, episode steps: 658, steps per second: 134, episode reward: 108.533, mean reward: 0.165 [-10.564, 0.298], mean action: -0.987 [-1.127, -0.792], mean observation: 0.150 [-1.854, 2.158], loss: 0.793470, mean_absolute_error: 0.771427, mean_q: 17.463337
 970225/1000000: episode: 2554, duration: 5.498s, episode steps: 740, steps per second: 135, episode reward: 176.737, mean reward: 0.239 [-10.374, 0.422], mean action: -1.002 [-1.122, -0.881], mean observation: 0.216 [-0.369, 1.743], loss: 0.826260, mean_absolute_error: 0.776424, mean_q: 17.514135
 970844/1000000: episode: 2555, duration: 4.608s, episode steps: 619, steps per second: 134, episode reward: 199.961, mean reward: 0.323 [-10.511, 0.558], mean action: -1.006 [-1.125, -0.823], mean observation: 0.259 [-0.379, 2.156], loss: 0.800280, mean_absolute_error: 0.773587, mean_q: 17.540422
 971726/1000000: episode: 2556, duration: 6.588s, episode steps: 882, steps per second: 134, episode reward: 26.491, mean reward: 0.030 [-10.647, 0.254], mean action: -0.995 [-1.142, -0.855], mean observation: 0.142 [-0.827, 1.971], loss: 0.856741, mean_absolute_error: 0.788878, mean_q: 17.557222
 972176/1000000: episode: 2557, duration: 3.349s, episode steps: 450, steps per second: 134, episode reward: 83.507, mean reward: 0.186 [-9.954, 0.240], mean action: -1.022 [-1.163, -0.874], mean observation: 0.148 [-1.353, 1.290], loss: 0.782737, mean_absolute_error: 0.776754, mean_q: 17.519735
 972925/1000000: episode: 2558, duration: 5.549s, episode steps: 749, steps per second: 135, episode reward: -54.139, mean reward: -0.072 [-10.626, 0.097], mean action: -1.014 [-1.146, -0.805], mean observation: 0.227 [-0.215, 1.314], loss: 0.781621, mean_absolute_error: 0.778063, mean_q: 17.577217
 973506/1000000: episode: 2559, duration: 4.327s, episode steps: 581, steps per second: 134, episode reward: 168.368, mean reward: 0.290 [-10.308, 0.463], mean action: -0.979 [-1.106, -0.850], mean observation: 0.210 [-0.572, 1.992], loss: 0.787496, mean_absolute_error: 0.778817, mean_q: 17.583736
 974394/1000000: episode: 2560, duration: 6.597s, episode steps: 888, steps per second: 135, episode reward: 206.676, mean reward: 0.233 [-10.485, 0.447], mean action: -0.991 [-1.100, -0.867], mean observation: 0.172 [-0.646, 2.438], loss: 0.798006, mean_absolute_error: 0.776390, mean_q: 17.679846
 975031/1000000: episode: 2561, duration: 4.732s, episode steps: 637, steps per second: 135, episode reward: 56.426, mean reward: 0.089 [-10.328, 0.245], mean action: -1.002 [-1.177, -0.845], mean observation: 0.082 [-1.183, 1.000], loss: 0.806451, mean_absolute_error: 0.776732, mean_q: 17.679031
 975995/1000000: episode: 2562, duration: 7.181s, episode steps: 964, steps per second: 134, episode reward: 161.858, mean reward: 0.168 [-10.410, 0.357], mean action: -0.991 [-1.131, -0.860], mean observation: 0.095 [-1.659, 1.000], loss: 0.796073, mean_absolute_error: 0.778102, mean_q: 17.705439
 976826/1000000: episode: 2563, duration: 6.317s, episode steps: 831, steps per second: 132, episode reward: 52.875, mean reward: 0.064 [-10.415, 0.154], mean action: -1.006 [-1.148, -0.887], mean observation: 0.041 [-2.333, 1.000], loss: 0.888868, mean_absolute_error: 0.787557, mean_q: 17.654696
 977712/1000000: episode: 2564, duration: 6.606s, episode steps: 886, steps per second: 134, episode reward: 181.463, mean reward: 0.205 [-10.488, 0.397], mean action: -0.983 [-1.148, -0.847], mean observation: 0.134 [-1.342, 1.436], loss: 0.869250, mean_absolute_error: 0.788786, mean_q: 17.662500
 978405/1000000: episode: 2565, duration: 5.200s, episode steps: 693, steps per second: 133, episode reward: 84.153, mean reward: 0.121 [-10.601, 0.387], mean action: -0.991 [-1.109, -0.830], mean observation: 0.120 [-1.516, 1.000], loss: 0.785784, mean_absolute_error: 0.777852, mean_q: 17.768480
 979149/1000000: episode: 2566, duration: 5.566s, episode steps: 744, steps per second: 134, episode reward: 389.129, mean reward: 0.523 [-10.364, 0.731], mean action: -0.990 [-1.110, -0.825], mean observation: 0.209 [-0.767, 2.414], loss: 0.765763, mean_absolute_error: 0.770896, mean_q: 17.742880
 980161/1000000: episode: 2567, duration: 8.140s, episode steps: 1012, steps per second: 124, episode reward: 247.972, mean reward: 0.245 [-10.253, 0.366], mean action: -1.004 [-1.145, -0.838], mean observation: 0.127 [-0.797, 1.062], loss: 0.775496, mean_absolute_error: 0.773794, mean_q: 17.789297
 980841/1000000: episode: 2568, duration: 5.085s, episode steps: 680, steps per second: 134, episode reward: 137.650, mean reward: 0.202 [-10.532, 0.362], mean action: -1.014 [-1.166, -0.852], mean observation: 0.062 [-2.444, 1.000], loss: 0.792420, mean_absolute_error: 0.771999, mean_q: 17.827496
 981455/1000000: episode: 2569, duration: 4.987s, episode steps: 614, steps per second: 123, episode reward: 56.126, mean reward: 0.091 [-10.445, 0.252], mean action: -0.999 [-1.109, -0.861], mean observation: 0.240 [-0.294, 1.522], loss: 0.829046, mean_absolute_error: 0.771053, mean_q: 17.847282
 982101/1000000: episode: 2570, duration: 5.654s, episode steps: 646, steps per second: 114, episode reward: 142.021, mean reward: 0.220 [-10.607, 0.402], mean action: -0.997 [-1.154, -0.821], mean observation: 0.133 [-2.114, 1.770], loss: 0.745196, mean_absolute_error: 0.770025, mean_q: 17.874712
 983108/1000000: episode: 2571, duration: 8.553s, episode steps: 1007, steps per second: 118, episode reward: 115.577, mean reward: 0.115 [-10.540, 0.317], mean action: -0.992 [-1.158, -0.824], mean observation: 0.207 [-0.494, 2.544], loss: 0.770378, mean_absolute_error: 0.767634, mean_q: 17.882040
 983644/1000000: episode: 2572, duration: 4.014s, episode steps: 536, steps per second: 134, episode reward: 58.687, mean reward: 0.109 [-10.345, 0.235], mean action: -1.003 [-1.132, -0.821], mean observation: 0.055 [-1.910, 1.206], loss: 0.863149, mean_absolute_error: 0.785332, mean_q: 17.905182
 984375/1000000: episode: 2573, duration: 5.600s, episode steps: 731, steps per second: 131, episode reward: 126.489, mean reward: 0.173 [-9.747, 0.262], mean action: -1.001 [-1.153, -0.866], mean observation: 0.111 [-1.135, 1.000], loss: 0.765540, mean_absolute_error: 0.769783, mean_q: 17.911032
 984957/1000000: episode: 2574, duration: 4.333s, episode steps: 582, steps per second: 134, episode reward: 131.037, mean reward: 0.225 [-9.914, 0.298], mean action: -0.988 [-1.126, -0.864], mean observation: 0.112 [-1.478, 1.371], loss: 0.801729, mean_absolute_error: 0.775896, mean_q: 17.944668
 985530/1000000: episode: 2575, duration: 4.266s, episode steps: 573, steps per second: 134, episode reward: 135.185, mean reward: 0.236 [-9.807, 0.292], mean action: -0.994 [-1.128, -0.878], mean observation: 0.082 [-1.533, 1.000], loss: 0.806940, mean_absolute_error: 0.773804, mean_q: 17.952671
 986210/1000000: episode: 2576, duration: 5.808s, episode steps: 680, steps per second: 117, episode reward: 157.674, mean reward: 0.232 [-10.677, 0.396], mean action: -0.992 [-1.176, -0.825], mean observation: 0.109 [-1.980, 2.336], loss: 0.892845, mean_absolute_error: 0.788922, mean_q: 17.952017
 987197/1000000: episode: 2577, duration: 8.980s, episode steps: 987, steps per second: 110, episode reward: 249.430, mean reward: 0.253 [-10.244, 0.430], mean action: -0.997 [-1.148, -0.854], mean observation: 0.094 [-1.004, 1.000], loss: 0.745955, mean_absolute_error: 0.768815, mean_q: 17.966721
 987898/1000000: episode: 2578, duration: 6.116s, episode steps: 701, steps per second: 115, episode reward: 155.967, mean reward: 0.222 [-10.758, 0.477], mean action: -1.015 [-1.173, -0.900], mean observation: 0.134 [-2.259, 2.273], loss: 0.835840, mean_absolute_error: 0.781854, mean_q: 17.977655
 988789/1000000: episode: 2579, duration: 6.945s, episode steps: 891, steps per second: 128, episode reward: 205.608, mean reward: 0.231 [-10.810, 0.418], mean action: -0.996 [-1.138, -0.848], mean observation: 0.023 [-2.793, 1.000], loss: 0.827005, mean_absolute_error: 0.780698, mean_q: 18.007458
 989556/1000000: episode: 2580, duration: 5.731s, episode steps: 767, steps per second: 134, episode reward: 174.347, mean reward: 0.227 [-10.651, 0.467], mean action: -1.014 [-1.134, -0.903], mean observation: 0.245 [-0.242, 1.624], loss: 0.807707, mean_absolute_error: 0.776549, mean_q: 18.020699
 990103/1000000: episode: 2581, duration: 4.588s, episode steps: 547, steps per second: 119, episode reward: 109.055, mean reward: 0.199 [-9.848, 0.236], mean action: -1.011 [-1.134, -0.883], mean observation: 0.087 [-0.514, 1.000], loss: 0.887699, mean_absolute_error: 0.791682, mean_q: 18.045008
 990781/1000000: episode: 2582, duration: 6.224s, episode steps: 678, steps per second: 109, episode reward: 155.765, mean reward: 0.230 [-10.352, 0.365], mean action: -0.986 [-1.156, -0.808], mean observation: 0.195 [-0.498, 1.988], loss: 0.828840, mean_absolute_error: 0.781462, mean_q: 18.123009
 991445/1000000: episode: 2583, duration: 5.585s, episode steps: 664, steps per second: 119, episode reward: 160.899, mean reward: 0.242 [-10.245, 0.362], mean action: -1.001 [-1.117, -0.890], mean observation: 0.057 [-1.712, 1.000], loss: 0.787375, mean_absolute_error: 0.765896, mean_q: 18.096317
 992338/1000000: episode: 2584, duration: 7.290s, episode steps: 893, steps per second: 122, episode reward: 186.774, mean reward: 0.209 [-10.413, 0.378], mean action: -1.002 [-1.148, -0.824], mean observation: 0.200 [-0.200, 1.333], loss: 0.780477, mean_absolute_error: 0.773934, mean_q: 18.131781
 993199/1000000: episode: 2585, duration: 6.810s, episode steps: 861, steps per second: 126, episode reward: 233.506, mean reward: 0.271 [-10.318, 0.433], mean action: -1.003 [-1.128, -0.869], mean observation: 0.176 [-0.621, 1.865], loss: 0.837901, mean_absolute_error: 0.781954, mean_q: 18.174974
 993880/1000000: episode: 2586, duration: 5.643s, episode steps: 681, steps per second: 121, episode reward: 56.761, mean reward: 0.083 [-10.216, 0.199], mean action: -1.011 [-1.152, -0.889], mean observation: 0.094 [-1.000, 1.000], loss: 0.843901, mean_absolute_error: 0.786687, mean_q: 18.172522
 994700/1000000: episode: 2587, duration: 6.761s, episode steps: 820, steps per second: 121, episode reward: 161.863, mean reward: 0.197 [-10.645, 0.358], mean action: -0.995 [-1.124, -0.850], mean observation: 0.015 [-2.011, 1.000], loss: 0.779418, mean_absolute_error: 0.775861, mean_q: 18.172110
 995334/1000000: episode: 2588, duration: 5.101s, episode steps: 634, steps per second: 124, episode reward: 59.717, mean reward: 0.094 [-9.857, 0.177], mean action: -0.996 [-1.150, -0.828], mean observation: 0.085 [-0.866, 1.000], loss: 0.835109, mean_absolute_error: 0.785625, mean_q: 18.274860
 996214/1000000: episode: 2589, duration: 6.933s, episode steps: 880, steps per second: 127, episode reward: 225.953, mean reward: 0.257 [-10.626, 0.470], mean action: -1.008 [-1.155, -0.858], mean observation: 0.148 [-1.192, 1.862], loss: 0.790932, mean_absolute_error: 0.784505, mean_q: 18.238911
 996744/1000000: episode: 2590, duration: 4.746s, episode steps: 530, steps per second: 112, episode reward: 109.272, mean reward: 0.206 [-10.090, 0.307], mean action: -1.001 [-1.127, -0.876], mean observation: 0.075 [-1.452, 1.000], loss: 0.792108, mean_absolute_error: 0.773937, mean_q: 18.383762
 997459/1000000: episode: 2591, duration: 5.805s, episode steps: 715, steps per second: 123, episode reward: 278.306, mean reward: 0.389 [-10.395, 0.599], mean action: -1.000 [-1.150, -0.776], mean observation: 0.080 [-2.212, 1.000], loss: 0.765059, mean_absolute_error: 0.783942, mean_q: 18.397167
 998478/1000000: episode: 2592, duration: 7.821s, episode steps: 1019, steps per second: 130, episode reward: 389.876, mean reward: 0.383 [-10.386, 0.640], mean action: -0.987 [-1.107, -0.846], mean observation: 0.112 [-1.930, 1.071], loss: 0.772613, mean_absolute_error: 0.786528, mean_q: 18.366322
 999249/1000000: episode: 2593, duration: 5.907s, episode steps: 771, steps per second: 131, episode reward: 127.798, mean reward: 0.166 [-10.647, 0.412], mean action: -1.004 [-1.161, -0.880], mean observation: 0.123 [-1.801, 1.089], loss: 0.810943, mean_absolute_error: 0.782840, mean_q: 18.421700
done, took 7676.971 seconds
Testing for 5 episodes ...
Episode 1: reward: 899.536, steps: 1500
Episode 2: reward: 450.448, steps: 1285
Episode 3: reward: 676.301, steps: 1500
Episode 4: reward: 514.441, steps: 1500
Episode 5: reward: 377.892, steps: 1424
output.txt
